{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gngnRelpLCC"
      },
      "source": [
        "# Experiment 3: two agents selecting non-overlapping image segments in a MARL environment (Fig 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7BEFDLpnC8t"
      },
      "outputs": [],
      "source": [
        "# To run the notebook, change the string below\n",
        "dir_path = '/path/to/my/directory'\n",
        "\n",
        "# Uncomment to run in Google Colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=True)\n",
        "import sys\n",
        "sys.path.append(dir_path)\n",
        "\n",
        "# ViT model based on https://github.com/facebookresearch/dino/blob/main/README.md\n",
        "import sys\n",
        "sys.path.append(dir_path)\n",
        "import time\n",
        "from torchvision.utils import save_image\n",
        "import importlib\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "import torch.utils.data as data_utils\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import vision_transformer_mlpcritic\n",
        "import fnmatch\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "device = torch.device(\"cuda\")\n",
        "import torch.random\n",
        "from tensordict.nn import TensorDictModule\n",
        "from tensordict.tensordict import TensorDict, TensorDictBase\n",
        "from tensordict.nn.distributions import NormalParamExtractor\n",
        "\n",
        "from torchrl.collectors import SyncDataCollector\n",
        "from torchrl.data.replay_buffers import ReplayBuffer\n",
        "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
        "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
        "\n",
        "from torchrl.envs import RewardSum, TransformedEnv\n",
        "from torchrl.envs.utils import check_env_specs\n",
        "\n",
        "from torchrl.modules import ProbabilisticActor, TanhNormal\n",
        "from torch.distributions import Categorical\n",
        "from torchrl.objectives import ClipPPOLoss, ValueEstimators\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "from torchrl.data import (\n",
        "    BoundedTensorSpec, CompositeSpec, UnboundedContinuousTensorSpec,\n",
        "    DiscreteTensorSpec\n",
        ")\n",
        "from torchrl.envs import (\n",
        "    CatTensors,\n",
        "    EnvBase,\n",
        "    Transform,\n",
        "    TransformedEnv,\n",
        "    UnsqueezeTransform,\n",
        "    StepCounter\n",
        ")\n",
        "from torchrl.envs.transforms.transforms import _apply_to_composite\n",
        "from torchrl.envs.utils import check_env_specs, step_mdp\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchrl.modules import  ValueOperator, ActorCriticOperator\n",
        "from tensordict.nn import (ProbabilisticTensorDictModule, ProbabilisticTensorDictSequential, TensorDictModule, TensorDictParams, TensorDictSequential)\n",
        "\n",
        "traindir = dir_path+\"/data/train/marl_full\"\n",
        "train_transforms = transforms.Compose([transforms.Resize((16,16)),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       ])\n",
        "\n",
        "train_data = datasets.ImageFolder(traindir,transform=train_transforms)\n",
        "def cycle(iterable):\n",
        "    while True:\n",
        "        for x in iterable:\n",
        "            yield x\n",
        "image_iter = iter(cycle(torch.utils.data.DataLoader(train_data, shuffle = True, batch_size=1)))\n",
        "\n",
        "device = \"cuda\"\n",
        "vmas_device = device\n",
        "\n",
        "frames_per_batch = 6000  # number of team frames collected per training iteration\n",
        "n_iters = 50  # number of sampling and training iterations\n",
        "total_frames = frames_per_batch * n_iters\n",
        "\n",
        "num_epochs = 30  # number of optimization steps per training iteration\n",
        "minibatch_size = 300  # size of the mini-batches in each optimization step\n",
        "lr = 2e-4\n",
        "max_grad_norm = 1.0  # maximum norm for the gradients\n",
        "\n",
        "clip_epsilon = 0.2  # clip value for PPO loss\n",
        "gamma = 0.9  # discount factor\n",
        "lmbda = 0.9  # lambda for generalised advantage estimation\n",
        "entropy_eps = 1e-5  # coefficient of the entropy term in the PPO loss\n",
        "prediction_coef = 1e-3\n",
        "FIELD = next(image_iter)[0].squeeze().to(\"cuda\")\n",
        "DISCOVER_WT = 0.5\n",
        "OVERLAP_WT = 1.7\n",
        "seeds = [89982, 44686, 50255, 95253, 81551,\n",
        "         56170, 93726, 62698, 73980, 18263,\n",
        "         61224, 24087, 13041, 99425, 70874]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rF5hQsKYfE3"
      },
      "outputs": [],
      "source": [
        "class SegmentationEnv(EnvBase):\n",
        "  def __init__(self, n_agents=2, n_envs=1, name=\"\", device=\"cuda\"):\n",
        "    self.name = name\n",
        "    self.step_count = 0\n",
        "    self.device=device\n",
        "    super(SegmentationEnv, self).__init__()\n",
        "    self.field_size = FIELD.shape[1:]\n",
        "    self.n_agents = n_agents\n",
        "    self.n_envs = n_envs\n",
        "    self.field = FIELD.expand(self.n_envs, 3, *self.field_size)\n",
        "    self.player_segs = torch.zeros(self.n_envs, self.n_agents, *self.field_size)\n",
        "\n",
        "    # specs: the expected shapes of variables the environment must keep track of\n",
        "    self.full_action_spec = CompositeSpec(\n",
        "        agents=CompositeSpec(\n",
        "          action=DiscreteTensorSpec(\n",
        "          n=2,\n",
        "          shape=torch.Size([self.n_envs, self.n_agents, *self.field_size],),\n",
        "           device=self.device),\n",
        "        device=self.device\n",
        "    ),device=self.device)\n",
        "\n",
        "    self.full_observation_spec = CompositeSpec(\n",
        "        agents=CompositeSpec(\n",
        "          observation=UnboundedContinuousTensorSpec(\n",
        "              # observation (shared for all agents): n_agents player fields and 1 game field\n",
        "              shape=torch.Size([self.n_envs, self.n_agents+3, *self.field_size],),\n",
        "         device=self.device),\n",
        "       device=self.device,\n",
        "    ),device=self.device)\n",
        "\n",
        "    self.full_reward_spec = CompositeSpec(\n",
        "        agents=CompositeSpec(\n",
        "          reward=UnboundedContinuousTensorSpec(shape=torch.Size([self.n_envs, self.n_agents, 1],),\n",
        "          device=self.device),\n",
        "          agent0=UnboundedContinuousTensorSpec(shape=torch.Size([self.n_envs, 1],),\n",
        "          device=self.device),\n",
        "          agent1=UnboundedContinuousTensorSpec(shape=torch.Size([self.n_envs, 1],),\n",
        "          device=self.device),\n",
        "          overlap=UnboundedContinuousTensorSpec(shape=torch.Size([self.n_envs, 1],),\n",
        "          device=self.device),\n",
        "        device=self.device\n",
        "    ),device=self.device)\n",
        "\n",
        "    self.full_state_spec = CompositeSpec(\n",
        "        agents=CompositeSpec(\n",
        "            episode_reward=UnboundedContinuousTensorSpec(\n",
        "                    shape=torch.Size([self.n_envs, self.n_agents, 1],),\n",
        "          device=self.device),\n",
        "        device=self.device\n",
        "    ),device=self.device)\n",
        "\n",
        "\n",
        "\n",
        "  def _reset(self, tensordict=None, **kwargs):\n",
        "          out_tensordict = TensorDict({}, batch_size=torch.Size(), device=self.device)\n",
        "          FIELD = next(image_iter)[0].squeeze().to(\"cuda\")\n",
        "          self.field = FIELD.expand(self.n_envs, 3, *self.field_size)\n",
        "          self.player_segs = torch.zeros(self.n_envs, self.n_agents, *self.field_size).to(self.device)\n",
        "          # out: [1, n_agents+3, field size]\n",
        "          out_tensordict.set((\"agents\",\"observation\"), torch.cat(\n",
        "              (self.field, self.player_segs), dim=1).to(self.device), batch_size=torch.Size(),\n",
        "                             device=self.device)\n",
        "          return out_tensordict\n",
        "\n",
        "  def _step(self, tensordict):\n",
        "          self.step_count +=1\n",
        "          action = tensordict[(\"agents\",\"action\")]\n",
        "          # print(\"Board:\\n\"+str(torch.sum(self.player_segs, dim=1)[0,...]))\n",
        "\n",
        "          # player_overlap: number of times agent's selection intersected others'\n",
        "          player_overlap = []\n",
        "          # new_segs: segments any agent selected that were untouched (value 0) before this round\n",
        "          new_segs = 0\n",
        "          new_segs_agent = []\n",
        "          for agent in range(self.n_agents):\n",
        "            self_action = action[:, agent, ...].unsqueeze(1)\n",
        "            # print(\"Agent \"+str(agent)+\" action:\\n\"+str(self_action[0,...]))\n",
        "            unselected_mask = ~(torch.sum(self.player_segs, dim=1) > 0).unsqueeze(1)\n",
        "            dones = torch.count_nonzero(unselected_mask,\n",
        "                                          dim=tuple(range(1,len(self_action.shape)))) == 0\n",
        "            # print(dones)\n",
        "            new_segs += torch.count_nonzero(self_action*unselected_mask)\n",
        "            new_segs_agent.append(torch.count_nonzero(self_action*unselected_mask))\n",
        "            others_action = torch.cat((action[:, :agent, ...],\n",
        "                                       action[:, agent+1:, ...]), dim=1)\n",
        "            self_action = self_action.expand(*others_action.shape)\n",
        "            overlap = self_action == others_action\n",
        "            nonzero_overlap = self_action * overlap\n",
        "            nonzero_overlap = torch.count_nonzero(nonzero_overlap,\n",
        "                                          dim=tuple(range(1,len(nonzero_overlap.shape))))\n",
        "            # print(\"Num new: \"+str(num_new_segs[0].item()) +\", Overlap: \"+str(nonzero_overlap[0].item()))\n",
        "            player_overlap.append(nonzero_overlap.unsqueeze(-1))\n",
        "\n",
        "          player_overlap = torch.stack(player_overlap, dim=1)\n",
        "          new_segs = torch.Tensor(new_segs).expand(*player_overlap.shape)\n",
        "\n",
        "          self.player_segs += action\n",
        "\n",
        "          reward = (DISCOVER_WT*new_segs)/(1+(OVERLAP_WT*player_overlap))\n",
        "\n",
        "          out_tensordict = TensorDict(\n",
        "              {\n",
        "              \"agents\": {\n",
        "              \"observation\": torch.cat((self.field, self.player_segs), dim=1),\n",
        "              \"reward\": reward,\n",
        "              \"agent0\": new_segs_agent[0],\n",
        "              \"agent1\": new_segs_agent[1],\n",
        "              \"overlap\": player_overlap[:,0].squeeze(-1)\n",
        "              },\n",
        "              \"done\": dones\n",
        "              }, batch_size=torch.Size(), device=self.device)\n",
        "\n",
        "          return out_tensordict\n",
        "\n",
        "  def _set_seed(self, seed):\n",
        "          pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFkTSKLYL256"
      },
      "outputs": [],
      "source": [
        "senv = SegmentationEnv(n_envs=1, n_agents=2)\n",
        "print(senv.reset())\n",
        "\n",
        "senv = TransformedEnv(\n",
        "    senv,\n",
        "    RewardSum(in_keys=[(\"agents\", \"reward\")], out_keys=[(\"agents\", \"episode_reward\")]),\n",
        ")\n",
        "senv = TransformedEnv(\n",
        "    senv,\n",
        "    RewardSum(in_keys=[(\"agents\", \"agent0\")], out_keys=[(\"agents\", \"episode_agent0\")]),\n",
        ")\n",
        "senv = TransformedEnv(\n",
        "    senv,\n",
        "    RewardSum(in_keys=[(\"agents\", \"agent1\")], out_keys=[(\"agents\", \"episode_agent1\")]),\n",
        ")\n",
        "senv = TransformedEnv(\n",
        "    senv,\n",
        "    RewardSum(in_keys=[(\"agents\", \"overlap\")], out_keys=[(\"agents\", \"episode_overlap\")]),\n",
        ")\n",
        "\n",
        "senv = TransformedEnv(\n",
        "    senv,\n",
        "    StepCounter(max_steps=256),\n",
        ")\n",
        "\n",
        "print(senv.reset())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkSF0g8LMWZx"
      },
      "outputs": [],
      "source": [
        "schema_count = 0\n",
        "control_count = 0\n",
        "mixed_count = 0\n",
        "\n",
        "for trial in range(16):\n",
        "  torch.manual_seed(seeds[trial])\n",
        "  for mode in [\"schema\", \"mixed\", \"control\"]:\n",
        "    schema = (mode == \"schema\")\n",
        "    if mode == \"mixed\":\n",
        "      mixed_count += 1\n",
        "      name = \"chan5_mixed_\"+str(mixed_count)\n",
        "      out = [(\"agents\",\"h1m\"), (\"agents\", \"pred_attn\"), (\"agents\", \"logits\")]\n",
        "      policy_net = torch.nn.Sequential(\n",
        "        vision_transformer_mlpcritic.MultiAgentMixed(\n",
        "            n_agents=senv.n_agents,\n",
        "            img_size=[16],\n",
        "            in_chans=5,\n",
        "            n_agent_outputs= (senv.action_spec.shape[-1]**2)*2,\n",
        "        ),\n",
        "      )\n",
        "    else:\n",
        "      if schema:\n",
        "          schema_count += 1\n",
        "          name = \"chan5_schema_\"+str(schema_count)\n",
        "          out = [(\"agents\",\"h1m\"), (\"agents\", \"pred_attn\"), (\"agents\", \"logits\")]\n",
        "      else:\n",
        "          control_count += 1\n",
        "          name = \"chan5_control_\"+str(control_count)\n",
        "          out = [(\"agents\", \"logits\")]\n",
        "\n",
        "      policy_net = torch.nn.Sequential(\n",
        "          vision_transformer_mlpcritic.MultiAgentVit(\n",
        "              n_agents=senv.n_agents,\n",
        "              img_size=[16],\n",
        "              in_chans=5,\n",
        "              # each square of the field is a discrete binary action\n",
        "              n_agent_outputs= (senv.action_spec.shape[-1]**2)*2,\n",
        "              schema=schema,\n",
        "          ), # reshaped to [1, 2, 16, 16, 2]\n",
        "      )\n",
        "\n",
        "    policy_module = TensorDictModule(\n",
        "        policy_net,\n",
        "        in_keys=[(\"agents\", \"observation\")],\n",
        "        out_keys=out,\n",
        "    )\n",
        "\n",
        "    policy = ProbabilisticActor(\n",
        "        module=policy_module,\n",
        "        spec=senv.action_spec,\n",
        "        in_keys=[(\"agents\", \"logits\")],\n",
        "        out_keys=[senv.action_key],\n",
        "        distribution_class= Categorical,\n",
        "        return_log_prob=True,\n",
        "        log_prob_key=(\"agents\", \"sample_log_prob\"),\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    centralised = False\n",
        "    _, channels, field_dim, _ = senv.observation_spec[\"agents\", \"observation\"].shape\n",
        "\n",
        "    critic_net = vision_transformer_mlpcritic.MultiAgentMlp(\n",
        "        n_agents=senv.n_agents,\n",
        "        n_chans=5,\n",
        "        n_agent_outputs=1,\n",
        "        centralised=centralised,\n",
        "        field_dim=field_dim\n",
        "    )\n",
        "\n",
        "    critic = TensorDictModule(\n",
        "        module=critic_net,\n",
        "        in_keys=[(\"agents\", \"observation\")],\n",
        "        out_keys=[(\"agents\", \"state_value\")],\n",
        "    ).to(device)\n",
        "\n",
        "    collector = SyncDataCollector(\n",
        "        senv,\n",
        "        policy,\n",
        "        device=\"cuda\",\n",
        "        storing_device=\"cuda\",\n",
        "        frames_per_batch=frames_per_batch,\n",
        "        total_frames=total_frames,\n",
        "    )\n",
        "\n",
        "    replay_buffer = ReplayBuffer(\n",
        "        storage=LazyTensorStorage(\n",
        "            frames_per_batch, device=\"cuda\"\n",
        "        ),  # store the frames_per_batch collected at each iteration\n",
        "        sampler=SamplerWithoutReplacement(),\n",
        "        batch_size=minibatch_size,\n",
        "    )\n",
        "\n",
        "    pred_loss_module = torch.nn.MSELoss()\n",
        "\n",
        "    loss_module = ClipPPOLoss(\n",
        "        actor=policy,\n",
        "        critic=critic,\n",
        "        clip_epsilon=clip_epsilon,\n",
        "        entropy_coef=entropy_eps,\n",
        "        normalize_advantage=False,  #  avoid normalizing across the agent dimension\n",
        "    )\n",
        "    loss_module.set_keys(  # tell the loss where to find the keys\n",
        "        reward=(\"agents\", \"reward\"),\n",
        "        action=senv.action_key,\n",
        "        sample_log_prob=(\"agents\", \"sample_log_prob\"),\n",
        "        value=(\"agents\", \"state_value\"),\n",
        "        # last 2 keys will be expanded to match the reward shape\n",
        "        done=(\"done\"),\n",
        "        terminated=(\"agents\", \"terminated\"),\n",
        "    )\n",
        "\n",
        "    loss_module.make_value_estimator(\n",
        "        ValueEstimators.GAE, gamma=gamma, lmbda=lmbda\n",
        "    )\n",
        "    GAE = loss_module.value_estimator\n",
        "\n",
        "    optim = torch.optim.Adam(loss_module.parameters(), lr)\n",
        "\n",
        "    episode_reward_mean_list = []\n",
        "    episode_agent0_mean_list = []\n",
        "    episode_agent1_mean_list = []\n",
        "    episode_overlap_mean_list = []\n",
        "    episode_steps_list = []\n",
        "\n",
        "    td_counter = 0\n",
        "    for tensordict_data in collector:\n",
        "    # expand the reward, done, and terminated vectors to match ndims of action space\n",
        "    # (expected by the value estimator)\n",
        "        td_counter +=1\n",
        "        tensordict_data.set(\n",
        "            (\"next\", \"agents\", \"reward\"),\n",
        "            tensordict_data.get((\"next\", \"agents\", \"reward\"))\n",
        "            .view(*tensordict_data.get_item_shape((\"next\", \"agents\", \"reward\")), 1, 1)\n",
        "            .squeeze(1)\n",
        "            )\n",
        "\n",
        "        tensordict_data.set(\n",
        "            (\"next\", \"done\"),\n",
        "            tensordict_data.get((\"next\", \"done\"))\n",
        "            .view(*tensordict_data.get_item_shape((\"next\", \"done\")), 1, 1, 1)\n",
        "            .expand(tensordict_data.get_item_shape((\"next\", \"agents\", \"reward\")))\n",
        "        )\n",
        "\n",
        "        tensordict_data.set(\n",
        "            (\"next\", \"terminated\"),\n",
        "            tensordict_data.get((\"next\", \"terminated\"))\n",
        "            .view(*tensordict_data.get_item_shape((\"next\", \"terminated\")), 1, 1, 1)\n",
        "            .expand(tensordict_data.get_item_shape((\"next\", \"agents\", \"reward\")))\n",
        "        )\n",
        "\n",
        "        tensordict_data.set(\n",
        "            (\"done\"),\n",
        "            tensordict_data.get((\"done\"))\n",
        "            .view(*tensordict_data.get_item_shape((\"done\")), 1, 1, 1)\n",
        "            .expand(tensordict_data.get_item_shape((\"next\", \"agents\", \"reward\")))\n",
        "        )\n",
        "\n",
        "        tensordict_data.set(\n",
        "            (\"terminated\"),\n",
        "            tensordict_data.get((\"terminated\"))\n",
        "            .view(*tensordict_data.get_item_shape((\"terminated\")), 1, 1, 1)\n",
        "            .expand(tensordict_data.get_item_shape((\"next\", \"agents\", \"reward\")))\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            GAE(\n",
        "                tensordict_data,\n",
        "                params=loss_module.critic_network_params,\n",
        "                target_params=loss_module.target_critic_network_params,\n",
        "            )  # get advantages\n",
        "\n",
        "        data_view = tensordict_data.reshape(-1)  # flatten the batch size to shuffle data\n",
        "        replay_buffer.extend(data_view) # refill the buffer\n",
        "\n",
        "        for ep in range(num_epochs):\n",
        "            for _ in range(frames_per_batch // minibatch_size):\n",
        "                subdata = replay_buffer.sample()\n",
        "                # combine subdata batch dim with n_envs\n",
        "                for key in subdata.keys(include_nested=True):\n",
        "                  if (len(subdata.get(key).shape) > 4):\n",
        "                    subdata.set(key, subdata.get(key).squeeze(1))\n",
        "                loss_vals = loss_module(subdata)\n",
        "                if schema:\n",
        "                      pred_loss = prediction_coef*pred_loss_module(\n",
        "                        subdata.get((\"agents\", \"pred_attn\")),\n",
        "                        subdata.get((\"agents\", \"h1m\")))\n",
        "                else:\n",
        "                      pred_loss = 0\n",
        "\n",
        "                loss_value = (\n",
        "                    loss_vals[\"loss_objective\"]\n",
        "                    + loss_vals[\"loss_critic\"]\n",
        "                    + loss_vals[\"loss_entropy\"]\n",
        "                    + pred_loss\n",
        "                )\n",
        "\n",
        "                loss_value.backward()\n",
        "                optim.step()\n",
        "                optim.zero_grad()\n",
        "\n",
        "        collector.update_policy_weights_()\n",
        "        done = tensordict_data.get((\"next\", \"done\"))\n",
        "        episode_reward_mean = (\n",
        "            tensordict_data.get((\"next\", \"agents\", \"episode_reward\")).mean().item()\n",
        "        )\n",
        "        episode_agent0_mean = (\n",
        "            tensordict_data.get((\"next\", \"agents\", \"episode_agent0\")).mean().item()\n",
        "        )\n",
        "        episode_agent1_mean = (\n",
        "            tensordict_data.get((\"next\", \"agents\", \"episode_agent1\")).mean().item()\n",
        "        )\n",
        "        episode_overlap_mean = (\n",
        "            tensordict_data.get((\"next\", \"agents\", \"overlap\")).mean().item()\n",
        "        )\n",
        "        n_eps = torch.count_nonzero(tensordict_data.get((\"next\",\"done\")) == True).item() + 1\n",
        "        episode_steps_list.append(frames_per_batch/n_eps)\n",
        "        print(str(td_counter)+\": \"+str(episode_reward_mean))\n",
        "        episode_reward_mean_list.append(episode_reward_mean)\n",
        "        episode_agent0_mean_list.append(episode_agent0_mean)\n",
        "        episode_agent1_mean_list.append(episode_agent1_mean)\n",
        "        episode_overlap_mean_list.append(episode_overlap_mean)\n",
        "\n",
        "    file = open(dir_path+\"reward_\"+name+\".txt\",\"w\")\n",
        "    for item in episode_reward_mean_list:\n",
        "        file.write(str(item)+\"\\n\")\n",
        "    file.close()\n",
        "\n",
        "\n",
        "plt.plot(episode_reward_mean_list)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}