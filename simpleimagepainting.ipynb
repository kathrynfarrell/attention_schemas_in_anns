{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gngnRelpLCC"
      },
      "source": [
        "# Collaborative task: two agents selecting non-overlapping image segments in a MARL environment (Fig 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7BEFDLpnC8t",
        "outputId": "19a4e173-02ed-4328-b908-259d50fd020c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.3.1+cu121\n",
            "Uninstalling torch-2.3.1+cu121:\n",
            "  Successfully uninstalled torch-2.3.1+cu121\n",
            "Found existing installation: torchaudio 2.3.1+cu121\n",
            "Uninstalling torchaudio-2.3.1+cu121:\n",
            "  Successfully uninstalled torchaudio-2.3.1+cu121\n",
            "Found existing installation: torchvision 0.18.1+cu121\n",
            "Uninstalling torchvision-0.18.1+cu121:\n",
            "  Successfully uninstalled torchvision-0.18.1+cu121\n",
            "Found existing installation: torchtext 0.18.0\n",
            "Uninstalling torchtext-0.18.0:\n",
            "  Successfully uninstalled torchtext-0.18.0\n",
            "\u001b[33mWARNING: Skipping torchdata as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting torch\n",
            "  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "Collecting torchdata\n",
            "  Downloading torchdata-0.8.0-cp310-cp310-manylinux1_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch)\n",
            "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.32.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m119.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdata-0.8.0-cp310-cp310-manylinux1_x86_64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchdata\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.1\n",
            "    Uninstalling triton-2.3.1:\n",
            "      Successfully uninstalled triton-2.3.1\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 torch-2.4.0 torchdata-0.8.0 torchvision-0.19.0 triton-3.0.0\n",
            "Collecting torchrl\n",
            "  Downloading torchrl-0.5.0-cp310-cp310-manylinux1_x86_64.whl.metadata (33 kB)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchrl) (2.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchrl) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchrl) (24.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from torchrl) (2.2.1)\n",
            "Collecting tensordict>=0.5.0 (from torchrl)\n",
            "  Downloading tensordict-0.5.0-cp310-cp310-manylinux1_x86_64.whl.metadata (22 kB)\n",
            "Collecting orjson (from tensordict>=0.5.0->torchrl)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m57.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchrl) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.3.0->torchrl) (12.6.20)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.0->torchrl) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.3.0->torchrl) (1.3.0)\n",
            "Downloading torchrl-0.5.0-cp310-cp310-manylinux1_x86_64.whl (987 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m988.0/988.0 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensordict-0.5.0-cp310-cp310-manylinux1_x86_64.whl (336 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.1/336.1 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: orjson, tensordict, torchrl\n",
            "Successfully installed orjson-3.10.7 tensordict-0.5.0 torchrl-0.5.0\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip3 uninstall --yes torch torchaudio torchvision torchtext torchdata\n",
        "!pip3 install torch torchvision torchdata\n",
        "!pip3 install torchrl\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "import sys\n",
        "import time\n",
        "from torchvision.utils import save_image\n",
        "sys.path.append('/content/drive/My Drive/networkattention')\n",
        "import importlib\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "import torch.utils.data as data_utils\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import vision_transformer_mlpcritic\n",
        "import fnmatch\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "device = torch.device(\"cuda\")\n",
        "import torch.random\n",
        "from tensordict.nn import TensorDictModule\n",
        "from tensordict.tensordict import TensorDict, TensorDictBase\n",
        "from tensordict.nn.distributions import NormalParamExtractor\n",
        "\n",
        "from torchrl.collectors import SyncDataCollector\n",
        "from torchrl.data.replay_buffers import ReplayBuffer\n",
        "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
        "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
        "\n",
        "from torchrl.envs import RewardSum, TransformedEnv\n",
        "from torchrl.envs.utils import check_env_specs\n",
        "\n",
        "from torchrl.modules import ProbabilisticActor, TanhNormal\n",
        "from torch.distributions import Categorical\n",
        "from torchrl.objectives import ClipPPOLoss, ValueEstimators\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "from torchrl.data import (\n",
        "    BoundedTensorSpec, CompositeSpec, UnboundedContinuousTensorSpec,\n",
        "    DiscreteTensorSpec\n",
        ")\n",
        "from torchrl.envs import (\n",
        "    CatTensors,\n",
        "    EnvBase,\n",
        "    Transform,\n",
        "    TransformedEnv,\n",
        "    UnsqueezeTransform,\n",
        "    StepCounter\n",
        ")\n",
        "from torchrl.envs.transforms.transforms import _apply_to_composite\n",
        "from torchrl.envs.utils import check_env_specs, step_mdp\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchrl.modules import  ValueOperator, ActorCriticOperator\n",
        "from tensordict.nn import (ProbabilisticTensorDictModule, ProbabilisticTensorDictSequential, TensorDictModule, TensorDictParams, TensorDictSequential)\n",
        "\n",
        "traindir = \"/content/drive/My Drive/networkattention/data/train/marl_full\"\n",
        "train_transforms = transforms.Compose([transforms.Resize((16,16)),\n",
        "                                      #  transforms.Grayscale(),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       ])\n",
        "\n",
        "train_data = datasets.ImageFolder(traindir,transform=train_transforms)\n",
        "def cycle(iterable):\n",
        "    while True:\n",
        "        for x in iterable:\n",
        "            yield x\n",
        "image_iter = iter(cycle(torch.utils.data.DataLoader(train_data, shuffle = True, batch_size=1)))\n",
        "\n",
        "device = \"cuda\"\n",
        "vmas_device = device\n",
        "\n",
        "frames_per_batch = 6000  # number of team frames collected per training iteration\n",
        "n_iters = 50  # Number of sampling and training iterations\n",
        "total_frames = frames_per_batch * n_iters\n",
        "\n",
        "num_epochs = 30  # number of optimization steps per training iteration\n",
        "minibatch_size = 300  # size of the mini-batches in each optimization step\n",
        "lr = 2e-4\n",
        "max_grad_norm = 1.0  # maximum norm for the gradients\n",
        "\n",
        "clip_epsilon = 0.2  # clip value for PPO loss\n",
        "gamma = 0.9  # discount factor\n",
        "lmbda = 0.9  # lambda for generalised advantage estimation\n",
        "entropy_eps = 1e-5  # coefficient of the entropy term in the PPO loss\n",
        "prediction_coef = 1e-3\n",
        "# FIELD = x.to(\"cuda\")\n",
        "FIELD = next(image_iter)[0].squeeze().to(\"cuda\")\n",
        "# FIELD = torch.ones(16,16).to(\"cuda\")\n",
        "DISCOVER_WT = 0.5\n",
        "OVERLAP_WT = 1.7\n",
        "seeds = [89982, 44686, 50255, 95253, 81551,\n",
        "         56170, 93726, 62698, 73980, 18263,\n",
        "         61224, 24087, 13041, 99425, 70874]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rF5hQsKYfE3"
      },
      "outputs": [],
      "source": [
        "class SegmentationEnv(EnvBase):\n",
        "  def __init__(self, n_agents=2, n_envs=1, name=\"\", device=\"cuda\"):\n",
        "    self.name = name\n",
        "    self.step_count = 0\n",
        "    self.device=device\n",
        "    super(SegmentationEnv, self).__init__()\n",
        "    self.field_size = FIELD.shape[1:]\n",
        "    self.n_agents = n_agents\n",
        "    self.n_envs = n_envs\n",
        "    self.field = FIELD.expand(self.n_envs, 3, *self.field_size)\n",
        "    self.player_segs = torch.zeros(self.n_envs, self.n_agents, *self.field_size)\n",
        "\n",
        "    # specs: the expected shapes of variables the environment must keep track of\n",
        "    self.full_action_spec = CompositeSpec(\n",
        "        agents=CompositeSpec(\n",
        "          action=DiscreteTensorSpec(\n",
        "          n=2,\n",
        "          shape=torch.Size([self.n_envs, self.n_agents, *self.field_size],),\n",
        "           device=self.device),\n",
        "        device=self.device\n",
        "    ),device=self.device)\n",
        "\n",
        "    self.full_observation_spec = CompositeSpec(\n",
        "        agents=CompositeSpec(\n",
        "          observation=UnboundedContinuousTensorSpec(\n",
        "              # observation (shared for all agents): n_agents player fields and 1 game field\n",
        "              shape=torch.Size([self.n_envs, self.n_agents+3, *self.field_size],),\n",
        "         device=self.device),\n",
        "       device=self.device,\n",
        "    ),device=self.device)\n",
        "\n",
        "    self.full_reward_spec = CompositeSpec(\n",
        "        agents=CompositeSpec(\n",
        "          reward=UnboundedContinuousTensorSpec(shape=torch.Size([self.n_envs, self.n_agents, 1],),\n",
        "          device=self.device),\n",
        "          agent0=UnboundedContinuousTensorSpec(shape=torch.Size([self.n_envs, 1],),\n",
        "          device=self.device),\n",
        "          agent1=UnboundedContinuousTensorSpec(shape=torch.Size([self.n_envs, 1],),\n",
        "          device=self.device),\n",
        "          overlap=UnboundedContinuousTensorSpec(shape=torch.Size([self.n_envs, 1],),\n",
        "          device=self.device),\n",
        "        device=self.device\n",
        "    ),device=self.device)\n",
        "\n",
        "    self.full_state_spec = CompositeSpec(\n",
        "        agents=CompositeSpec(\n",
        "            episode_reward=UnboundedContinuousTensorSpec(\n",
        "                    shape=torch.Size([self.n_envs, self.n_agents, 1],),\n",
        "          device=self.device),\n",
        "        device=self.device\n",
        "    ),device=self.device)\n",
        "\n",
        "\n",
        "\n",
        "  def _reset(self, tensordict=None, **kwargs):\n",
        "          out_tensordict = TensorDict({}, batch_size=torch.Size(), device=self.device)\n",
        "          FIELD = next(image_iter)[0].squeeze().to(\"cuda\")\n",
        "          self.field = FIELD.expand(self.n_envs, 3, *self.field_size)\n",
        "          self.player_segs = torch.zeros(self.n_envs, self.n_agents, *self.field_size).to(self.device)\n",
        "          # out: [n_envs, n_agents+3, field size]\n",
        "          out_tensordict.set((\"agents\",\"observation\"), torch.cat(\n",
        "              (self.field, self.player_segs), dim=1).to(self.device), batch_size=torch.Size(),\n",
        "                             device=self.device)\n",
        "          return out_tensordict\n",
        "\n",
        "  def _step(self, tensordict):\n",
        "          self.step_count +=1\n",
        "          action = tensordict[(\"agents\",\"action\")]\n",
        "          # print(\"Board:\\n\"+str(torch.sum(self.player_segs, dim=1)[0,...]))\n",
        "          # player_overlap: number of times agent's selection intersected others'\n",
        "          player_overlap = []\n",
        "          # new_segs: segments any agent selected that were untouched (value 0) before this round\n",
        "          new_segs = 0\n",
        "          new_segs_agent = []\n",
        "          for agent in range(self.n_agents):\n",
        "            self_action = action[:, agent, ...].unsqueeze(1)\n",
        "            # print(\"Agent \"+str(agent)+\" action:\\n\"+str(self_action[0,...]))\n",
        "            unselected_mask = ~(torch.sum(self.player_segs, dim=1) > 0).unsqueeze(1)\n",
        "            dones = torch.count_nonzero(unselected_mask,\n",
        "                                          dim=tuple(range(1,len(self_action.shape)))) == 0\n",
        "            # print(dones)\n",
        "            new_segs += torch.count_nonzero(self_action*unselected_mask)\n",
        "            new_segs_agent.append(torch.count_nonzero(self_action*unselected_mask))\n",
        "            others_action = torch.cat((action[:, :agent, ...],\n",
        "                                       action[:, agent+1:, ...]), dim=1)\n",
        "            self_action = self_action.expand(*others_action.shape)\n",
        "            overlap = self_action == others_action\n",
        "            nonzero_overlap = self_action * overlap\n",
        "            nonzero_overlap = torch.count_nonzero(nonzero_overlap,\n",
        "                                          dim=tuple(range(1,len(nonzero_overlap.shape))))\n",
        "            # print(\"Num new: \"+str(num_new_segs[0].item()) +\", Overlap: \"+str(nonzero_overlap[0].item()))\n",
        "            player_overlap.append(nonzero_overlap.unsqueeze(-1))\n",
        "\n",
        "          player_overlap = torch.stack(player_overlap, dim=1)\n",
        "          new_segs = torch.Tensor(new_segs).expand(*player_overlap.shape)\n",
        "\n",
        "          self.player_segs += action\n",
        "\n",
        "          reward = (DISCOVER_WT*new_segs)/(1+(OVERLAP_WT*player_overlap))\n",
        "\n",
        "          out_tensordict = TensorDict(\n",
        "              {\n",
        "              \"agents\": {\n",
        "              \"observation\": torch.cat((self.field, self.player_segs), dim=1),\n",
        "              \"reward\": reward,\n",
        "              \"agent0\": new_segs_agent[0],\n",
        "              \"agent1\": new_segs_agent[1],\n",
        "              \"overlap\": player_overlap[:,0].squeeze(-1)\n",
        "              },\n",
        "              \"done\": dones\n",
        "              }, batch_size=torch.Size(), device=self.device)\n",
        "\n",
        "          return out_tensordict\n",
        "\n",
        "  def _set_seed(self, seed):\n",
        "          pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFkTSKLYL256",
        "outputId": "223eb232-b149-4242-b39a-2b458f777f39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initializing player_segs to torch.Size([1, 2, 16, 16])\n",
            "TensorDict(\n",
            "    fields={\n",
            "        agents: TensorDict(\n",
            "            fields={\n",
            "                observation: Tensor(shape=torch.Size([1, 5, 16, 16]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
            "            batch_size=torch.Size([]),\n",
            "            device=cuda,\n",
            "            is_shared=True),\n",
            "        done: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
            "        terminated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
            "    batch_size=torch.Size([]),\n",
            "    device=cuda,\n",
            "    is_shared=True)\n",
            "TensorDict(\n",
            "    fields={\n",
            "        agents: TensorDict(\n",
            "            fields={\n",
            "                episode_agent0: Tensor(shape=torch.Size([1, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                episode_agent1: Tensor(shape=torch.Size([1, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                episode_overlap: Tensor(shape=torch.Size([1, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                episode_reward: Tensor(shape=torch.Size([1, 2, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                observation: Tensor(shape=torch.Size([1, 5, 16, 16]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
            "            batch_size=torch.Size([]),\n",
            "            device=cuda,\n",
            "            is_shared=True),\n",
            "        done: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
            "        step_count: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
            "        terminated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
            "        truncated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
            "    batch_size=torch.Size([]),\n",
            "    device=cuda,\n",
            "    is_shared=True)\n",
            "TensorDict(\n",
            "    fields={\n",
            "        agents: TensorDict(\n",
            "            fields={\n",
            "                action: Tensor(shape=torch.Size([5, 1, 2, 16, 16]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
            "                episode_agent0: Tensor(shape=torch.Size([5, 1, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                episode_agent1: Tensor(shape=torch.Size([5, 1, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                episode_overlap: Tensor(shape=torch.Size([5, 1, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                episode_reward: Tensor(shape=torch.Size([5, 1, 2, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                observation: Tensor(shape=torch.Size([5, 1, 5, 16, 16]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
            "            batch_size=torch.Size([5]),\n",
            "            device=cuda,\n",
            "            is_shared=True),\n",
            "        done: Tensor(shape=torch.Size([5, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
            "        next: TensorDict(\n",
            "            fields={\n",
            "                agents: TensorDict(\n",
            "                    fields={\n",
            "                        agent0: Tensor(shape=torch.Size([5, 1, 1]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
            "                        agent1: Tensor(shape=torch.Size([5, 1, 1]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
            "                        episode_agent0: Tensor(shape=torch.Size([5, 1, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                        episode_agent1: Tensor(shape=torch.Size([5, 1, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                        episode_overlap: Tensor(shape=torch.Size([5, 1, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                        episode_reward: Tensor(shape=torch.Size([5, 1, 2, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                        observation: Tensor(shape=torch.Size([5, 1, 5, 16, 16]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                        overlap: Tensor(shape=torch.Size([5, 1, 1]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
            "                        reward: Tensor(shape=torch.Size([5, 1, 2, 1]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
            "                    batch_size=torch.Size([5]),\n",
            "                    device=cuda,\n",
            "                    is_shared=True),\n",
            "                done: Tensor(shape=torch.Size([5, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
            "                step_count: Tensor(shape=torch.Size([5, 1]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
            "                terminated: Tensor(shape=torch.Size([5, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
            "                truncated: Tensor(shape=torch.Size([5, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
            "            batch_size=torch.Size([5]),\n",
            "            device=cuda,\n",
            "            is_shared=True),\n",
            "        step_count: Tensor(shape=torch.Size([5, 1]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
            "        terminated: Tensor(shape=torch.Size([5, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
            "        truncated: Tensor(shape=torch.Size([5, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
            "    batch_size=torch.Size([5]),\n",
            "    device=cuda,\n",
            "    is_shared=True)\n"
          ]
        }
      ],
      "source": [
        "senv = SegmentationEnv(n_envs=1, n_agents=2)\n",
        "print(senv.reset())\n",
        "\n",
        "senv = TransformedEnv(\n",
        "    senv,\n",
        "    RewardSum(in_keys=[(\"agents\", \"reward\")], out_keys=[(\"agents\", \"episode_reward\")]),\n",
        ")\n",
        "senv = TransformedEnv(\n",
        "    senv,\n",
        "    RewardSum(in_keys=[(\"agents\", \"agent0\")], out_keys=[(\"agents\", \"episode_agent0\")]),\n",
        ")\n",
        "senv = TransformedEnv(\n",
        "    senv,\n",
        "    RewardSum(in_keys=[(\"agents\", \"agent1\")], out_keys=[(\"agents\", \"episode_agent1\")]),\n",
        ")\n",
        "senv = TransformedEnv(\n",
        "    senv,\n",
        "    RewardSum(in_keys=[(\"agents\", \"overlap\")], out_keys=[(\"agents\", \"episode_overlap\")]),\n",
        ")\n",
        "\n",
        "senv = TransformedEnv(\n",
        "    senv,\n",
        "    StepCounter(max_steps=256),\n",
        ")\n",
        "\n",
        "print(senv.reset())\n",
        "print(senv.rollout(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OP9W9TbVXx8f"
      },
      "outputs": [],
      "source": [
        "policy_net = torch.nn.Sequential(\n",
        "        vision_transformer_mlpcritic.MultiAgentMixed(\n",
        "            n_agents=senv.n_agents,\n",
        "            img_size=[16],\n",
        "            in_chans=5,\n",
        "            n_agent_outputs= (senv.action_spec.shape[-1]**2)*2,\n",
        "        ),\n",
        "      )\n",
        "\n",
        "policy_net = torch.nn.Sequential(\n",
        "          vision_transformer_mlpcritic.MultiAgentVit(\n",
        "              n_agents=senv.n_agents,\n",
        "              img_size=[16],\n",
        "              in_chans=5,\n",
        "              # each square of the field is a discrete binary action\n",
        "              n_agent_outputs= (senv.action_spec.shape[-1]**2)*2,\n",
        "              schema=True,\n",
        "          ), # reshaped to [1, 2, 16, 16, 2]\n",
        "      )\n",
        "\n",
        "policy_net = torch.nn.Sequential(\n",
        "          vision_transformer_mlpcritic.MultiAgentVit(\n",
        "              n_agents=senv.n_agents,\n",
        "              img_size=[16],\n",
        "              in_chans=5,\n",
        "              # each square of the field is a discrete binary action\n",
        "              n_agent_outputs= (senv.action_spec.shape[-1]**2)*2,\n",
        "              schema=False,\n",
        "          ), # reshaped to [1, 2, 16, 16, 2]\n",
        "      )\n",
        "\n",
        "del(policy_net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkSF0g8LMWZx"
      },
      "outputs": [],
      "source": [
        "schema_count = 0\n",
        "control_count = 0\n",
        "mixed_count = 0\n",
        "\n",
        "for trial in range(16):\n",
        "  torch.manual_seed(seeds[trial])\n",
        "  for mode in [\"schema\", \"mixed\", \"control\"]:\n",
        "    schema = (mode == \"schema\")\n",
        "    if mode == \"mixed\":\n",
        "      mixed_count += 1\n",
        "      name = \"chan5_mixed_\"+str(mixed_count)\n",
        "      out = [(\"agents\",\"h1m\"), (\"agents\", \"pred_attn\"), (\"agents\", \"logits\")]\n",
        "      policy_net = torch.nn.Sequential(\n",
        "        vision_transformer_mlpcritic.MultiAgentMixed(\n",
        "            n_agents=senv.n_agents,\n",
        "            img_size=[16],\n",
        "            in_chans=5,\n",
        "            n_agent_outputs= (senv.action_spec.shape[-1]**2)*2,\n",
        "        ),\n",
        "      )\n",
        "    else:\n",
        "      if schema:\n",
        "          schema_count += 1\n",
        "          name = \"chan5_schema_\"+str(schema_count)\n",
        "          out = [(\"agents\",\"h1m\"), (\"agents\", \"pred_attn\"), (\"agents\", \"logits\")]\n",
        "      else:\n",
        "          control_count += 1\n",
        "          name = \"chan5_control_\"+str(control_count)\n",
        "          out = [(\"agents\", \"logits\")]\n",
        "\n",
        "      policy_net = torch.nn.Sequential(\n",
        "          vision_transformer_mlpcritic.MultiAgentVit(\n",
        "              n_agents=senv.n_agents,\n",
        "              img_size=[16],\n",
        "              in_chans=5,\n",
        "              # each square of the field is a discrete binary action\n",
        "              n_agent_outputs= (senv.action_spec.shape[-1]**2)*2,\n",
        "              schema=schema,\n",
        "          ), # reshaped to [1, 2, 16, 16, 2]\n",
        "      )\n",
        "\n",
        "    policy_module = TensorDictModule(\n",
        "        policy_net,\n",
        "        in_keys=[(\"agents\", \"observation\")],\n",
        "        out_keys=out,\n",
        "    )\n",
        "\n",
        "    policy = ProbabilisticActor(\n",
        "        module=policy_module,\n",
        "        spec=senv.action_spec,\n",
        "        in_keys=[(\"agents\", \"logits\")],\n",
        "        out_keys=[senv.action_key],\n",
        "        distribution_class= Categorical,\n",
        "        return_log_prob=True,\n",
        "        log_prob_key=(\"agents\", \"sample_log_prob\"),\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    centralised = False  # IPPO\n",
        "    _, channels, field_dim, _ = senv.observation_spec[\"agents\", \"observation\"].shape\n",
        "\n",
        "    critic_net = vision_transformer_mlpcritic.MultiAgentMlp(\n",
        "        n_agents=senv.n_agents,\n",
        "        n_chans=5,\n",
        "        n_agent_outputs=1,\n",
        "        centralised=centralised,\n",
        "        field_dim=field_dim\n",
        "    )\n",
        "\n",
        "    critic = TensorDictModule(\n",
        "        module=critic_net,\n",
        "        in_keys=[(\"agents\", \"observation\")],\n",
        "        out_keys=[(\"agents\", \"state_value\")],\n",
        "    ).to(device)\n",
        "\n",
        "    collector = SyncDataCollector(\n",
        "        senv,\n",
        "        policy,\n",
        "        device=\"cuda\",\n",
        "        storing_device=\"cuda\",\n",
        "        frames_per_batch=frames_per_batch,\n",
        "        total_frames=total_frames,\n",
        "    )\n",
        "\n",
        "    replay_buffer = ReplayBuffer(\n",
        "        storage=LazyTensorStorage(\n",
        "            frames_per_batch, device=\"cuda\"\n",
        "        ),  # store the frames_per_batch collected at each iteration\n",
        "        sampler=SamplerWithoutReplacement(),\n",
        "        batch_size=minibatch_size,\n",
        "    )\n",
        "\n",
        "    pred_loss_module = torch.nn.MSELoss()\n",
        "\n",
        "    loss_module = ClipPPOLoss(\n",
        "        actor=policy,\n",
        "        critic=critic,\n",
        "        clip_epsilon=clip_epsilon,\n",
        "        entropy_coef=entropy_eps,\n",
        "        normalize_advantage=False,  #  avoid normalizing across the agent dimension\n",
        "    )\n",
        "    loss_module.set_keys(  # tell the loss where to find the keys\n",
        "        reward=(\"agents\", \"reward\"),\n",
        "        action=senv.action_key,\n",
        "        sample_log_prob=(\"agents\", \"sample_log_prob\"),\n",
        "        value=(\"agents\", \"state_value\"),\n",
        "        # last 2 keys will be expanded to match the reward shape\n",
        "        done=(\"done\"),\n",
        "        terminated=(\"agents\", \"terminated\"),\n",
        "    )\n",
        "\n",
        "    loss_module.make_value_estimator(\n",
        "        ValueEstimators.GAE, gamma=gamma, lmbda=lmbda\n",
        "    )\n",
        "    GAE = loss_module.value_estimator\n",
        "\n",
        "    optim = torch.optim.Adam(loss_module.parameters(), lr)\n",
        "\n",
        "    episode_reward_mean_list = []\n",
        "    episode_agent0_mean_list = []\n",
        "    episode_agent1_mean_list = []\n",
        "    episode_overlap_mean_list = []\n",
        "    episode_steps_list = []\n",
        "\n",
        "    td_counter = 0\n",
        "    for tensordict_data in collector:\n",
        "    # expand the reward, done, and terminated vectors to match ndims of action space\n",
        "    # (expected by the value estimator)\n",
        "        td_counter +=1\n",
        "        tensordict_data.set(\n",
        "            (\"next\", \"agents\", \"reward\"),\n",
        "            tensordict_data.get((\"next\", \"agents\", \"reward\"))\n",
        "            .view(*tensordict_data.get_item_shape((\"next\", \"agents\", \"reward\")), 1, 1)\n",
        "            .squeeze(1)\n",
        "            )\n",
        "\n",
        "        tensordict_data.set(\n",
        "            (\"next\", \"done\"),\n",
        "            tensordict_data.get((\"next\", \"done\"))\n",
        "            .view(*tensordict_data.get_item_shape((\"next\", \"done\")), 1, 1, 1)\n",
        "            .expand(tensordict_data.get_item_shape((\"next\", \"agents\", \"reward\")))\n",
        "        )\n",
        "\n",
        "        tensordict_data.set(\n",
        "            (\"next\", \"terminated\"),\n",
        "            tensordict_data.get((\"next\", \"terminated\"))\n",
        "            .view(*tensordict_data.get_item_shape((\"next\", \"terminated\")), 1, 1, 1)\n",
        "            .expand(tensordict_data.get_item_shape((\"next\", \"agents\", \"reward\")))\n",
        "        )\n",
        "\n",
        "        tensordict_data.set(\n",
        "            (\"done\"),\n",
        "            tensordict_data.get((\"done\"))\n",
        "            .view(*tensordict_data.get_item_shape((\"done\")), 1, 1, 1)\n",
        "            .expand(tensordict_data.get_item_shape((\"next\", \"agents\", \"reward\")))\n",
        "        )\n",
        "\n",
        "        tensordict_data.set(\n",
        "            (\"terminated\"),\n",
        "            tensordict_data.get((\"terminated\"))\n",
        "            .view(*tensordict_data.get_item_shape((\"terminated\")), 1, 1, 1)\n",
        "            .expand(tensordict_data.get_item_shape((\"next\", \"agents\", \"reward\")))\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            GAE(\n",
        "                tensordict_data,\n",
        "                params=loss_module.critic_network_params,\n",
        "                target_params=loss_module.target_critic_network_params,\n",
        "            )  # get advantages\n",
        "\n",
        "        data_view = tensordict_data.reshape(-1)  # flatten the batch size to shuffle data\n",
        "        replay_buffer.extend(data_view) # refill the buffer\n",
        "\n",
        "        for ep in range(num_epochs):\n",
        "            for _ in range(frames_per_batch // minibatch_size):\n",
        "                subdata = replay_buffer.sample()\n",
        "                # combine subdata batch dim with n_envs\n",
        "                for key in subdata.keys(include_nested=True):\n",
        "                  if (len(subdata.get(key).shape) > 4):\n",
        "                    subdata.set(key, subdata.get(key).squeeze(1))\n",
        "                loss_vals = loss_module(subdata)\n",
        "                if schema:\n",
        "                      pred_loss = prediction_coef*pred_loss_module(\n",
        "                        subdata.get((\"agents\", \"pred_attn\")),\n",
        "                        subdata.get((\"agents\", \"h1m\")))\n",
        "                else:\n",
        "                      pred_loss = 0\n",
        "\n",
        "                loss_value = (\n",
        "                    loss_vals[\"loss_objective\"]\n",
        "                    + loss_vals[\"loss_critic\"]\n",
        "                    + loss_vals[\"loss_entropy\"]\n",
        "                    + pred_loss\n",
        "                )\n",
        "\n",
        "                loss_value.backward()\n",
        "                optim.step()\n",
        "                optim.zero_grad()\n",
        "\n",
        "        collector.update_policy_weights_()\n",
        "        done = tensordict_data.get((\"next\", \"done\"))\n",
        "        episode_reward_mean = (\n",
        "            tensordict_data.get((\"next\", \"agents\", \"episode_reward\")).mean().item()\n",
        "        )\n",
        "        episode_agent0_mean = (\n",
        "            tensordict_data.get((\"next\", \"agents\", \"episode_agent0\")).mean().item()\n",
        "        )\n",
        "        episode_agent1_mean = (\n",
        "            tensordict_data.get((\"next\", \"agents\", \"episode_agent1\")).mean().item()\n",
        "        )\n",
        "        episode_overlap_mean = (\n",
        "            tensordict_data.get((\"next\", \"agents\", \"overlap\")).mean().item()\n",
        "        )\n",
        "        n_eps = torch.count_nonzero(tensordict_data.get((\"next\",\"done\")) == True).item() + 1\n",
        "        episode_steps_list.append(frames_per_batch/n_eps)\n",
        "        print(str(td_counter)+\": \"+str(episode_reward_mean))\n",
        "        episode_reward_mean_list.append(episode_reward_mean)\n",
        "        episode_agent0_mean_list.append(episode_agent0_mean)\n",
        "        episode_agent1_mean_list.append(episode_agent1_mean)\n",
        "        episode_overlap_mean_list.append(episode_overlap_mean)\n",
        "\n",
        "    file = open(\"/content/drive/My Drive/networkattention/rewardcurves/\"+name+\".txt\",\"w\")\n",
        "    for item in episode_reward_mean_list:\n",
        "        file.write(str(item)+\"\\n\")\n",
        "    file.close()\n",
        "\n",
        "    file = open(\"/content/drive/My Drive/networkattention/rewardcurves/\"+name+\"agent0.txt\",\"w\")\n",
        "    for item in episode_agent0_mean_list:\n",
        "        file.write(str(item)+\"\\n\")\n",
        "    file.close()\n",
        "\n",
        "    file = open(\"/content/drive/My Drive/networkattention/rewardcurves/\"+name+\"agent1.txt\",\"w\")\n",
        "    for item in episode_agent1_mean_list:\n",
        "        file.write(str(item)+\"\\n\")\n",
        "    file.close()\n",
        "\n",
        "    file = open(\"/content/drive/My Drive/networkattention/rewardcurves/\"+name+\"overlap.txt\",\"w\")\n",
        "    for item in episode_overlap_mean_list:\n",
        "        file.write(str(item)+\"\\n\")\n",
        "    file.close()\n",
        "\n",
        "    file = open(\"/content/drive/My Drive/networkattention/episodesteps/\"+name+\".txt\",\"w\")\n",
        "    for item in episode_steps_list:\n",
        "        file.write(str(item)+\"\\n\")\n",
        "    file.close()\n",
        "\n",
        "\n",
        "plt.plot(episode_reward_mean_list)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}