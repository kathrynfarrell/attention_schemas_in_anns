{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Attention task: the discrimination of genuine and artificially shuffled full attentional vectors (Fig 2)"
      ],
      "metadata": {
        "id": "F7hAcakInyRk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyTz-3NgKimO"
      },
      "outputs": [],
      "source": [
        "!pip3 uninstall --yes torch torchaudio torchvision torchtext torchdata\n",
        "!pip3 install torch torchvision torchdata\n",
        "!pip3 install torchrl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RLJF3R41eFA",
        "outputId": "35cbde37-7115-430c-e195-748de129929f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/networkattention')\n",
        "# vit model from https://github.com/facebookresearch/dino/blob/main/README.md\n",
        "import importlib\n",
        "import torch\n",
        "import torch.random\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.utils.data as data_utils\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import vision_transformer\n",
        "import fnmatch\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "# IMAGE CLASSIFICATION A, B, AND C DATA LOADERS\n",
        "\n",
        "traindirA = \"/content/drive/My Drive/networkattention/data/train/classificationA\"\n",
        "valdirA = \"/content/drive/My Drive/networkattention/data/val/classificationA\"\n",
        "\n",
        "traindirB = \"/content/drive/My Drive/networkattention/data/train/classificationB\"\n",
        "valdirB = \"/content/drive/My Drive/networkattention/data/val/classificationB\"\n",
        "\n",
        "traindirC = \"/content/drive/My Drive/networkattention/data/train/classificationC\"\n",
        "valdirC = \"/content/drive/My Drive/networkattention/data/val/classificationC\"\n",
        "\n",
        "train_transforms = transforms.Compose([transforms.Resize((256,256)),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       ])\n",
        "val_transforms = transforms.Compose([transforms.Resize((256,256)),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      ])\n",
        "\n",
        "\n",
        "def schematrain(model, x, y, optimizer):\n",
        "    pred_attn, h1m, policy = model.forward(x)\n",
        "    mse = torch.nn.MSELoss()\n",
        "    bce = torch.nn.BCEWithLogitsLoss()\n",
        "    pred_loss = 0.05*mse(pred_attn, h1m)\n",
        "    policy_loss = bce(policy, y)\n",
        "    total_loss = sum([pred_loss, policy_loss])\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    return total_loss\n",
        "\n",
        "def schematrain_policy(model, x, y, optimizer):\n",
        "    pred_attn, h1m, policy = model.forward(x)\n",
        "    bce = torch.nn.BCEWithLogitsLoss()\n",
        "    policy_loss = bce(policy, y)\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    return policy_loss\n",
        "\n",
        "def controltrain(model, x, y, optimizer):\n",
        "    h1, policy = model.forward(x)\n",
        "    bce = torch.nn.BCEWithLogitsLoss()\n",
        "    policy_loss = bce(policy, y)\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    return policy_loss\n",
        "\n",
        "def fitschema(model, trainloader, valloader, name=\"\", n_epochs=20, policy_only=False):\n",
        "  bce = torch.nn.BCEWithLogitsLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "  losses = []\n",
        "  val_losses = []\n",
        "\n",
        "  epoch_train_losses = []\n",
        "  epoch_val_losses = []\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "      epoch_loss = 0\n",
        "      for i, data in enumerate(trainloader): #iterate over batches\n",
        "          x_batch, y_batch = data\n",
        "          x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "          y_batch = y_batch.unsqueeze(1).float() #convert target to same nn output shape\n",
        "          model.train()\n",
        "          if policy_only:\n",
        "            loss = schematrain_policy(model, x_batch, y_batch, optimizer)\n",
        "          else:\n",
        "            loss = schematrain(model, x_batch, y_batch, optimizer)\n",
        "          epoch_loss += loss.item()/len(trainloader)\n",
        "          losses.append(loss.item())\n",
        "          if epoch == 0:\n",
        "            print(str(i)+\": \"+str(loss.item())+\" / \"+str(len(trainloader))+\": \"+str(epoch_loss))\n",
        "      epoch_train_losses.append(epoch_loss)\n",
        "      print('\\nEpoch : {}, train loss : {}'.format(epoch+1,epoch_loss))\n",
        "      with torch.no_grad():\n",
        "        cum_loss = 0\n",
        "        for x_batch, y_batch in valloader:\n",
        "          x_batch = x_batch.to(device)\n",
        "          y_batch = y_batch.unsqueeze(1).float() #convert target to same nn output shape\n",
        "          y_batch = y_batch.to(device)\n",
        "\n",
        "          #model to eval mode\n",
        "          model.eval()\n",
        "\n",
        "          _, _, policy = model(x_batch)\n",
        "          val_loss = bce(policy,y_batch)\n",
        "          cum_loss += val_loss.item()/len(valloader)\n",
        "          val_losses.append(val_loss.item())\n",
        "\n",
        "        epoch_val_losses.append(cum_loss)\n",
        "        print('Epoch : {}, val loss : {}'.format(epoch+1,cum_loss))\n",
        "\n",
        "        best_loss = min(epoch_val_losses)\n",
        "\n",
        "        #save best model\n",
        "        if cum_loss <= best_loss:\n",
        "          best_model_wts = model.state_dict()\n",
        "\n",
        "  model.load_state_dict(best_model_wts)\n",
        "\n",
        "  file = open(\"/content/drive/My Drive/networkattention/losscurves/\"+name+\"schema.txt\",\"w\")\n",
        "  for item in epoch_train_losses:\n",
        "    file.write(str(item)+\"\\n\")\n",
        "  file.close()\n",
        "\n",
        "def fitcontrol(model, trainloader, valloader, name=\"\", n_epochs=20):\n",
        "  bce = torch.nn.BCEWithLogitsLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "  losses = []\n",
        "  val_losses = []\n",
        "\n",
        "  epoch_train_losses = []\n",
        "  epoch_val_losses = []\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "      epoch_loss = 0\n",
        "      for i, data in enumerate(trainloader): #iterate over batches\n",
        "          x_batch, y_batch = data\n",
        "          x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "          y_batch = y_batch.unsqueeze(1).float() #convert target to same nn output shape\n",
        "          model.train()\n",
        "          loss = controltrain(model, x_batch, y_batch, optimizer)\n",
        "          epoch_loss += loss.item()/len(trainloader)\n",
        "          losses.append(loss.item())\n",
        "          if epoch == 0:\n",
        "            print(str(i)+\": \"+str(loss.item())+\" / \"+str(len(trainloader))+\": \"+str(epoch_loss))\n",
        "      epoch_train_losses.append(epoch_loss)\n",
        "      print('\\nEpoch : {}, train loss : {}'.format(epoch+1,epoch_loss))\n",
        "      with torch.no_grad():\n",
        "        cum_loss = 0\n",
        "        for x_batch, y_batch in valloader:\n",
        "          x_batch = x_batch.to(device)\n",
        "          y_batch = y_batch.unsqueeze(1).float() #convert target to same nn output shape\n",
        "          y_batch = y_batch.to(device)\n",
        "\n",
        "          #model to eval mode\n",
        "          model.eval()\n",
        "\n",
        "          _, policy = model(x_batch)\n",
        "          val_loss = bce(policy,y_batch)\n",
        "          cum_loss += val_loss.item()/len(valloader)\n",
        "          val_losses.append(val_loss.item())\n",
        "\n",
        "        epoch_val_losses.append(cum_loss)\n",
        "        print('Epoch : {}, val loss : {}'.format(epoch+1,cum_loss))\n",
        "\n",
        "        best_loss = min(epoch_val_losses)\n",
        "\n",
        "        #save best model\n",
        "        if cum_loss <= best_loss:\n",
        "          best_model_wts = model.state_dict()\n",
        "\n",
        "  model.load_state_dict(best_model_wts)\n",
        "\n",
        "  file = open(\"/content/drive/My Drive/networkattention/losscurves/\"+name+\"control.txt\",\"w\")\n",
        "  for item in epoch_train_losses:\n",
        "    file.write(str(item)+\"\\n\")\n",
        "  file.close()\n",
        "\n",
        "def evaluate(model, valloader, name=\"\", save_attn=False):\n",
        "  classifications = []\n",
        "  labels = []\n",
        "  model.eval()\n",
        "  sigmoid = torch.nn.Sigmoid()\n",
        "  total_acc = 0\n",
        "  for i, data in enumerate(valloader):\n",
        "      accuracy = 0\n",
        "      x_batch, y_batch = data\n",
        "      x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "      y_batch = y_batch.unsqueeze(1).float()\n",
        "      outputs = model.forward(x_batch)\n",
        "      policy = outputs[-1]\n",
        "      policy = torch.round(sigmoid(policy))\n",
        "      accuracy = 1-(torch.sum(abs(policy - y_batch))/len(y_batch))\n",
        "      total_acc += accuracy.item()/len(valloader)\n",
        "      for pol in policy:\n",
        "        classifications.append(pol.item())\n",
        "      for yb in y_batch:\n",
        "        labels.append(yb.item())\n",
        "\n",
        "  file = open(\"/content/drive/My Drive/networkattention/accuracies/acc\"+name+\".txt\",\"w\")\n",
        "  file.write(str(total_acc))\n",
        "  file.close()\n",
        "\n",
        "  file = open(\"/content/drive/My Drive/networkattention/classifications/\"+name+\"_classifications.txt\",\"w\")\n",
        "  file.write(str(classifications))\n",
        "  file.close()\n",
        "\n",
        "  file = open(\"/content/drive/My Drive/networkattention/classifications/\"+name+\"_labels.txt\",\"w\")\n",
        "  file.write(str(labels))\n",
        "  file.close()\n",
        "\n",
        "def freeze_models(models):\n",
        "  for i, model in enumerate(models):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    for param in model.policy.parameters():\n",
        "        param.requires_grad = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvGJaXMs5v25",
        "outputId": "8eb3c14b-8bda-4302-d9f5-e0f58d9f0d30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\n",
            "Epoch : 579, train loss : 0.3516036630579919\n",
            "Epoch : 579, val loss : 0.37962951628785385\n",
            "\n",
            "Epoch : 580, train loss : 0.34724008594498496\n",
            "Epoch : 580, val loss : 0.37722756752842346\n",
            "\n",
            "Epoch : 581, train loss : 0.3485862810503354\n",
            "Epoch : 581, val loss : 0.37159482272047745\n",
            "\n",
            "Epoch : 582, train loss : 0.3504237967910188\n",
            "Epoch : 582, val loss : 0.3699114338347787\n",
            "\n",
            "Epoch : 583, train loss : 0.3487185764493365\n",
            "Epoch : 583, val loss : 0.3831799924373627\n",
            "\n",
            "Epoch : 584, train loss : 0.3514153922146015\n",
            "Epoch : 584, val loss : 0.3668239587231687\n",
            "\n",
            "Epoch : 585, train loss : 0.34972875118255614\n",
            "Epoch : 585, val loss : 0.37437489471937474\n",
            "\n",
            "Epoch : 586, train loss : 0.34805323255784587\n",
            "Epoch : 586, val loss : 0.36929420577852345\n",
            "\n",
            "Epoch : 587, train loss : 0.35683784999630663\n",
            "Epoch : 587, val loss : 0.372655821473975\n",
            "\n",
            "Epoch : 588, train loss : 0.3478414093003131\n",
            "Epoch : 588, val loss : 0.37273580149600377\n",
            "\n",
            "Epoch : 589, train loss : 0.3459906779455417\n",
            "Epoch : 589, val loss : 0.3656466328784038\n",
            "\n",
            "Epoch : 590, train loss : 0.34733219110604496\n",
            "Epoch : 590, val loss : 0.3753240657480139\n",
            "\n",
            "Epoch : 591, train loss : 0.34644597841031616\n",
            "Epoch : 591, val loss : 0.36461224524598373\n",
            "\n",
            "Epoch : 592, train loss : 0.34620624003988315\n",
            "Epoch : 592, val loss : 0.37579955944889465\n",
            "\n",
            "Epoch : 593, train loss : 0.345447468035149\n",
            "Epoch : 593, val loss : 0.3769429098618658\n",
            "\n",
            "Epoch : 594, train loss : 0.3452969821113529\n",
            "Epoch : 594, val loss : 0.3778527880969801\n",
            "\n",
            "Epoch : 595, train loss : 0.3468089538090157\n",
            "Epoch : 595, val loss : 0.3647844760041488\n",
            "\n",
            "Epoch : 596, train loss : 0.3468809816873434\n",
            "Epoch : 596, val loss : 0.37674408053096964\n",
            "\n",
            "Epoch : 597, train loss : 0.34490206530599876\n",
            "Epoch : 597, val loss : 0.36437841857734476\n",
            "\n",
            "Epoch : 598, train loss : 0.3516145546327938\n",
            "Epoch : 598, val loss : 0.3704028709938652\n",
            "\n",
            "Epoch : 599, train loss : 0.3437122819098559\n",
            "Epoch : 599, val loss : 0.3800728744582126\n",
            "\n",
            "Epoch : 600, train loss : 0.3453666355573769\n",
            "Epoch : 600, val loss : 0.3630664850536146\n",
            "\n",
            "Epoch : 601, train loss : 0.3482329392975026\n",
            "Epoch : 601, val loss : 0.36863793667994044\n",
            "\n",
            "Epoch : 602, train loss : 0.3440796881011037\n",
            "Epoch : 602, val loss : 0.3737850204894418\n",
            "\n",
            "Epoch : 603, train loss : 0.3499026242530706\n",
            "Epoch : 603, val loss : 0.36320936679840093\n",
            "\n",
            "Epoch : 604, train loss : 0.3508716445077549\n",
            "Epoch : 604, val loss : 0.3664977268168801\n",
            "\n",
            "Epoch : 605, train loss : 0.3447321583827335\n",
            "Epoch : 605, val loss : 0.3591242887471852\n",
            "\n",
            "Epoch : 606, train loss : 0.3427652385198709\n",
            "Epoch : 606, val loss : 0.36681463687043453\n",
            "\n",
            "Epoch : 607, train loss : 0.3419720964901377\n",
            "Epoch : 607, val loss : 0.3790946367539858\n",
            "\n",
            "Epoch : 608, train loss : 0.34159376684463383\n",
            "Epoch : 608, val loss : 0.35767529983269536\n",
            "\n",
            "Epoch : 609, train loss : 0.3433899487509872\n",
            "Epoch : 609, val loss : 0.3640332363153758\n",
            "\n",
            "Epoch : 610, train loss : 0.34088545152635297\n",
            "Epoch : 610, val loss : 0.37240380205606155\n",
            "\n",
            "Epoch : 611, train loss : 0.34701074280522093\n",
            "Epoch : 611, val loss : 0.36252597915498835\n",
            "\n",
            "Epoch : 612, train loss : 0.34395520497452126\n",
            "Epoch : 612, val loss : 0.36306051204079076\n",
            "\n",
            "Epoch : 613, train loss : 0.34594187853914316\n",
            "Epoch : 613, val loss : 0.3634150451735446\n",
            "\n",
            "Epoch : 614, train loss : 0.34449333033778445\n",
            "Epoch : 614, val loss : 0.3596220432143462\n",
            "\n",
            "Epoch : 615, train loss : 0.3398918396595751\n",
            "Epoch : 615, val loss : 0.3711486050957128\n",
            "\n",
            "Epoch : 616, train loss : 0.34477026020035584\n",
            "Epoch : 616, val loss : 0.36674298346042633\n",
            "\n",
            "Epoch : 617, train loss : 0.3401501078497281\n",
            "Epoch : 617, val loss : 0.35633260639090286\n",
            "\n",
            "Epoch : 618, train loss : 0.33999660132509285\n",
            "Epoch : 618, val loss : 0.3578780136610332\n",
            "\n",
            "Epoch : 619, train loss : 0.33935039016333507\n",
            "Epoch : 619, val loss : 0.3629096213139985\n",
            "\n",
            "Epoch : 620, train loss : 0.34118841696869245\n",
            "Epoch : 620, val loss : 0.37133720987721497\n",
            "\n",
            "Epoch : 621, train loss : 0.3388620108366012\n",
            "Epoch : 621, val loss : 0.3809683322906494\n",
            "\n",
            "Epoch : 622, train loss : 0.34062534534569955\n",
            "Epoch : 622, val loss : 0.3683647756513797\n",
            "\n",
            "Epoch : 623, train loss : 0.3408779361934372\n",
            "Epoch : 623, val loss : 0.3720235361864692\n",
            "\n",
            "Epoch : 624, train loss : 0.34368956974058423\n",
            "Epoch : 624, val loss : 0.36715427982179744\n",
            "\n",
            "Epoch : 625, train loss : 0.3379792751687945\n",
            "Epoch : 625, val loss : 0.3722233333085712\n",
            "\n",
            "Epoch : 626, train loss : 0.3387872296752351\n",
            "Epoch : 626, val loss : 0.3591782874182651\n",
            "\n",
            "Epoch : 627, train loss : 0.3363799582828177\n",
            "Epoch : 627, val loss : 0.36121002545482234\n",
            "\n",
            "Epoch : 628, train loss : 0.34020986195766567\n",
            "Epoch : 628, val loss : 0.35872391179988256\n",
            "\n",
            "Epoch : 629, train loss : 0.33562814791997253\n",
            "Epoch : 629, val loss : 0.3517429812958366\n",
            "\n",
            "Epoch : 630, train loss : 0.33631580995790855\n",
            "Epoch : 630, val loss : 0.35947264338794505\n",
            "\n",
            "Epoch : 631, train loss : 0.33818339443568035\n",
            "Epoch : 631, val loss : 0.36257550120353704\n",
            "\n",
            "Epoch : 632, train loss : 0.33595350917541617\n",
            "Epoch : 632, val loss : 0.36099797487258917\n",
            "\n",
            "Epoch : 633, train loss : 0.33563062702164503\n",
            "Epoch : 633, val loss : 0.3589730921544527\n",
            "\n",
            "Epoch : 634, train loss : 0.3389863019639798\n",
            "Epoch : 634, val loss : 0.3722330190633473\n",
            "\n",
            "Epoch : 635, train loss : 0.3373897393544514\n",
            "Epoch : 635, val loss : 0.35792865094385645\n",
            "\n",
            "Epoch : 636, train loss : 0.33475183456233065\n",
            "Epoch : 636, val loss : 0.3564381662168001\n",
            "\n",
            "Epoch : 637, train loss : 0.33594314844319306\n",
            "Epoch : 637, val loss : 0.36361799114628834\n",
            "\n",
            "Epoch : 638, train loss : 0.33391169752135436\n",
            "Epoch : 638, val loss : 0.3808397682089554\n",
            "\n",
            "Epoch : 639, train loss : 0.33423019837249396\n",
            "Epoch : 639, val loss : 0.35691789184746\n",
            "\n",
            "Epoch : 640, train loss : 0.3358085092270011\n",
            "Epoch : 640, val loss : 0.3584374808951428\n",
            "\n",
            "Epoch : 641, train loss : 0.3390663407968753\n",
            "Epoch : 641, val loss : 0.3591134148208719\n",
            "\n",
            "Epoch : 642, train loss : 0.33280561462496266\n",
            "Epoch : 642, val loss : 0.36725237651875137\n",
            "\n",
            "Epoch : 643, train loss : 0.3347594831025962\n",
            "Epoch : 643, val loss : 0.35636632222878295\n",
            "\n",
            "Epoch : 644, train loss : 0.33456985417640556\n",
            "Epoch : 644, val loss : 0.35112021079188904\n",
            "\n",
            "Epoch : 645, train loss : 0.3322608587416735\n",
            "Epoch : 645, val loss : 0.3553128783640108\n",
            "\n",
            "Epoch : 646, train loss : 0.33333802006461416\n",
            "Epoch : 646, val loss : 0.3546038811144076\n",
            "\n",
            "Epoch : 647, train loss : 0.33307625244964256\n",
            "Epoch : 647, val loss : 0.3648207171967155\n",
            "\n",
            "Epoch : 648, train loss : 0.33179935307213737\n",
            "Epoch : 648, val loss : 0.3549201002246455\n",
            "\n",
            "Epoch : 649, train loss : 0.3320290198831849\n",
            "Epoch : 649, val loss : 0.35146016039346395\n",
            "\n",
            "Epoch : 650, train loss : 0.3308550254865128\n",
            "Epoch : 650, val loss : 0.35367937307608754\n",
            "\n",
            "Epoch : 651, train loss : 0.33476193637558904\n",
            "Epoch : 651, val loss : 0.35323824380573476\n",
            "\n",
            "Epoch : 652, train loss : 0.3316972074183553\n",
            "Epoch : 652, val loss : 0.3540246596461849\n",
            "\n",
            "Epoch : 653, train loss : 0.3312621831893921\n",
            "Epoch : 653, val loss : 0.3508904780212201\n",
            "\n",
            "Epoch : 654, train loss : 0.3305926320227709\n",
            "Epoch : 654, val loss : 0.3439069829489056\n",
            "\n",
            "Epoch : 655, train loss : 0.3301560823664521\n",
            "Epoch : 655, val loss : 0.3622035682201385\n",
            "\n",
            "Epoch : 656, train loss : 0.33145059283935685\n",
            "Epoch : 656, val loss : 0.3474474941429338\n",
            "\n",
            "Epoch : 657, train loss : 0.329517034147725\n",
            "Epoch : 657, val loss : 0.359255535037894\n",
            "\n",
            "Epoch : 658, train loss : 0.3313691522135879\n",
            "Epoch : 658, val loss : 0.35035183868910147\n",
            "\n",
            "Epoch : 659, train loss : 0.33970533934506497\n",
            "Epoch : 659, val loss : 0.36089063475006505\n",
            "\n",
            "Epoch : 660, train loss : 0.3282424023657133\n",
            "Epoch : 660, val loss : 0.3521515899582913\n",
            "\n",
            "Epoch : 661, train loss : 0.3285138132897291\n",
            "Epoch : 661, val loss : 0.36379914848428024\n",
            "\n",
            "Epoch : 662, train loss : 0.3289857783100821\n",
            "Epoch : 662, val loss : 0.35047495835705805\n",
            "\n",
            "Epoch : 663, train loss : 0.33459136296402336\n",
            "Epoch : 663, val loss : 0.34956374764442444\n",
            "\n",
            "Epoch : 664, train loss : 0.32908141324014367\n",
            "Epoch : 664, val loss : 0.3588474188980303\n",
            "\n",
            "Epoch : 665, train loss : 0.32764620496468105\n",
            "Epoch : 665, val loss : 0.35446560304415853\n",
            "\n",
            "Epoch : 666, train loss : 0.331273044329701\n",
            "Epoch : 666, val loss : 0.34670974078931305\n",
            "\n",
            "Epoch : 667, train loss : 0.3279930440765438\n",
            "Epoch : 667, val loss : 0.3622221084017503\n",
            "\n",
            "Epoch : 668, train loss : 0.3269891009186254\n",
            "Epoch : 668, val loss : 0.35779967119819234\n",
            "\n",
            "Epoch : 669, train loss : 0.3269946562521388\n",
            "Epoch : 669, val loss : 0.3504961023205204\n",
            "\n",
            "Epoch : 670, train loss : 0.32666174525564373\n",
            "Epoch : 670, val loss : 0.347655588859006\n",
            "\n",
            "Epoch : 671, train loss : 0.326761617172848\n",
            "Epoch : 671, val loss : 0.3518808091941632\n",
            "\n",
            "Epoch : 672, train loss : 0.32818735287044976\n",
            "Epoch : 672, val loss : 0.35095073831708806\n",
            "\n",
            "Epoch : 673, train loss : 0.3277511228214609\n",
            "Epoch : 673, val loss : 0.3457797357910558\n",
            "\n",
            "Epoch : 674, train loss : 0.3380937107584692\n",
            "Epoch : 674, val loss : 0.3463923217434632\n",
            "\n",
            "Epoch : 675, train loss : 0.32550471018661165\n",
            "Epoch : 675, val loss : 0.34529211097641993\n",
            "\n",
            "Epoch : 676, train loss : 0.3260476102431614\n",
            "Epoch : 676, val loss : 0.34654044163854497\n",
            "\n",
            "Epoch : 677, train loss : 0.3243053697275392\n",
            "Epoch : 677, val loss : 0.35073017289764\n",
            "\n",
            "Epoch : 678, train loss : 0.32443389549399876\n",
            "Epoch : 678, val loss : 0.3502491265535354\n",
            "\n",
            "Epoch : 679, train loss : 0.3270017610354857\n",
            "Epoch : 679, val loss : 0.36852398906883443\n",
            "\n",
            "Epoch : 680, train loss : 0.325644351677461\n",
            "Epoch : 680, val loss : 0.35193875432014465\n",
            "\n",
            "Epoch : 681, train loss : 0.32733962860974386\n",
            "Epoch : 681, val loss : 0.34105398858848374\n",
            "\n",
            "Epoch : 682, train loss : 0.32888522401000514\n",
            "Epoch : 682, val loss : 0.3471068559508575\n",
            "\n",
            "Epoch : 683, train loss : 0.3257037870811695\n",
            "Epoch : 683, val loss : 0.3486323223302239\n",
            "\n",
            "Epoch : 684, train loss : 0.3268293273268323\n",
            "Epoch : 684, val loss : 0.3440839883528258\n",
            "\n",
            "Epoch : 685, train loss : 0.32257928703770483\n",
            "Epoch : 685, val loss : 0.3561835508597525\n",
            "\n",
            "Epoch : 686, train loss : 0.32411317373767073\n",
            "Epoch : 686, val loss : 0.35118859692623744\n",
            "\n",
            "Epoch : 687, train loss : 0.3235175666483966\n",
            "Epoch : 687, val loss : 0.3575409329251239\n",
            "\n",
            "Epoch : 688, train loss : 0.32525882368738\n",
            "Epoch : 688, val loss : 0.34246006137446355\n",
            "\n",
            "Epoch : 689, train loss : 0.32285263330647457\n",
            "Epoch : 689, val loss : 0.3401942347225391\n",
            "\n",
            "Epoch : 690, train loss : 0.321739788217978\n",
            "Epoch : 690, val loss : 0.36144477913254186\n",
            "\n",
            "Epoch : 691, train loss : 0.3226245403289796\n",
            "Epoch : 691, val loss : 0.34536568898903686\n",
            "\n",
            "Epoch : 692, train loss : 0.32226713516495437\n",
            "Epoch : 692, val loss : 0.3432356856371227\n",
            "\n",
            "Epoch : 693, train loss : 0.32350525440591743\n",
            "Epoch : 693, val loss : 0.3401800480328109\n",
            "\n",
            "Epoch : 694, train loss : 0.3204266572540457\n",
            "Epoch : 694, val loss : 0.34816098605331625\n",
            "\n",
            "Epoch : 695, train loss : 0.32392814619974647\n",
            "Epoch : 695, val loss : 0.34375063055439997\n",
            "\n",
            "Epoch : 696, train loss : 0.3269061559077465\n",
            "Epoch : 696, val loss : 0.35204903389278214\n",
            "\n",
            "Epoch : 697, train loss : 0.32200964111270325\n",
            "Epoch : 697, val loss : 0.34386647531860753\n",
            "\n",
            "Epoch : 698, train loss : 0.3202312958059889\n",
            "Epoch : 698, val loss : 0.3500818312168122\n",
            "\n",
            "Epoch : 699, train loss : 0.32048072291143015\n",
            "Epoch : 699, val loss : 0.3430856121213812\n",
            "\n",
            "Epoch : 700, train loss : 0.3205039184201848\n",
            "Epoch : 700, val loss : 0.33998997352625193\n",
            "\n",
            "Epoch : 701, train loss : 0.31947699622674425\n",
            "Epoch : 701, val loss : 0.3457417346929249\n",
            "\n",
            "Epoch : 702, train loss : 0.31964244508382034\n",
            "Epoch : 702, val loss : 0.34875166965158366\n",
            "\n",
            "Epoch : 703, train loss : 0.3220777936053999\n",
            "Epoch : 703, val loss : 0.33520988884725067\n",
            "\n",
            "Epoch : 704, train loss : 0.3192405045032501\n",
            "Epoch : 704, val loss : 0.3568359534991415\n",
            "\n",
            "Epoch : 705, train loss : 0.3250499980016187\n",
            "Epoch : 705, val loss : 0.3417044736837086\n",
            "\n",
            "Epoch : 706, train loss : 0.32117977431326183\n",
            "Epoch : 706, val loss : 0.3438731510388224\n",
            "\n",
            "Epoch : 707, train loss : 0.317500108119213\n",
            "Epoch : 707, val loss : 0.33764060312195826\n",
            "\n",
            "Epoch : 708, train loss : 0.31745440788341284\n",
            "Epoch : 708, val loss : 0.3628723017479244\n",
            "\n",
            "Epoch : 709, train loss : 0.3180687721028472\n",
            "Epoch : 709, val loss : 0.3384412762365843\n",
            "\n",
            "Epoch : 710, train loss : 0.31703706257271036\n",
            "Epoch : 710, val loss : 0.35465948205245174\n",
            "\n",
            "Epoch : 711, train loss : 0.31767952956936557\n",
            "Epoch : 711, val loss : 0.33651097275708847\n",
            "\n",
            "Epoch : 712, train loss : 0.3162927694392927\n",
            "Epoch : 712, val loss : 0.34072132800754745\n",
            "\n",
            "Epoch : 713, train loss : 0.3185623792987883\n",
            "Epoch : 713, val loss : 0.3428492930374647\n",
            "\n",
            "Epoch : 714, train loss : 0.320005876187122\n",
            "Epoch : 714, val loss : 0.3489494143347991\n",
            "\n",
            "Epoch : 715, train loss : 0.31670596053202954\n",
            "Epoch : 715, val loss : 0.35492190561796483\n",
            "\n",
            "Epoch : 716, train loss : 0.31590794180378773\n",
            "Epoch : 716, val loss : 0.36869038249316965\n",
            "\n",
            "Epoch : 717, train loss : 0.3156438130320923\n",
            "Epoch : 717, val loss : 0.343403022540243\n",
            "\n",
            "Epoch : 718, train loss : 0.31990190961144194\n",
            "Epoch : 718, val loss : 0.33516703781328705\n",
            "\n",
            "Epoch : 719, train loss : 0.3158944867777102\n",
            "Epoch : 719, val loss : 0.3772430733630531\n",
            "\n",
            "Epoch : 720, train loss : 0.31743001097982604\n",
            "Epoch : 720, val loss : 0.33892279706503214\n",
            "\n",
            "Epoch : 721, train loss : 0.3154403315349061\n",
            "Epoch : 721, val loss : 0.3599670631320852\n",
            "\n",
            "Epoch : 722, train loss : 0.3140780696363159\n",
            "Epoch : 722, val loss : 0.3352684488421992\n",
            "\n",
            "Epoch : 723, train loss : 0.3147731510075656\n",
            "Epoch : 723, val loss : 0.3392835722157829\n",
            "\n",
            "Epoch : 724, train loss : 0.31384082048228307\n",
            "Epoch : 724, val loss : 0.3433397482884558\n",
            "\n",
            "Epoch : 725, train loss : 0.31339743859840175\n",
            "Epoch : 725, val loss : 0.3444936855843192\n",
            "\n",
            "Epoch : 726, train loss : 0.31379271592154634\n",
            "Epoch : 726, val loss : 0.35140408183399\n",
            "\n",
            "Epoch : 727, train loss : 0.3187623464699947\n",
            "Epoch : 727, val loss : 0.3386165531058061\n",
            "\n",
            "Epoch : 728, train loss : 0.31403803157083915\n",
            "Epoch : 728, val loss : 0.3489306286761636\n",
            "\n",
            "Epoch : 729, train loss : 0.31342137772025513\n",
            "Epoch : 729, val loss : 0.3468462911091353\n",
            "\n",
            "Epoch : 730, train loss : 0.31450598375363803\n",
            "Epoch : 730, val loss : 0.34706547150486394\n",
            "\n",
            "Epoch : 731, train loss : 0.31436480265675193\n",
            "Epoch : 731, val loss : 0.3387382783387837\n",
            "\n",
            "Epoch : 732, train loss : 0.31364267332987344\n",
            "Epoch : 732, val loss : 0.3466129538260009\n",
            "\n",
            "Epoch : 733, train loss : 0.31183379509232245\n",
            "Epoch : 733, val loss : 0.3402408165367026\n",
            "\n",
            "Epoch : 734, train loss : 0.31530287085157466\n",
            "Epoch : 734, val loss : 0.33790527048863866\n",
            "\n",
            "Epoch : 735, train loss : 0.3138899287491134\n",
            "Epoch : 735, val loss : 0.3591526001691819\n",
            "\n",
            "Epoch : 736, train loss : 0.31256368277650887\n",
            "Epoch : 736, val loss : 0.32905519322345134\n",
            "\n",
            "Epoch : 737, train loss : 0.31547734927047366\n",
            "Epoch : 737, val loss : 0.33308456916558116\n",
            "\n",
            "Epoch : 738, train loss : 0.31062005776347545\n",
            "Epoch : 738, val loss : 0.33850095303435085\n",
            "\n",
            "Epoch : 739, train loss : 0.31103180249532064\n",
            "Epoch : 739, val loss : 0.3315584988970506\n",
            "\n",
            "Epoch : 740, train loss : 0.3127951789986001\n",
            "Epoch : 740, val loss : 0.3324123479818043\n",
            "\n",
            "Epoch : 741, train loss : 0.31062020144679336\n",
            "Epoch : 741, val loss : 0.3302932177719317\n",
            "\n",
            "Epoch : 742, train loss : 0.3099324007829031\n",
            "Epoch : 742, val loss : 0.33155481282033417\n",
            "\n",
            "Epoch : 743, train loss : 0.3136256786909969\n",
            "Epoch : 743, val loss : 0.3294866336019415\n",
            "\n",
            "Epoch : 744, train loss : 0.31346184873219685\n",
            "Epoch : 744, val loss : 0.3362390869542172\n",
            "\n",
            "Epoch : 745, train loss : 0.31765975446412065\n",
            "Epoch : 745, val loss : 0.33961624926642364\n",
            "\n",
            "Epoch : 746, train loss : 0.3104269105376619\n",
            "Epoch : 746, val loss : 0.33047314066635936\n",
            "\n",
            "Epoch : 747, train loss : 0.31062387128671004\n",
            "Epoch : 747, val loss : 0.3328127923764681\n",
            "\n",
            "Epoch : 748, train loss : 0.3139382942156358\n",
            "Epoch : 748, val loss : 0.3544352085966813\n",
            "\n",
            "Epoch : 749, train loss : 0.30850642648610216\n",
            "Epoch : 749, val loss : 0.33202396097936127\n",
            "\n",
            "Epoch : 750, train loss : 0.3085428437500287\n",
            "Epoch : 750, val loss : 0.3335058367566059\n",
            "\n",
            "Epoch : 751, train loss : 0.3137545989318329\n",
            "Epoch : 751, val loss : 0.35298954499395274\n",
            "\n",
            "Epoch : 752, train loss : 0.3094335565061281\n",
            "Epoch : 752, val loss : 0.32882647137892873\n",
            "\n",
            "Epoch : 753, train loss : 0.3106689877130768\n",
            "Epoch : 753, val loss : 0.33903553611353826\n",
            "\n",
            "Epoch : 754, train loss : 0.3128649456031394\n",
            "Epoch : 754, val loss : 0.3260608613491058\n",
            "\n",
            "Epoch : 755, train loss : 0.3073575426231731\n",
            "Epoch : 755, val loss : 0.32822695296061666\n",
            "\n",
            "Epoch : 756, train loss : 0.31025776312206715\n",
            "Epoch : 756, val loss : 0.34563133120536804\n",
            "\n",
            "Epoch : 757, train loss : 0.3139431516329447\n",
            "Epoch : 757, val loss : 0.3517130817237653\n",
            "\n",
            "Epoch : 758, train loss : 0.30843525863055027\n",
            "Epoch : 758, val loss : 0.3328085220173786\n",
            "\n",
            "Epoch : 759, train loss : 0.3063936117020521\n",
            "Epoch : 759, val loss : 0.3317838601375881\n",
            "\n",
            "Epoch : 760, train loss : 0.307942389900034\n",
            "Epoch : 760, val loss : 0.34155976458599696\n",
            "\n",
            "Epoch : 761, train loss : 0.306957270250176\n",
            "Epoch : 761, val loss : 0.32611739086477376\n",
            "\n",
            "Epoch : 762, train loss : 0.30512331191337466\n",
            "Epoch : 762, val loss : 0.33380687472067383\n",
            "\n",
            "Epoch : 763, train loss : 0.3089524194146648\n",
            "Epoch : 763, val loss : 0.3344629579468778\n",
            "\n",
            "Epoch : 764, train loss : 0.30778052030187664\n",
            "Epoch : 764, val loss : 0.3255027438464918\n",
            "\n",
            "Epoch : 765, train loss : 0.30585291873325\n",
            "Epoch : 765, val loss : 0.3251084246133503\n",
            "\n",
            "Epoch : 766, train loss : 0.30581466826525605\n",
            "Epoch : 766, val loss : 0.3287548618881326\n",
            "\n",
            "Epoch : 767, train loss : 0.3067374189694724\n",
            "Epoch : 767, val loss : 0.32931928415047496\n",
            "\n",
            "Epoch : 768, train loss : 0.30597561363017933\n",
            "Epoch : 768, val loss : 0.3289336562156678\n",
            "\n",
            "Epoch : 769, train loss : 0.3050559122905589\n",
            "Epoch : 769, val loss : 0.3322954554306834\n",
            "\n",
            "Epoch : 770, train loss : 0.3066381347901892\n",
            "Epoch : 770, val loss : 0.3289062121981069\n",
            "\n",
            "Epoch : 771, train loss : 0.3033295519424208\n",
            "Epoch : 771, val loss : 0.3346626601721111\n",
            "\n",
            "Epoch : 772, train loss : 0.30407334719643453\n",
            "Epoch : 772, val loss : 0.3241551024349113\n",
            "\n",
            "Epoch : 773, train loss : 0.3021450040015309\n",
            "Epoch : 773, val loss : 0.32958149204128667\n",
            "\n",
            "Epoch : 774, train loss : 0.30405016640822097\n",
            "Epoch : 774, val loss : 0.33104481116721507\n",
            "\n",
            "Epoch : 775, train loss : 0.3036118136210877\n",
            "Epoch : 775, val loss : 0.3234018244241413\n",
            "\n",
            "Epoch : 776, train loss : 0.3033198311473384\n",
            "Epoch : 776, val loss : 0.3250679358055717\n",
            "\n",
            "Epoch : 777, train loss : 0.3023004706158783\n",
            "Epoch : 777, val loss : 0.3271845861485131\n",
            "\n",
            "Epoch : 778, train loss : 0.30276398694876466\n",
            "Epoch : 778, val loss : 0.322998506458182\n",
            "\n",
            "Epoch : 779, train loss : 0.30187715409380006\n",
            "Epoch : 779, val loss : 0.33360944139330007\n",
            "\n",
            "Epoch : 780, train loss : 0.30289722152731635\n",
            "Epoch : 780, val loss : 0.32997641986922205\n",
            "\n",
            "Epoch : 781, train loss : 0.3023589265165907\n",
            "Epoch : 781, val loss : 0.3337122892078601\n",
            "\n",
            "Epoch : 782, train loss : 0.302731800530896\n",
            "Epoch : 782, val loss : 0.329561322927475\n",
            "\n",
            "Epoch : 783, train loss : 0.3021377663720736\n",
            "Epoch : 783, val loss : 0.3382883652260429\n",
            "\n",
            "Epoch : 784, train loss : 0.30310232305165485\n",
            "Epoch : 784, val loss : 0.341508706149302\n",
            "\n",
            "Epoch : 785, train loss : 0.30130002390254657\n",
            "Epoch : 785, val loss : 0.3367707086236853\n",
            "\n",
            "Epoch : 786, train loss : 0.3025188831668911\n",
            "Epoch : 786, val loss : 0.321932097014628\n",
            "\n",
            "Epoch : 787, train loss : 0.30186640484766536\n",
            "Epoch : 787, val loss : 0.3348166244594674\n",
            "\n",
            "Epoch : 788, train loss : 0.3014217495015173\n",
            "Epoch : 788, val loss : 0.3286242163495014\n",
            "\n",
            "Epoch : 789, train loss : 0.2997051608381849\n",
            "Epoch : 789, val loss : 0.33020958147550883\n",
            "\n",
            "Epoch : 790, train loss : 0.3011822900085737\n",
            "Epoch : 790, val loss : 0.3264106982632687\n",
            "\n",
            "Epoch : 791, train loss : 0.3011147619196865\n",
            "Epoch : 791, val loss : 0.3304969040971053\n",
            "\n",
            "Epoch : 792, train loss : 0.3022832425254765\n",
            "Epoch : 792, val loss : 0.31974072283820104\n",
            "\n",
            "Epoch : 793, train loss : 0.29915527502695727\n",
            "Epoch : 793, val loss : 0.32223169270314667\n",
            "\n",
            "Epoch : 794, train loss : 0.3003926644722622\n",
            "Epoch : 794, val loss : 0.3252002079235881\n",
            "\n",
            "Epoch : 795, train loss : 0.30211886691324635\n",
            "Epoch : 795, val loss : 0.3276990683455216\n",
            "\n",
            "Epoch : 796, train loss : 0.3065243849248596\n",
            "Epoch : 796, val loss : 0.3218930520509419\n",
            "\n",
            "Epoch : 797, train loss : 0.3025616091309171\n",
            "Epoch : 797, val loss : 0.32505718187281957\n",
            "\n",
            "Epoch : 798, train loss : 0.30485075969587666\n",
            "Epoch : 798, val loss : 0.32255936061081136\n",
            "\n",
            "Epoch : 799, train loss : 0.30012514654434097\n",
            "Epoch : 799, val loss : 0.33997180038376856\n",
            "\n",
            "Epoch : 800, train loss : 0.29799463965676054\n",
            "Epoch : 800, val loss : 0.3222736081010418\n",
            "0: 2.29764986038208 / 165: 0.013925150668982304\n",
            "1: 1.0255546569824219 / 165: 0.02014063343857274\n",
            "2: 1.3399827480316162 / 165: 0.028261741002400715\n",
            "3: 0.963532567024231 / 165: 0.03410133231769909\n",
            "4: 2.9756054878234863 / 165: 0.05213530497117476\n",
            "5: 1.3677116632461548 / 165: 0.060424466566606\n",
            "6: 2.8737330436706543 / 165: 0.07784103046764027\n",
            "7: 2.8770527839660645 / 165: 0.09527771400682854\n",
            "8: 1.7153124809265137 / 165: 0.10567354722456498\n",
            "9: 2.101774215698242 / 165: 0.1184115727742513\n",
            "10: 1.007751226425171 / 165: 0.12451915596470688\n",
            "11: 1.657142996788025 / 165: 0.13456244685433127\n",
            "12: 2.128298282623291 / 165: 0.14746122432477546\n",
            "13: 1.7692196369171143 / 165: 0.15818376757881858\n",
            "14: 3.4558138847351074 / 165: 0.17912809415297073\n",
            "15: 1.6546330451965332 / 165: 0.1891561732147679\n",
            "16: 1.7986137866973877 / 165: 0.2000568628311157\n",
            "17: 1.3536427021026611 / 165: 0.20826075799537425\n",
            "18: 1.6623554229736328 / 165: 0.2183356393467296\n",
            "19: 0.47087982296943665 / 165: 0.22118945645563526\n",
            "20: 2.1006743907928467 / 165: 0.23392081639983434\n",
            "21: 1.0472627878189087 / 165: 0.2402678635987368\n",
            "22: 1.210603952407837 / 165: 0.2476048572496934\n",
            "23: 1.6032214164733887 / 165: 0.25732135068286544\n",
            "24: 1.9156874418258667 / 165: 0.2689315776030222\n",
            "25: 1.9889148473739624 / 165: 0.28098560698104624\n",
            "26: 0.5115382075309753 / 165: 0.28408583854184005\n",
            "27: 1.14266836643219 / 165: 0.2910111013687018\n",
            "28: 1.2066400051116943 / 165: 0.29832407109665143\n",
            "29: 0.847241222858429 / 165: 0.3034588663867025\n",
            "30: 1.5665478706359863 / 165: 0.31295309590570847\n",
            "31: 1.9639614820480347 / 165: 0.32485589276660565\n",
            "32: 2.04845929145813 / 165: 0.33727079756332157\n",
            "33: 1.6486173868179321 / 165: 0.3472624180894909\n",
            "34: 1.6824493408203125 / 165: 0.35745908076112914\n",
            "35: 2.4142584800720215 / 165: 0.3720909503373232\n",
            "36: 1.0765385627746582 / 165: 0.37861542647535146\n",
            "37: 1.6040420532226562 / 165: 0.38833689346457967\n",
            "38: 1.929805874824524 / 165: 0.40003268664533437\n",
            "39: 1.728689193725586 / 165: 0.4105095908497319\n",
            "40: 1.379992961883545 / 165: 0.418873184558117\n",
            "41: 1.1094720363616943 / 165: 0.42559725750576366\n",
            "42: 1.9481103420257568 / 165: 0.4374039868513743\n",
            "43: 1.1852874755859375 / 165: 0.4445875473094709\n",
            "44: 1.9510711431503296 / 165: 0.45641222090432143\n",
            "45: 2.3424837589263916 / 165: 0.470609092170542\n",
            "46: 1.1310334205627441 / 165: 0.47746384017395255\n",
            "47: 1.1527284383773804 / 165: 0.48445007313381544\n",
            "48: 1.4891712665557861 / 165: 0.49347535353718386\n",
            "49: 1.095754623413086 / 165: 0.5001162906487783\n",
            "50: 1.194312572479248 / 165: 0.507354548663804\n",
            "51: 0.7546471357345581 / 165: 0.5119281676682559\n",
            "52: 1.5528087615966797 / 165: 0.521339129859751\n",
            "53: 0.472267210483551 / 165: 0.5242013553778331\n",
            "54: 1.4738905429840088 / 165: 0.533134025335312\n",
            "55: 1.1874597072601318 / 165: 0.5403307508338582\n",
            "56: 1.0666216611862183 / 165: 0.546795124538017\n",
            "57: 1.2069416046142578 / 165: 0.5541099221417398\n",
            "58: 0.7839201092720032 / 165: 0.5588609531070247\n",
            "59: 0.3095107078552246 / 165: 0.5607367755788745\n",
            "60: 1.1546697616577148 / 165: 0.5677347741343758\n",
            "61: 2.071537971496582 / 165: 0.5802895497192035\n",
            "62: 1.3114538192749023 / 165: 0.588237754684506\n",
            "63: 1.056289792060852 / 165: 0.5946395110000263\n",
            "64: 1.7425270080566406 / 165: 0.6052002807458241\n",
            "65: 1.3673059940338135 / 165: 0.6134869837399685\n",
            "66: 1.605576992034912 / 165: 0.6232177533886649\n",
            "67: 1.245309591293335 / 165: 0.6307650842449881\n",
            "68: 1.7202452421188354 / 165: 0.6411908129851023\n",
            "69: 1.1027452945709229 / 165: 0.6478741178006836\n",
            "70: 1.096505880355835 / 165: 0.6545196079846584\n",
            "71: 1.7967493534088135 / 165: 0.6654089980053178\n",
            "72: 0.09827429801225662 / 165: 0.6660045998114528\n",
            "73: 1.351752758026123 / 165: 0.6741970407691869\n",
            "74: 1.6963990926742554 / 165: 0.684478247391455\n",
            "75: 1.0398046970367432 / 165: 0.6907800940401626\n",
            "76: 1.3323482275009155 / 165: 0.6988549317825924\n",
            "77: 1.1334607601165771 / 165: 0.7057243909348141\n",
            "78: 1.598326325416565 / 165: 0.71541121714946\n",
            "79: 0.6991053223609924 / 165: 0.719648219103163\n",
            "80: 0.680080771446228 / 165: 0.7237699207482917\n",
            "81: 0.7683960795402527 / 165: 0.7284268666848993\n",
            "82: 0.7739309072494507 / 165: 0.7331173570318656\n",
            "83: 0.6520057916641235 / 165: 0.7370689072843755\n",
            "84: 1.8117597103118896 / 165: 0.7480492691650535\n",
            "85: 1.6130437850952148 / 165: 0.7578252921050246\n",
            "86: 1.5122158527374268 / 165: 0.7669902366670696\n",
            "87: 1.6902837753295898 / 165: 0.7772343807599762\n",
            "88: 1.7937813997268677 / 165: 0.7881057831825633\n",
            "89: 1.353985071182251 / 165: 0.7963117533109406\n",
            "90: 1.851879596710205 / 165: 0.8075352660182752\n",
            "91: 0.5447229146957397 / 165: 0.8108366170164312\n",
            "92: 1.5810797214508057 / 165: 0.8204189183585573\n",
            "93: 0.9996039867401123 / 165: 0.8264771243388004\n",
            "94: 1.4859397411346436 / 165: 0.8354828197396164\n",
            "95: 1.0592554807662964 / 165: 0.8419025499260787\n",
            "96: 1.5824055671691895 / 165: 0.8514928866968011\n",
            "97: 1.2126240730285645 / 165: 0.8588421235030348\n",
            "98: 1.4535105228424072 / 165: 0.8676512781869282\n",
            "99: 0.38899511098861694 / 165: 0.870008824314132\n",
            "100: 1.5354723930358887 / 165: 0.8793147176052586\n",
            "101: 0.9830388426780701 / 165: 0.8852725287730044\n",
            "102: 0.862764835357666 / 165: 0.8905014065630509\n",
            "103: 1.281245470046997 / 165: 0.8982665306239418\n",
            "104: 2.0303192138671875 / 165: 0.9105714955564702\n",
            "105: 1.4428367614746094 / 165: 0.9193159607775284\n",
            "106: 1.6967695951461792 / 165: 0.9295994128693235\n",
            "107: 1.186506986618042 / 165: 0.9367903643033721\n",
            "108: 1.3902727365493774 / 165: 0.9452162596763987\n",
            "109: 1.6634634733200073 / 165: 0.9552978564843987\n",
            "110: 1.8262897729873657 / 165: 0.9663662793509887\n",
            "111: 0.9130582809448242 / 165: 0.9718999659021695\n",
            "112: 1.0858561992645264 / 165: 0.9784809125643787\n",
            "113: 1.2732980251312256 / 165: 0.9861978702924468\n",
            "114: 1.1393263339996338 / 165: 0.993102878377293\n",
            "115: 1.1690560579299927 / 165: 1.0001880666071719\n",
            "116: 1.1728419065475464 / 165: 1.0072961993741267\n",
            "117: 1.46599543094635 / 165: 1.016181020167741\n",
            "118: 1.0991257429122925 / 165: 1.0228423883066033\n",
            "119: 1.186773419380188 / 165: 1.0300349544846652\n",
            "120: 1.4774166345596313 / 165: 1.0389889946941175\n",
            "121: 1.5540486574172974 / 165: 1.0484074714057374\n",
            "122: 1.1212482452392578 / 165: 1.0552029153162783\n",
            "123: 0.8888202905654907 / 165: 1.060589704956069\n",
            "124: 1.2180657386779785 / 165: 1.0679719215541175\n",
            "125: 0.9534488916397095 / 165: 1.0737503996852673\n",
            "126: 0.6333231329917908 / 165: 1.0775887217033995\n",
            "127: 1.0602424144744873 / 165: 1.0840144333062751\n",
            "128: 0.967369556427002 / 165: 1.0898772791028024\n",
            "129: 0.7348636388778687 / 165: 1.0943309981263047\n",
            "130: 1.3889362812042236 / 165: 1.1027487937699667\n",
            "131: 0.8339869976043701 / 165: 1.1078032604221144\n",
            "132: 1.1913530826568604 / 165: 1.1150235821351864\n",
            "133: 1.1966094970703125 / 165: 1.1222757609053096\n",
            "134: 0.844639778137207 / 165: 1.1273947898637169\n",
            "135: 1.0877201557159424 / 165: 1.1339870332316924\n",
            "136: 1.0002830028533936 / 165: 1.1400493544611068\n",
            "137: 1.508336067199707 / 165: 1.149190785171408\n",
            "138: 1.0698903799057007 / 165: 1.1556749692920487\n",
            "139: 0.7249151468276978 / 165: 1.1600683944243377\n",
            "140: 0.42369091510772705 / 165: 1.1626362181522634\n",
            "141: 0.9695026874542236 / 165: 1.1685119920156224\n",
            "142: 1.0051581859588623 / 165: 1.1746038598093125\n",
            "143: 0.9791191816329956 / 165: 1.180537915455573\n",
            "144: 0.7249858379364014 / 165: 1.184931769018824\n",
            "145: 0.607750415802002 / 165: 1.1886151048721694\n",
            "146: 1.106430172920227 / 165: 1.195320742283807\n",
            "147: 0.9489538669586182 / 165: 1.201071977841132\n",
            "148: 1.2547425031661987 / 165: 1.2086764778603212\n",
            "149: 0.787688672542572 / 165: 1.2134503486030035\n",
            "150: 0.5680036544799805 / 165: 1.2168927949937913\n",
            "151: 0.7716091871261597 / 165: 1.2215692143097074\n",
            "152: 0.8898584842681885 / 165: 1.226962296032545\n",
            "153: 0.3741539716720581 / 165: 1.2292298958608605\n",
            "154: 0.919282853603363 / 165: 1.2348013070948203\n",
            "155: 0.5294367074966431 / 165: 1.2380100144129818\n",
            "156: 1.1013106107711792 / 165: 1.2446846241752314\n",
            "157: 0.9495881795883179 / 165: 1.2504397040515243\n",
            "158: 1.2136815786361694 / 165: 1.2577953499826526\n",
            "159: 0.7646907567977905 / 165: 1.2624298394177909\n",
            "160: 1.1991652250289917 / 165: 1.2696975074482697\n",
            "161: 0.6026747822761536 / 165: 1.273350081886307\n",
            "162: 1.2673412561416626 / 165: 1.2810309379841354\n",
            "163: 0.5380626916885376 / 165: 1.2842919239943689\n",
            "164: 0.2192230224609375 / 165: 1.28562054837292\n",
            "\n",
            "Epoch : 1, train loss : 1.28562054837292\n",
            "Epoch : 1, val loss : 0.9197952339523714\n",
            "\n",
            "Epoch : 2, train loss : 0.7496023235899033\n",
            "Epoch : 2, val loss : 0.7065080937586332\n",
            "\n",
            "Epoch : 3, train loss : 0.6941931944904903\n",
            "Epoch : 3, val loss : 0.6941115824799788\n",
            "\n",
            "Epoch : 4, train loss : 0.6923791072585366\n",
            "Epoch : 4, val loss : 0.6941948564429034\n",
            "\n",
            "Epoch : 5, train loss : 0.6924773938728099\n",
            "Epoch : 5, val loss : 0.6916935004686056\n",
            "\n",
            "Epoch : 6, train loss : 0.6911192348509122\n",
            "Epoch : 6, val loss : 0.6881401789815803\n",
            "\n",
            "Epoch : 7, train loss : 0.6917724031390569\n",
            "Epoch : 7, val loss : 0.6900936553352757\n",
            "\n",
            "Epoch : 8, train loss : 0.6910994518886913\n",
            "Epoch : 8, val loss : 0.6905135167272468\n",
            "\n",
            "Epoch : 9, train loss : 0.690695643424988\n",
            "Epoch : 9, val loss : 0.6889473011619167\n",
            "\n",
            "Epoch : 10, train loss : 0.6901680722381126\n",
            "Epoch : 10, val loss : 0.6895746055402254\n",
            "\n",
            "Epoch : 11, train loss : 0.6899079124132795\n",
            "Epoch : 11, val loss : 0.6853293337320028\n",
            "\n",
            "Epoch : 12, train loss : 0.6892321033911273\n",
            "Epoch : 12, val loss : 0.689905627777702\n",
            "\n",
            "Epoch : 13, train loss : 0.6889727596080661\n",
            "Epoch : 13, val loss : 0.6873595055780912\n",
            "\n",
            "Epoch : 14, train loss : 0.6886712009256537\n",
            "Epoch : 14, val loss : 0.6864488250330874\n",
            "\n",
            "Epoch : 15, train loss : 0.6875838102716385\n",
            "Epoch : 15, val loss : 0.686994471048054\n",
            "\n",
            "Epoch : 16, train loss : 0.6877607714046133\n",
            "Epoch : 16, val loss : 0.689203779948385\n",
            "\n",
            "Epoch : 17, train loss : 0.686967910419811\n",
            "Epoch : 17, val loss : 0.6862760970467016\n",
            "\n",
            "Epoch : 18, train loss : 0.6864927703684025\n",
            "Epoch : 18, val loss : 0.6863275358551426\n",
            "\n",
            "Epoch : 19, train loss : 0.6863290458014522\n",
            "Epoch : 19, val loss : 0.6841994367147748\n",
            "\n",
            "Epoch : 20, train loss : 0.6856213215625647\n",
            "Epoch : 20, val loss : 0.6897714545852258\n",
            "\n",
            "Epoch : 21, train loss : 0.6853927702614759\n",
            "Epoch : 21, val loss : 0.6862268259650783\n",
            "\n",
            "Epoch : 22, train loss : 0.6851566769860009\n",
            "Epoch : 22, val loss : 0.6837831704240097\n",
            "\n",
            "Epoch : 23, train loss : 0.6845007372624945\n",
            "Epoch : 23, val loss : 0.6871327193159806\n",
            "\n",
            "Epoch : 24, train loss : 0.6850720383904197\n",
            "Epoch : 24, val loss : 0.6800942389588607\n",
            "\n",
            "Epoch : 25, train loss : 0.6831829038533297\n",
            "Epoch : 25, val loss : 0.6789880645902534\n",
            "\n",
            "Epoch : 26, train loss : 0.6831952947558783\n",
            "Epoch : 26, val loss : 0.6834297086063184\n",
            "\n",
            "Epoch : 27, train loss : 0.6844754316590042\n",
            "Epoch : 27, val loss : 0.6812067157343814\n",
            "\n",
            "Epoch : 28, train loss : 0.6820710467569759\n",
            "Epoch : 28, val loss : 0.6823778779883133\n",
            "\n",
            "Epoch : 29, train loss : 0.681510027610894\n",
            "Epoch : 29, val loss : 0.6812352820446617\n",
            "\n",
            "Epoch : 30, train loss : 0.6814985192183297\n",
            "Epoch : 30, val loss : 0.6787559107730265\n",
            "\n",
            "Epoch : 31, train loss : 0.680330648566737\n",
            "Epoch : 31, val loss : 0.6840826743527462\n",
            "\n",
            "Epoch : 32, train loss : 0.6805754965001882\n",
            "Epoch : 32, val loss : 0.6770598574688559\n",
            "\n",
            "Epoch : 33, train loss : 0.6815142494259455\n",
            "Epoch : 33, val loss : 0.6766816973686218\n",
            "\n",
            "Epoch : 34, train loss : 0.680199291489341\n",
            "Epoch : 34, val loss : 0.6787540630290382\n",
            "\n",
            "Epoch : 35, train loss : 0.6795779885667745\n",
            "Epoch : 35, val loss : 0.6807408426937305\n",
            "\n",
            "Epoch : 36, train loss : 0.6790469935446072\n",
            "Epoch : 36, val loss : 0.6760235334697522\n",
            "\n",
            "Epoch : 37, train loss : 0.6785655538241068\n",
            "Epoch : 37, val loss : 0.6777830657206085\n",
            "\n",
            "Epoch : 38, train loss : 0.6788382215933363\n",
            "Epoch : 38, val loss : 0.6780446171760559\n",
            "\n",
            "Epoch : 39, train loss : 0.6779737999944978\n",
            "Epoch : 39, val loss : 0.675736433581302\n",
            "\n",
            "Epoch : 40, train loss : 0.6770614649310257\n",
            "Epoch : 40, val loss : 0.6756039606897454\n",
            "\n",
            "Epoch : 41, train loss : 0.676650495601423\n",
            "Epoch : 41, val loss : 0.6740723911084626\n",
            "\n",
            "Epoch : 42, train loss : 0.6770493637431749\n",
            "Epoch : 42, val loss : 0.6727200934761448\n",
            "\n",
            "Epoch : 43, train loss : 0.6758544705130843\n",
            "Epoch : 43, val loss : 0.6746923170591657\n",
            "\n",
            "Epoch : 44, train loss : 0.6763194445407753\n",
            "Epoch : 44, val loss : 0.6757159138980665\n",
            "\n",
            "Epoch : 45, train loss : 0.6759612231543567\n",
            "Epoch : 45, val loss : 0.6738316322627821\n",
            "\n",
            "Epoch : 46, train loss : 0.6758102081038742\n",
            "Epoch : 46, val loss : 0.6729813061262431\n",
            "\n",
            "Epoch : 47, train loss : 0.6752665212660124\n",
            "Epoch : 47, val loss : 0.6732009931614525\n",
            "\n",
            "Epoch : 48, train loss : 0.6747387716264436\n",
            "Epoch : 48, val loss : 0.6727715354216727\n",
            "\n",
            "Epoch : 49, train loss : 0.6741728446700357\n",
            "Epoch : 49, val loss : 0.6718036908852426\n",
            "\n",
            "Epoch : 50, train loss : 0.6734723120024709\n",
            "Epoch : 50, val loss : 0.6735138046114069\n",
            "\n",
            "Epoch : 51, train loss : 0.6732318155693281\n",
            "Epoch : 51, val loss : 0.6705439937742133\n",
            "\n",
            "Epoch : 52, train loss : 0.6729536753712282\n",
            "Epoch : 52, val loss : 0.6728936590646445\n",
            "\n",
            "Epoch : 53, train loss : 0.6720982201171646\n",
            "Epoch : 53, val loss : 0.6704071791548479\n",
            "\n",
            "Epoch : 54, train loss : 0.6732985510970606\n",
            "Epoch : 54, val loss : 0.6704784568987395\n",
            "\n",
            "Epoch : 55, train loss : 0.6726997527209196\n",
            "Epoch : 55, val loss : 0.6673775691735118\n",
            "\n",
            "Epoch : 56, train loss : 0.6716326449856614\n",
            "Epoch : 56, val loss : 0.6719952194314254\n",
            "\n",
            "Epoch : 57, train loss : 0.6709521557345537\n",
            "Epoch : 57, val loss : 0.6704987130667035\n",
            "\n",
            "Epoch : 58, train loss : 0.6709716580130833\n",
            "Epoch : 58, val loss : 0.6704921691041243\n",
            "\n",
            "Epoch : 59, train loss : 0.671039681001143\n",
            "Epoch : 59, val loss : 0.6687220742827967\n",
            "\n",
            "Epoch : 60, train loss : 0.6702631603587759\n",
            "Epoch : 60, val loss : 0.6636673149309661\n",
            "\n",
            "Epoch : 61, train loss : 0.6697174057816014\n",
            "Epoch : 61, val loss : 0.6684284586655466\n",
            "\n",
            "Epoch : 62, train loss : 0.6694009788108595\n",
            "Epoch : 62, val loss : 0.6669773867255764\n",
            "\n",
            "Epoch : 63, train loss : 0.6694741599487536\n",
            "Epoch : 63, val loss : 0.6657509960626302\n",
            "\n",
            "Epoch : 64, train loss : 0.6686740730748026\n",
            "Epoch : 64, val loss : 0.6674221314881976\n",
            "\n",
            "Epoch : 65, train loss : 0.6684414758826747\n",
            "Epoch : 65, val loss : 0.6662151437056691\n",
            "\n",
            "Epoch : 66, train loss : 0.6676631848017376\n",
            "Epoch : 66, val loss : 0.6678295166868911\n",
            "\n",
            "Epoch : 67, train loss : 0.6678484317028157\n",
            "Epoch : 67, val loss : 0.6692667728976199\n",
            "\n",
            "Epoch : 68, train loss : 0.6681324041250981\n",
            "Epoch : 68, val loss : 0.6674326846474095\n",
            "\n",
            "Epoch : 69, train loss : 0.6665338570421392\n",
            "Epoch : 69, val loss : 0.6667319724434299\n",
            "\n",
            "Epoch : 70, train loss : 0.666455734859814\n",
            "Epoch : 70, val loss : 0.6598503871967918\n",
            "\n",
            "Epoch : 71, train loss : 0.6683173186851267\n",
            "Epoch : 71, val loss : 0.6669701902489914\n",
            "\n",
            "Epoch : 72, train loss : 0.6660419059522228\n",
            "Epoch : 72, val loss : 0.6639865260375173\n",
            "\n",
            "Epoch : 73, train loss : 0.6662408196564876\n",
            "Epoch : 73, val loss : 0.6626862036554437\n",
            "\n",
            "Epoch : 74, train loss : 0.6654915874654599\n",
            "Epoch : 74, val loss : 0.6634735025857624\n",
            "\n",
            "Epoch : 75, train loss : 0.6646227941368567\n",
            "Epoch : 75, val loss : 0.6646918999521355\n",
            "\n",
            "Epoch : 76, train loss : 0.6645694768790038\n",
            "Epoch : 76, val loss : 0.6624670907070762\n",
            "\n",
            "Epoch : 77, train loss : 0.6640783479719448\n",
            "Epoch : 77, val loss : 0.6629755967541745\n",
            "\n",
            "Epoch : 78, train loss : 0.6645246364853598\n",
            "Epoch : 78, val loss : 0.6624833282671476\n",
            "\n",
            "Epoch : 79, train loss : 0.6636983611366964\n",
            "Epoch : 79, val loss : 0.660307786966625\n",
            "\n",
            "Epoch : 80, train loss : 0.6636602398120995\n",
            "Epoch : 80, val loss : 0.6625803709030151\n",
            "\n",
            "Epoch : 81, train loss : 0.6633044878641768\n",
            "Epoch : 81, val loss : 0.6609709921636079\n",
            "\n",
            "Epoch : 82, train loss : 0.662755478512157\n",
            "Epoch : 82, val loss : 0.6652719315729644\n",
            "\n",
            "Epoch : 83, train loss : 0.66264420783881\n",
            "Epoch : 83, val loss : 0.6581027915603237\n",
            "\n",
            "Epoch : 84, train loss : 0.6620510809349294\n",
            "Epoch : 84, val loss : 0.6649219017279776\n",
            "\n",
            "Epoch : 85, train loss : 0.6623279694354892\n",
            "Epoch : 85, val loss : 0.6549813182730423\n",
            "\n",
            "Epoch : 86, train loss : 0.6609091653968346\n",
            "Epoch : 86, val loss : 0.6680906038535269\n",
            "\n",
            "Epoch : 87, train loss : 0.6610377048001144\n",
            "Epoch : 87, val loss : 0.6580863344041926\n",
            "\n",
            "Epoch : 88, train loss : 0.6609127792445101\n",
            "Epoch : 88, val loss : 0.6566424432553742\n",
            "\n",
            "Epoch : 89, train loss : 0.6614637580784885\n",
            "Epoch : 89, val loss : 0.6518486204900239\n",
            "\n",
            "Epoch : 90, train loss : 0.6599102876403115\n",
            "Epoch : 90, val loss : 0.6598670388522901\n",
            "\n",
            "Epoch : 91, train loss : 0.6604520310055129\n",
            "Epoch : 91, val loss : 0.6550138247640509\n",
            "\n",
            "Epoch : 92, train loss : 0.6594695373014968\n",
            "Epoch : 92, val loss : 0.6622100095999868\n",
            "\n",
            "Epoch : 93, train loss : 0.6593579956979463\n",
            "Epoch : 93, val loss : 0.6564437401922126\n",
            "\n",
            "Epoch : 94, train loss : 0.6581999135739873\n",
            "Epoch : 94, val loss : 0.6609235625517995\n",
            "\n",
            "Epoch : 95, train loss : 0.6590077808409027\n",
            "Epoch : 95, val loss : 0.6644097064670764\n",
            "\n",
            "Epoch : 96, train loss : 0.6584444739601832\n",
            "Epoch : 96, val loss : 0.6599091479652807\n",
            "\n",
            "Epoch : 97, train loss : 0.6584609573537655\n",
            "Epoch : 97, val loss : 0.6579456894021286\n",
            "\n",
            "Epoch : 98, train loss : 0.6575279477870825\n",
            "Epoch : 98, val loss : 0.6590127474383304\n",
            "\n",
            "Epoch : 99, train loss : 0.6575313427231527\n",
            "Epoch : 99, val loss : 0.6530744966707732\n",
            "\n",
            "Epoch : 100, train loss : 0.6572855851866988\n",
            "Epoch : 100, val loss : 0.6602856171758553\n",
            "\n",
            "Epoch : 101, train loss : 0.6572401317683136\n",
            "Epoch : 101, val loss : 0.6536595915493213\n",
            "\n",
            "Epoch : 102, train loss : 0.6572259469465775\n",
            "Epoch : 102, val loss : 0.6533067571489435\n",
            "\n",
            "Epoch : 103, train loss : 0.6567221106904925\n",
            "Epoch : 103, val loss : 0.6523634948228536\n",
            "\n",
            "Epoch : 104, train loss : 0.6555305983081009\n",
            "Epoch : 104, val loss : 0.656373795710112\n",
            "\n",
            "Epoch : 105, train loss : 0.6551116141405972\n",
            "Epoch : 105, val loss : 0.6539521687909177\n",
            "\n",
            "Epoch : 106, train loss : 0.656223095185829\n",
            "Epoch : 106, val loss : 0.6508367657661439\n",
            "\n",
            "Epoch : 107, train loss : 0.6550048777551365\n",
            "Epoch : 107, val loss : 0.6514472051670676\n",
            "\n",
            "Epoch : 108, train loss : 0.6549969962148953\n",
            "Epoch : 108, val loss : 0.6518519018825731\n",
            "\n",
            "Epoch : 109, train loss : 0.6542415452725964\n",
            "Epoch : 109, val loss : 0.6546927157201266\n",
            "\n",
            "Epoch : 110, train loss : 0.6540776783769784\n",
            "Epoch : 110, val loss : 0.6527706604254873\n",
            "\n",
            "Epoch : 111, train loss : 0.6539162653865239\n",
            "Epoch : 111, val loss : 0.6536467326314825\n",
            "\n",
            "Epoch : 112, train loss : 0.6532615033063018\n",
            "Epoch : 112, val loss : 0.6539664174381056\n",
            "\n",
            "Epoch : 113, train loss : 0.6527867729013618\n",
            "Epoch : 113, val loss : 0.652505231531043\n",
            "\n",
            "Epoch : 114, train loss : 0.653816627733635\n",
            "Epoch : 114, val loss : 0.6589111999461524\n",
            "\n",
            "Epoch : 115, train loss : 0.6531317902333806\n",
            "Epoch : 115, val loss : 0.653522274996105\n",
            "\n",
            "Epoch : 116, train loss : 0.6527142922083536\n",
            "Epoch : 116, val loss : 0.646409765670174\n",
            "\n",
            "Epoch : 117, train loss : 0.6523705840110775\n",
            "Epoch : 117, val loss : 0.6538858476438022\n",
            "\n",
            "Epoch : 118, train loss : 0.6520336035526157\n",
            "Epoch : 118, val loss : 0.6460426512517428\n",
            "\n",
            "Epoch : 119, train loss : 0.651834938020417\n",
            "Epoch : 119, val loss : 0.6562750810071041\n",
            "\n",
            "Epoch : 120, train loss : 0.6510287841161092\n",
            "Epoch : 120, val loss : 0.6557127456916007\n",
            "\n",
            "Epoch : 121, train loss : 0.650649146600203\n",
            "Epoch : 121, val loss : 0.6484531164169312\n",
            "\n",
            "Epoch : 122, train loss : 0.6514382943962557\n",
            "Epoch : 122, val loss : 0.6430730976556476\n",
            "\n",
            "Epoch : 123, train loss : 0.6505433967619231\n",
            "Epoch : 123, val loss : 0.6505076069580881\n",
            "\n",
            "Epoch : 124, train loss : 0.6498817913460013\n",
            "Epoch : 124, val loss : 0.6486704506372151\n",
            "\n",
            "Epoch : 125, train loss : 0.6495212197303772\n",
            "Epoch : 125, val loss : 0.6470845498536762\n",
            "\n",
            "Epoch : 126, train loss : 0.6500239328904583\n",
            "Epoch : 126, val loss : 0.6493771829103168\n",
            "\n",
            "Epoch : 127, train loss : 0.6494078386913648\n",
            "Epoch : 127, val loss : 0.6393459062827261\n",
            "\n",
            "Epoch : 128, train loss : 0.648650833693418\n",
            "Epoch : 128, val loss : 0.644782514948594\n",
            "\n",
            "Epoch : 129, train loss : 0.6487033489978677\n",
            "Epoch : 129, val loss : 0.6426752492001183\n",
            "\n",
            "Epoch : 130, train loss : 0.6484682549129831\n",
            "Epoch : 130, val loss : 0.6501337132955852\n",
            "\n",
            "Epoch : 131, train loss : 0.648821627732479\n",
            "Epoch : 131, val loss : 0.6467648524987071\n",
            "\n",
            "Epoch : 132, train loss : 0.6476852388092965\n",
            "Epoch : 132, val loss : 0.6475978932882609\n",
            "\n",
            "Epoch : 133, train loss : 0.647713566910137\n",
            "Epoch : 133, val loss : 0.6450292468070985\n",
            "\n",
            "Epoch : 134, train loss : 0.6471377296881243\n",
            "Epoch : 134, val loss : 0.6460882142970436\n",
            "\n",
            "Epoch : 135, train loss : 0.6476592652725449\n",
            "Epoch : 135, val loss : 0.6489895864536888\n",
            "\n",
            "Epoch : 136, train loss : 0.647640649477641\n",
            "Epoch : 136, val loss : 0.6464898084339342\n",
            "\n",
            "Epoch : 137, train loss : 0.6465908931963372\n",
            "Epoch : 137, val loss : 0.6468869133999473\n",
            "\n",
            "Epoch : 138, train loss : 0.6469146461197822\n",
            "Epoch : 138, val loss : 0.6411538845614383\n",
            "\n",
            "Epoch : 139, train loss : 0.646967075810288\n",
            "Epoch : 139, val loss : 0.6485294762410616\n",
            "\n",
            "Epoch : 140, train loss : 0.6453549428419627\n",
            "Epoch : 140, val loss : 0.6437707135551854\n",
            "\n",
            "Epoch : 141, train loss : 0.6462049090501035\n",
            "Epoch : 141, val loss : 0.6408248575110185\n",
            "\n",
            "Epoch : 142, train loss : 0.645278907544685\n",
            "Epoch : 142, val loss : 0.6403447264119199\n",
            "\n",
            "Epoch : 143, train loss : 0.6447874845880449\n",
            "Epoch : 143, val loss : 0.6458324162583603\n",
            "\n",
            "Epoch : 144, train loss : 0.6451151165095244\n",
            "Epoch : 144, val loss : 0.6414556566037629\n",
            "\n",
            "Epoch : 145, train loss : 0.6445135159925984\n",
            "Epoch : 145, val loss : 0.6394833232227124\n",
            "\n",
            "Epoch : 146, train loss : 0.6441637429324062\n",
            "Epoch : 146, val loss : 0.6426630333850257\n",
            "\n",
            "Epoch : 147, train loss : 0.6443244825709951\n",
            "Epoch : 147, val loss : 0.6471331119537355\n",
            "\n",
            "Epoch : 148, train loss : 0.6430003166198726\n",
            "Epoch : 148, val loss : 0.6421175818694266\n",
            "\n",
            "Epoch : 149, train loss : 0.6444063388940063\n",
            "Epoch : 149, val loss : 0.6378463757665533\n",
            "\n",
            "Epoch : 150, train loss : 0.6429321747837642\n",
            "Epoch : 150, val loss : 0.6439779149858575\n",
            "\n",
            "Epoch : 151, train loss : 0.642836184212656\n",
            "Epoch : 151, val loss : 0.6463334936844677\n",
            "\n",
            "Epoch : 152, train loss : 0.6422004284280721\n",
            "Epoch : 152, val loss : 0.636488001597555\n",
            "\n",
            "Epoch : 153, train loss : 0.6421818321401422\n",
            "Epoch : 153, val loss : 0.6398793364826001\n",
            "\n",
            "Epoch : 154, train loss : 0.642369474786701\n",
            "Epoch : 154, val loss : 0.6396605842991879\n",
            "\n",
            "Epoch : 155, train loss : 0.6414317618716847\n",
            "Epoch : 155, val loss : 0.6419199830607365\n",
            "\n",
            "Epoch : 156, train loss : 0.6412587433150321\n",
            "Epoch : 156, val loss : 0.6430671215057374\n",
            "\n",
            "Epoch : 157, train loss : 0.6415366010232407\n",
            "Epoch : 157, val loss : 0.6445916232309843\n",
            "\n",
            "Epoch : 158, train loss : 0.6404590733123549\n",
            "Epoch : 158, val loss : 0.6385485874979119\n",
            "\n",
            "Epoch : 159, train loss : 0.6419467940475001\n",
            "Epoch : 159, val loss : 0.6392672061920166\n",
            "\n",
            "Epoch : 160, train loss : 0.6395304878552758\n",
            "Epoch : 160, val loss : 0.6459318211204127\n",
            "\n",
            "Epoch : 161, train loss : 0.6414587602470861\n",
            "Epoch : 161, val loss : 0.6407044529914855\n",
            "\n",
            "Epoch : 162, train loss : 0.6393822344866664\n",
            "Epoch : 162, val loss : 0.6365726339189629\n",
            "\n",
            "Epoch : 163, train loss : 0.6378097758148653\n",
            "Epoch : 163, val loss : 0.6401927094710501\n",
            "\n",
            "Epoch : 164, train loss : 0.6396118012341586\n",
            "Epoch : 164, val loss : 0.6382961869239809\n",
            "\n",
            "Epoch : 165, train loss : 0.6377965737472875\n",
            "Epoch : 165, val loss : 0.6368929931991978\n",
            "\n",
            "Epoch : 166, train loss : 0.638475274136572\n",
            "Epoch : 166, val loss : 0.6301238160384328\n",
            "\n",
            "Epoch : 167, train loss : 0.6385979121381584\n",
            "Epoch : 167, val loss : 0.6409422065082351\n",
            "\n",
            "Epoch : 168, train loss : 0.6379767732186752\n",
            "Epoch : 168, val loss : 0.6365458181029872\n",
            "\n",
            "Epoch : 169, train loss : 0.6381719202706307\n",
            "Epoch : 169, val loss : 0.6406111968191048\n",
            "\n",
            "Epoch : 170, train loss : 0.6387782981901453\n",
            "Epoch : 170, val loss : 0.6352957643960652\n",
            "\n",
            "Epoch : 171, train loss : 0.637466133363319\n",
            "Epoch : 171, val loss : 0.6342185861185977\n",
            "\n",
            "Epoch : 172, train loss : 0.6376621025981326\n",
            "Epoch : 172, val loss : 0.6342340676408065\n",
            "\n",
            "Epoch : 173, train loss : 0.637297111930269\n",
            "Epoch : 173, val loss : 0.6363118416384649\n",
            "\n",
            "Epoch : 174, train loss : 0.6373338287526914\n",
            "Epoch : 174, val loss : 0.6286219452556812\n",
            "\n",
            "Epoch : 175, train loss : 0.6364036411950085\n",
            "Epoch : 175, val loss : 0.631666876767811\n",
            "\n",
            "Epoch : 176, train loss : 0.6361078800577109\n",
            "Epoch : 176, val loss : 0.6362444946640416\n",
            "\n",
            "Epoch : 177, train loss : 0.6354195988539499\n",
            "Epoch : 177, val loss : 0.6369714987905402\n",
            "\n",
            "Epoch : 178, train loss : 0.6360552607160626\n",
            "Epoch : 178, val loss : 0.6387336944278917\n",
            "\n",
            "Epoch : 179, train loss : 0.6357356085921779\n",
            "Epoch : 179, val loss : 0.6319951289578488\n",
            "\n",
            "Epoch : 180, train loss : 0.6350852738727221\n",
            "Epoch : 180, val loss : 0.635821056993384\n",
            "\n",
            "Epoch : 181, train loss : 0.6350571769656559\n",
            "Epoch : 181, val loss : 0.631617307662964\n",
            "\n",
            "Epoch : 182, train loss : 0.6349927584330239\n",
            "Epoch : 182, val loss : 0.6344146508919565\n",
            "\n",
            "Epoch : 183, train loss : 0.6351655595230333\n",
            "Epoch : 183, val loss : 0.6327135060962877\n",
            "\n",
            "Epoch : 184, train loss : 0.6339316725730891\n",
            "Epoch : 184, val loss : 0.6372625012146799\n",
            "\n",
            "Epoch : 185, train loss : 0.6340256286389901\n",
            "Epoch : 185, val loss : 0.6247332064728988\n",
            "\n",
            "Epoch : 186, train loss : 0.6336486592437279\n",
            "Epoch : 186, val loss : 0.62835609285455\n",
            "\n",
            "Epoch : 187, train loss : 0.6335138555729025\n",
            "Epoch : 187, val loss : 0.6311086667211432\n",
            "\n",
            "Epoch : 188, train loss : 0.6338796073740178\n",
            "Epoch : 188, val loss : 0.6288767300154033\n",
            "\n",
            "Epoch : 189, train loss : 0.6336681459889268\n",
            "Epoch : 189, val loss : 0.6308802083918923\n",
            "\n",
            "Epoch : 190, train loss : 0.632889046091022\n",
            "Epoch : 190, val loss : 0.6318016962001198\n",
            "\n",
            "Epoch : 191, train loss : 0.6323710138147526\n",
            "Epoch : 191, val loss : 0.6334045937186794\n",
            "\n",
            "Epoch : 192, train loss : 0.632299521475127\n",
            "Epoch : 192, val loss : 0.6346683313972072\n",
            "\n",
            "Epoch : 193, train loss : 0.6309091524644331\n",
            "Epoch : 193, val loss : 0.6311770175632677\n",
            "\n",
            "Epoch : 194, train loss : 0.6321660251328441\n",
            "Epoch : 194, val loss : 0.6360068980016206\n",
            "\n",
            "Epoch : 195, train loss : 0.6310457406621992\n",
            "Epoch : 195, val loss : 0.6319286572305779\n",
            "\n",
            "Epoch : 196, train loss : 0.6309999404531539\n",
            "Epoch : 196, val loss : 0.6247550060874535\n",
            "\n",
            "Epoch : 197, train loss : 0.6300023537693601\n",
            "Epoch : 197, val loss : 0.6339893435177051\n",
            "\n",
            "Epoch : 198, train loss : 0.6310547760038663\n",
            "Epoch : 198, val loss : 0.6303208784053199\n",
            "\n",
            "Epoch : 199, train loss : 0.6305745359623075\n",
            "Epoch : 199, val loss : 0.6279102861881256\n",
            "\n",
            "Epoch : 200, train loss : 0.63100002841516\n",
            "Epoch : 200, val loss : 0.6271135838408218\n",
            "\n",
            "Epoch : 201, train loss : 0.6299678647156919\n",
            "Epoch : 201, val loss : 0.6318851207431995\n",
            "\n",
            "Epoch : 202, train loss : 0.6304754856860996\n",
            "Epoch : 202, val loss : 0.625120128455915\n",
            "\n",
            "Epoch : 203, train loss : 0.6300199566465439\n",
            "Epoch : 203, val loss : 0.627612082581771\n",
            "\n",
            "Epoch : 204, train loss : 0.6298398783712674\n",
            "Epoch : 204, val loss : 0.6258549658875715\n",
            "\n",
            "Epoch : 205, train loss : 0.6290202957211117\n",
            "Epoch : 205, val loss : 0.628144825759687\n",
            "\n",
            "Epoch : 206, train loss : 0.6304581056941639\n",
            "Epoch : 206, val loss : 0.6289722982205841\n",
            "\n",
            "Epoch : 207, train loss : 0.6290095061966867\n",
            "Epoch : 207, val loss : 0.6265276450859874\n",
            "\n",
            "Epoch : 208, train loss : 0.6282318286823505\n",
            "Epoch : 208, val loss : 0.6249315832790575\n",
            "\n",
            "Epoch : 209, train loss : 0.6285454284061083\n",
            "Epoch : 209, val loss : 0.6261533341909709\n",
            "\n",
            "Epoch : 210, train loss : 0.6281294595111501\n",
            "Epoch : 210, val loss : 0.6308709759461253\n",
            "\n",
            "Epoch : 211, train loss : 0.6274851166840755\n",
            "Epoch : 211, val loss : 0.6247476088373285\n",
            "\n",
            "Epoch : 212, train loss : 0.6277939171502083\n",
            "Epoch : 212, val loss : 0.6246696396877891\n",
            "\n",
            "Epoch : 213, train loss : 0.6270186435092582\n",
            "Epoch : 213, val loss : 0.6289731013147453\n",
            "\n",
            "Epoch : 214, train loss : 0.626967556910081\n",
            "Epoch : 214, val loss : 0.6214212461521752\n",
            "\n",
            "Epoch : 215, train loss : 0.6270769561782027\n",
            "Epoch : 215, val loss : 0.6197004255495572\n",
            "\n",
            "Epoch : 216, train loss : 0.6265723647493308\n",
            "Epoch : 216, val loss : 0.6276600862804211\n",
            "\n",
            "Epoch : 217, train loss : 0.6282142422415996\n",
            "Epoch : 217, val loss : 0.6198243373318724\n",
            "\n",
            "Epoch : 218, train loss : 0.626669544884653\n",
            "Epoch : 218, val loss : 0.6256792262980814\n",
            "\n",
            "Epoch : 219, train loss : 0.6266036604390001\n",
            "Epoch : 219, val loss : 0.6261621054850126\n",
            "\n",
            "Epoch : 220, train loss : 0.6259226818879445\n",
            "Epoch : 220, val loss : 0.6216313901700471\n",
            "\n",
            "Epoch : 221, train loss : 0.6257424885576424\n",
            "Epoch : 221, val loss : 0.6270574582250494\n",
            "\n",
            "Epoch : 222, train loss : 0.624960146708922\n",
            "Epoch : 222, val loss : 0.6163572167095385\n",
            "\n",
            "Epoch : 223, train loss : 0.6252612861719998\n",
            "Epoch : 223, val loss : 0.6254075796980607\n",
            "\n",
            "Epoch : 224, train loss : 0.6246223751342661\n",
            "Epoch : 224, val loss : 0.6250856895195809\n",
            "\n",
            "Epoch : 225, train loss : 0.6241131150361262\n",
            "Epoch : 225, val loss : 0.6311184544312327\n",
            "\n",
            "Epoch : 226, train loss : 0.6245846974127219\n",
            "Epoch : 226, val loss : 0.6244627801995528\n",
            "\n",
            "Epoch : 227, train loss : 0.6231840451558428\n",
            "Epoch : 227, val loss : 0.6219408010181628\n",
            "\n",
            "Epoch : 228, train loss : 0.623898954644348\n",
            "Epoch : 228, val loss : 0.6244822897409139\n",
            "\n",
            "Epoch : 229, train loss : 0.6232178846995031\n",
            "Epoch : 229, val loss : 0.6153885518249712\n",
            "\n",
            "Epoch : 230, train loss : 0.6235182227510391\n",
            "Epoch : 230, val loss : 0.6111813579735005\n",
            "\n",
            "Epoch : 231, train loss : 0.6235620693726978\n",
            "Epoch : 231, val loss : 0.6235580663931997\n",
            "\n",
            "Epoch : 232, train loss : 0.6226922112883944\n",
            "Epoch : 232, val loss : 0.6246262506434792\n",
            "\n",
            "Epoch : 233, train loss : 0.6218367273157295\n",
            "Epoch : 233, val loss : 0.6206272777758146\n",
            "\n",
            "Epoch : 234, train loss : 0.6231762627760566\n",
            "Epoch : 234, val loss : 0.618864191205878\n",
            "\n",
            "Epoch : 235, train loss : 0.6228382222580185\n",
            "Epoch : 235, val loss : 0.6173852117438066\n",
            "\n",
            "Epoch : 236, train loss : 0.6218813137574628\n",
            "Epoch : 236, val loss : 0.6213335049779793\n",
            "\n",
            "Epoch : 237, train loss : 0.621648586157596\n",
            "Epoch : 237, val loss : 0.6197415903994912\n",
            "\n",
            "Epoch : 238, train loss : 0.6223669019612399\n",
            "Epoch : 238, val loss : 0.6219788852490876\n",
            "\n",
            "Epoch : 239, train loss : 0.6218564129236973\n",
            "Epoch : 239, val loss : 0.6218802270136381\n",
            "\n",
            "Epoch : 240, train loss : 0.6210309902826942\n",
            "Epoch : 240, val loss : 0.6206288714157907\n",
            "\n",
            "Epoch : 241, train loss : 0.621979779185671\n",
            "Epoch : 241, val loss : 0.6225468202641136\n",
            "\n",
            "Epoch : 242, train loss : 0.6215715664805787\n",
            "Epoch : 242, val loss : 0.6212230324745178\n",
            "\n",
            "Epoch : 243, train loss : 0.6202902656612975\n",
            "Epoch : 243, val loss : 0.6305715943637646\n",
            "\n",
            "Epoch : 244, train loss : 0.6218046690478468\n",
            "Epoch : 244, val loss : 0.6238468383487903\n",
            "\n",
            "Epoch : 245, train loss : 0.6200351539886356\n",
            "Epoch : 245, val loss : 0.6156411516038997\n",
            "\n",
            "Epoch : 246, train loss : 0.6196562651431919\n",
            "Epoch : 246, val loss : 0.6173308711302907\n",
            "\n",
            "Epoch : 247, train loss : 0.6192968166235717\n",
            "Epoch : 247, val loss : 0.6202232618080943\n",
            "\n",
            "Epoch : 248, train loss : 0.6191788095416446\n",
            "Epoch : 248, val loss : 0.6223128845817163\n",
            "\n",
            "Epoch : 249, train loss : 0.6186251961823668\n",
            "Epoch : 249, val loss : 0.623631471081784\n",
            "\n",
            "Epoch : 250, train loss : 0.6185227108724188\n",
            "Epoch : 250, val loss : 0.6123261106641668\n",
            "\n",
            "Epoch : 251, train loss : 0.618645956480142\n",
            "Epoch : 251, val loss : 0.6182777348317599\n",
            "\n",
            "Epoch : 252, train loss : 0.6181970186305767\n",
            "Epoch : 252, val loss : 0.6151491968255295\n",
            "\n",
            "Epoch : 253, train loss : 0.6178352504065544\n",
            "Epoch : 253, val loss : 0.6059779838511818\n",
            "\n",
            "Epoch : 254, train loss : 0.6193722051201443\n",
            "Epoch : 254, val loss : 0.6139681778456035\n",
            "\n",
            "Epoch : 255, train loss : 0.6181828524127148\n",
            "Epoch : 255, val loss : 0.6193360498076991\n",
            "\n",
            "Epoch : 256, train loss : 0.617783357699712\n",
            "Epoch : 256, val loss : 0.6144422778957768\n",
            "\n",
            "Epoch : 257, train loss : 0.6176549362413806\n",
            "Epoch : 257, val loss : 0.6181538387348777\n",
            "\n",
            "Epoch : 258, train loss : 0.6175784958131388\n",
            "Epoch : 258, val loss : 0.6156380615736308\n",
            "\n",
            "Epoch : 259, train loss : 0.6175951697609637\n",
            "Epoch : 259, val loss : 0.622181917491712\n",
            "\n",
            "Epoch : 260, train loss : 0.6173158530032998\n",
            "Epoch : 260, val loss : 0.6153263355556285\n",
            "\n",
            "Epoch : 261, train loss : 0.6165556094863199\n",
            "Epoch : 261, val loss : 0.611209185499894\n",
            "\n",
            "Epoch : 262, train loss : 0.6165464903369097\n",
            "Epoch : 262, val loss : 0.6084793053175274\n",
            "\n",
            "Epoch : 263, train loss : 0.6169065647052999\n",
            "Epoch : 263, val loss : 0.6120705604553224\n",
            "\n",
            "Epoch : 264, train loss : 0.6162754474264202\n",
            "Epoch : 264, val loss : 0.6131722989835238\n",
            "\n",
            "Epoch : 265, train loss : 0.6165745464238248\n",
            "Epoch : 265, val loss : 0.6195478909894039\n",
            "\n",
            "Epoch : 266, train loss : 0.6158319299871271\n",
            "Epoch : 266, val loss : 0.6083127715085683\n",
            "\n",
            "Epoch : 267, train loss : 0.6150056661981526\n",
            "Epoch : 267, val loss : 0.6170032777284322\n",
            "\n",
            "Epoch : 268, train loss : 0.6156086495428374\n",
            "Epoch : 268, val loss : 0.6174882869971425\n",
            "\n",
            "Epoch : 269, train loss : 0.6151835981643561\n",
            "Epoch : 269, val loss : 0.6100270356002605\n",
            "\n",
            "Epoch : 270, train loss : 0.6140718192765204\n",
            "Epoch : 270, val loss : 0.6111515321229634\n",
            "\n",
            "Epoch : 271, train loss : 0.6145933622663671\n",
            "Epoch : 271, val loss : 0.6134027054435327\n",
            "\n",
            "Epoch : 272, train loss : 0.6147587328246141\n",
            "Epoch : 272, val loss : 0.6096553504467012\n",
            "\n",
            "Epoch : 273, train loss : 0.614242116610209\n",
            "Epoch : 273, val loss : 0.6207656107450786\n",
            "\n",
            "Epoch : 274, train loss : 0.6143884613658444\n",
            "Epoch : 274, val loss : 0.6126203976179425\n",
            "\n",
            "Epoch : 275, train loss : 0.6130555207079106\n",
            "Epoch : 275, val loss : 0.6124117248936702\n",
            "\n",
            "Epoch : 276, train loss : 0.6140979913148008\n",
            "Epoch : 276, val loss : 0.6146986641381917\n",
            "\n",
            "Epoch : 277, train loss : 0.6133218050003051\n",
            "Epoch : 277, val loss : 0.6220167906660783\n",
            "\n",
            "Epoch : 278, train loss : 0.613027461008592\n",
            "Epoch : 278, val loss : 0.610954971689927\n",
            "\n",
            "Epoch : 279, train loss : 0.6132049116221336\n",
            "Epoch : 279, val loss : 0.6126757483733329\n",
            "\n",
            "Epoch : 280, train loss : 0.6129913225318445\n",
            "Epoch : 280, val loss : 0.6177117103024532\n",
            "\n",
            "Epoch : 281, train loss : 0.6121799913319674\n",
            "Epoch : 281, val loss : 0.6128468984051755\n",
            "\n",
            "Epoch : 282, train loss : 0.6138667084954001\n",
            "Epoch : 282, val loss : 0.6219948279230219\n",
            "\n",
            "Epoch : 283, train loss : 0.6127647620258909\n",
            "Epoch : 283, val loss : 0.6182206806383636\n",
            "\n",
            "Epoch : 284, train loss : 0.6118876919601903\n",
            "Epoch : 284, val loss : 0.6097579347459894\n",
            "\n",
            "Epoch : 285, train loss : 0.6113619838700151\n",
            "Epoch : 285, val loss : 0.6124750501231143\n",
            "\n",
            "Epoch : 286, train loss : 0.6119084849502102\n",
            "Epoch : 286, val loss : 0.608769881097894\n",
            "\n",
            "Epoch : 287, train loss : 0.6116957155140966\n",
            "Epoch : 287, val loss : 0.6090248383973774\n",
            "\n",
            "Epoch : 288, train loss : 0.6107378497268217\n",
            "Epoch : 288, val loss : 0.6054342373421318\n",
            "\n",
            "Epoch : 289, train loss : 0.6104457161643289\n",
            "Epoch : 289, val loss : 0.6154593386148153\n",
            "\n",
            "Epoch : 290, train loss : 0.6104113542672364\n",
            "Epoch : 290, val loss : 0.602260172367096\n",
            "\n",
            "Epoch : 291, train loss : 0.6106965055971435\n",
            "Epoch : 291, val loss : 0.6154282093048096\n",
            "\n",
            "Epoch : 292, train loss : 0.6098688699982382\n",
            "Epoch : 292, val loss : 0.6056462165556455\n",
            "\n",
            "Epoch : 293, train loss : 0.6097960491975148\n",
            "Epoch : 293, val loss : 0.6138059685104771\n",
            "\n",
            "Epoch : 294, train loss : 0.6092731696186642\n",
            "Epoch : 294, val loss : 0.6108666752514086\n",
            "\n",
            "Epoch : 295, train loss : 0.6096609482259459\n",
            "Epoch : 295, val loss : 0.6082920463461625\n",
            "\n",
            "Epoch : 296, train loss : 0.6104338915059061\n",
            "Epoch : 296, val loss : 0.6075475811958312\n",
            "\n",
            "Epoch : 297, train loss : 0.6093240828225106\n",
            "Epoch : 297, val loss : 0.6000381049356962\n",
            "\n",
            "Epoch : 298, train loss : 0.6090447416811281\n",
            "Epoch : 298, val loss : 0.6086302776085704\n",
            "\n",
            "Epoch : 299, train loss : 0.6096454306082291\n",
            "Epoch : 299, val loss : 0.6068646123534756\n",
            "\n",
            "Epoch : 300, train loss : 0.6081739246845244\n",
            "Epoch : 300, val loss : 0.6115078988828156\n",
            "\n",
            "Epoch : 301, train loss : 0.6084733907020453\n",
            "Epoch : 301, val loss : 0.6063538350557026\n",
            "\n",
            "Epoch : 302, train loss : 0.6087531219829212\n",
            "Epoch : 302, val loss : 0.606359563375774\n",
            "\n",
            "Epoch : 303, train loss : 0.6101653135184087\n",
            "Epoch : 303, val loss : 0.6160537726000737\n",
            "\n",
            "Epoch : 304, train loss : 0.6083112835884094\n",
            "Epoch : 304, val loss : 0.6129453464558249\n",
            "\n",
            "Epoch : 305, train loss : 0.6074321219415375\n",
            "Epoch : 305, val loss : 0.6058662847468728\n",
            "\n",
            "Epoch : 306, train loss : 0.6087918861345812\n",
            "Epoch : 306, val loss : 0.607989555911014\n",
            "\n",
            "Epoch : 307, train loss : 0.6079951560858526\n",
            "Epoch : 307, val loss : 0.6083893556343881\n",
            "\n",
            "Epoch : 308, train loss : 0.6068540056546529\n",
            "Epoch : 308, val loss : 0.6000933019738448\n",
            "\n",
            "Epoch : 309, train loss : 0.6067495411092588\n",
            "Epoch : 309, val loss : 0.6114633444108463\n",
            "\n",
            "Epoch : 310, train loss : 0.6059602043845435\n",
            "Epoch : 310, val loss : 0.6098978268472772\n",
            "\n",
            "Epoch : 311, train loss : 0.6066574465144764\n",
            "Epoch : 311, val loss : 0.6082073936336918\n",
            "\n",
            "Epoch : 312, train loss : 0.606365753303875\n",
            "Epoch : 312, val loss : 0.6105702393933347\n",
            "\n",
            "Epoch : 313, train loss : 0.6062633664319012\n",
            "Epoch : 313, val loss : 0.6065352919854615\n",
            "\n",
            "Epoch : 314, train loss : 0.6088838617006939\n",
            "Epoch : 314, val loss : 0.6077366659515783\n",
            "\n",
            "Epoch : 315, train loss : 0.6057628436522052\n",
            "Epoch : 315, val loss : 0.6093102066140424\n",
            "\n",
            "Epoch : 316, train loss : 0.6066107760776172\n",
            "Epoch : 316, val loss : 0.6033126586361935\n",
            "\n",
            "Epoch : 317, train loss : 0.6056481903249566\n",
            "Epoch : 317, val loss : 0.6128663734385843\n",
            "\n",
            "Epoch : 318, train loss : 0.605372502225818\n",
            "Epoch : 318, val loss : 0.6032180190086365\n",
            "\n",
            "Epoch : 319, train loss : 0.6056912499846836\n",
            "Epoch : 319, val loss : 0.6030303992723165\n",
            "\n",
            "Epoch : 320, train loss : 0.60499951767199\n",
            "Epoch : 320, val loss : 0.6021302656123514\n",
            "\n",
            "Epoch : 321, train loss : 0.6041854565793819\n",
            "Epoch : 321, val loss : 0.6050840553484464\n",
            "\n",
            "Epoch : 322, train loss : 0.6050345549077697\n",
            "Epoch : 322, val loss : 0.609188349623429\n",
            "\n",
            "Epoch : 323, train loss : 0.6048911345727517\n",
            "Epoch : 323, val loss : 0.6095226658018011\n",
            "\n",
            "Epoch : 324, train loss : 0.6046438058217368\n",
            "Epoch : 324, val loss : 0.6023946247602765\n",
            "\n",
            "Epoch : 325, train loss : 0.6038604871793226\n",
            "Epoch : 325, val loss : 0.5994130090663307\n",
            "\n",
            "Epoch : 326, train loss : 0.6031804019754582\n",
            "Epoch : 326, val loss : 0.6049265955623828\n",
            "\n",
            "Epoch : 327, train loss : 0.6028192984335352\n",
            "Epoch : 327, val loss : 0.6056667158478185\n",
            "\n",
            "Epoch : 328, train loss : 0.6038840008504466\n",
            "Epoch : 328, val loss : 0.6022864391929225\n",
            "\n",
            "Epoch : 329, train loss : 0.6033901866638302\n",
            "Epoch : 329, val loss : 0.6034904687028183\n",
            "\n",
            "Epoch : 330, train loss : 0.6031249938589155\n",
            "Epoch : 330, val loss : 0.6037058673406903\n",
            "\n",
            "Epoch : 331, train loss : 0.602551499821923\n",
            "Epoch : 331, val loss : 0.606773654097005\n",
            "\n",
            "Epoch : 332, train loss : 0.6037785564408157\n",
            "Epoch : 332, val loss : 0.6009602232983238\n",
            "\n",
            "Epoch : 333, train loss : 0.6017787414969823\n",
            "Epoch : 333, val loss : 0.6095881869918421\n",
            "\n",
            "Epoch : 334, train loss : 0.6018920519135212\n",
            "Epoch : 334, val loss : 0.6008947428904082\n",
            "\n",
            "Epoch : 335, train loss : 0.6023481213685237\n",
            "Epoch : 335, val loss : 0.6081362460788926\n",
            "\n",
            "Epoch : 336, train loss : 0.6033027717561433\n",
            "Epoch : 336, val loss : 0.6021250266777841\n",
            "\n",
            "Epoch : 337, train loss : 0.6015499747160707\n",
            "Epoch : 337, val loss : 0.6042909088887667\n",
            "\n",
            "Epoch : 338, train loss : 0.6018142100536465\n",
            "Epoch : 338, val loss : 0.6174848942380202\n",
            "\n",
            "Epoch : 339, train loss : 0.6022602081298833\n",
            "Epoch : 339, val loss : 0.6154084958528216\n",
            "\n",
            "Epoch : 340, train loss : 0.6021786799936585\n",
            "Epoch : 340, val loss : 0.6013784094860679\n",
            "\n",
            "Epoch : 341, train loss : 0.6011127473730032\n",
            "Epoch : 341, val loss : 0.6019997251661201\n",
            "\n",
            "Epoch : 342, train loss : 0.6009604118087073\n",
            "Epoch : 342, val loss : 0.5981931655030502\n",
            "\n",
            "Epoch : 343, train loss : 0.6016578547882314\n",
            "Epoch : 343, val loss : 0.6033908128738403\n",
            "\n",
            "Epoch : 344, train loss : 0.6007387818712182\n",
            "Epoch : 344, val loss : 0.60250313501609\n",
            "\n",
            "Epoch : 345, train loss : 0.5999205016728605\n",
            "Epoch : 345, val loss : 0.6112648123189022\n",
            "\n",
            "Epoch : 346, train loss : 0.5997951480475341\n",
            "Epoch : 346, val loss : 0.6019675543433741\n",
            "\n",
            "Epoch : 347, train loss : 0.5995605900432124\n",
            "Epoch : 347, val loss : 0.5951894145262867\n",
            "\n",
            "Epoch : 348, train loss : 0.6002848738973793\n",
            "Epoch : 348, val loss : 0.6148229962901067\n",
            "\n",
            "Epoch : 349, train loss : 0.5991586854963596\n",
            "Epoch : 349, val loss : 0.5915036075993588\n",
            "\n",
            "Epoch : 350, train loss : 0.5996340654113078\n",
            "Epoch : 350, val loss : 0.5988954933066116\n",
            "\n",
            "Epoch : 351, train loss : 0.5992360590082223\n",
            "Epoch : 351, val loss : 0.6108111158797616\n",
            "\n",
            "Epoch : 352, train loss : 0.5994202653566997\n",
            "Epoch : 352, val loss : 0.5987757758090371\n",
            "\n",
            "Epoch : 353, train loss : 0.5993031337405698\n",
            "Epoch : 353, val loss : 0.6017498970031738\n",
            "\n",
            "Epoch : 354, train loss : 0.6001429239908854\n",
            "Epoch : 354, val loss : 0.6020815874400891\n",
            "\n",
            "Epoch : 355, train loss : 0.5982558409372967\n",
            "Epoch : 355, val loss : 0.6108241865509435\n",
            "\n",
            "Epoch : 356, train loss : 0.598121543905952\n",
            "Epoch : 356, val loss : 0.593216199623911\n",
            "\n",
            "Epoch : 357, train loss : 0.5984855984196521\n",
            "Epoch : 357, val loss : 0.5987815543224938\n",
            "\n",
            "Epoch : 358, train loss : 0.5973054116422483\n",
            "Epoch : 358, val loss : 0.5999667205308614\n",
            "\n",
            "Epoch : 359, train loss : 0.5969051650076204\n",
            "Epoch : 359, val loss : 0.6117288940831235\n",
            "\n",
            "Epoch : 360, train loss : 0.599085363835999\n",
            "Epoch : 360, val loss : 0.6014958776925741\n",
            "\n",
            "Epoch : 361, train loss : 0.5978419647072302\n",
            "Epoch : 361, val loss : 0.6028779243168078\n",
            "\n",
            "Epoch : 362, train loss : 0.5974362488948937\n",
            "Epoch : 362, val loss : 0.5961829549387883\n",
            "\n",
            "Epoch : 363, train loss : 0.5968585543560263\n",
            "Epoch : 363, val loss : 0.6016389962873961\n",
            "\n",
            "Epoch : 364, train loss : 0.5981104959141124\n",
            "Epoch : 364, val loss : 0.6052238470629643\n",
            "\n",
            "Epoch : 365, train loss : 0.5966047005219898\n",
            "Epoch : 365, val loss : 0.593964247327102\n",
            "\n",
            "Epoch : 366, train loss : 0.5970183903520758\n",
            "Epoch : 366, val loss : 0.5944700617539256\n",
            "\n",
            "Epoch : 367, train loss : 0.5967307742797967\n",
            "Epoch : 367, val loss : 0.5978802615090419\n",
            "\n",
            "Epoch : 368, train loss : 0.5970907910303637\n",
            "Epoch : 368, val loss : 0.5943819111899326\n",
            "\n",
            "Epoch : 369, train loss : 0.5965651790301005\n",
            "Epoch : 369, val loss : 0.5989634206420497\n",
            "\n",
            "Epoch : 370, train loss : 0.5950674640409872\n",
            "Epoch : 370, val loss : 0.5904096537514737\n",
            "\n",
            "Epoch : 371, train loss : 0.5961475977391905\n",
            "Epoch : 371, val loss : 0.6044669010137256\n",
            "\n",
            "Epoch : 372, train loss : 0.5956338154547142\n",
            "Epoch : 372, val loss : 0.5966201691251053\n",
            "\n",
            "Epoch : 373, train loss : 0.5956236398581304\n",
            "Epoch : 373, val loss : 0.5905180008787859\n",
            "\n",
            "Epoch : 374, train loss : 0.5950717824878118\n",
            "Epoch : 374, val loss : 0.5939858195028807\n",
            "\n",
            "Epoch : 375, train loss : 0.5938081333131503\n",
            "Epoch : 375, val loss : 0.591144368836754\n",
            "\n",
            "Epoch : 376, train loss : 0.595068469733903\n",
            "Epoch : 376, val loss : 0.5920419755734895\n",
            "\n",
            "Epoch : 377, train loss : 0.5948588665687677\n",
            "Epoch : 377, val loss : 0.6022725920928153\n",
            "\n",
            "Epoch : 378, train loss : 0.5949749213276485\n",
            "Epoch : 378, val loss : 0.5923811291393483\n",
            "\n",
            "Epoch : 379, train loss : 0.5946166347373616\n",
            "Epoch : 379, val loss : 0.5982429855748228\n",
            "\n",
            "Epoch : 380, train loss : 0.594508815534187\n",
            "Epoch : 380, val loss : 0.5948018557146975\n",
            "\n",
            "Epoch : 381, train loss : 0.5937306440237797\n",
            "Epoch : 381, val loss : 0.5992766054053056\n",
            "\n",
            "Epoch : 382, train loss : 0.5930307088476239\n",
            "Epoch : 382, val loss : 0.5889011370508295\n",
            "\n",
            "Epoch : 383, train loss : 0.593106948245655\n",
            "Epoch : 383, val loss : 0.590819449801194\n",
            "\n",
            "Epoch : 384, train loss : 0.5929454270637395\n",
            "Epoch : 384, val loss : 0.5955057677469755\n",
            "\n",
            "Epoch : 385, train loss : 0.5931637623093347\n",
            "Epoch : 385, val loss : 0.5923802366382197\n",
            "\n",
            "Epoch : 386, train loss : 0.5927232751340574\n",
            "Epoch : 386, val loss : 0.5882046975587545\n",
            "\n",
            "Epoch : 387, train loss : 0.5928292102885973\n",
            "Epoch : 387, val loss : 0.6007683528097052\n",
            "\n",
            "Epoch : 388, train loss : 0.592802727403063\n",
            "Epoch : 388, val loss : 0.5898182737199883\n",
            "\n",
            "Epoch : 389, train loss : 0.5926588753859202\n",
            "Epoch : 389, val loss : 0.5982291447488886\n",
            "\n",
            "Epoch : 390, train loss : 0.5929758525255953\n",
            "Epoch : 390, val loss : 0.5937906582104532\n",
            "\n",
            "Epoch : 391, train loss : 0.5924292492144037\n",
            "Epoch : 391, val loss : 0.5901113089762237\n",
            "\n",
            "Epoch : 392, train loss : 0.5921169765067824\n",
            "Epoch : 392, val loss : 0.5921033150271365\n",
            "\n",
            "Epoch : 393, train loss : 0.5916078876365316\n",
            "Epoch : 393, val loss : 0.5932852406250804\n",
            "\n",
            "Epoch : 394, train loss : 0.5913085057879942\n",
            "Epoch : 394, val loss : 0.5913813521987513\n",
            "\n",
            "Epoch : 395, train loss : 0.591219178474311\n",
            "Epoch : 395, val loss : 0.5873085385874699\n",
            "\n",
            "Epoch : 396, train loss : 0.5911096518689934\n",
            "Epoch : 396, val loss : 0.5826407404322373\n",
            "\n",
            "Epoch : 397, train loss : 0.5920295924851385\n",
            "Epoch : 397, val loss : 0.5890742383505169\n",
            "\n",
            "Epoch : 398, train loss : 0.5914285376216427\n",
            "Epoch : 398, val loss : 0.5903393626213073\n",
            "\n",
            "Epoch : 399, train loss : 0.5906319307558463\n",
            "Epoch : 399, val loss : 0.6048175770985454\n",
            "\n",
            "Epoch : 400, train loss : 0.5900592160947394\n",
            "Epoch : 400, val loss : 0.5861436348212392\n",
            "\n",
            "Epoch : 401, train loss : 0.5900843690742145\n",
            "Epoch : 401, val loss : 0.5928076725257071\n",
            "\n",
            "Epoch : 402, train loss : 0.5903936579371941\n",
            "Epoch : 402, val loss : 0.5955877900123595\n",
            "\n",
            "Epoch : 403, train loss : 0.5896034213629638\n",
            "Epoch : 403, val loss : 0.5931391857172312\n",
            "\n",
            "Epoch : 404, train loss : 0.5897603806221127\n",
            "Epoch : 404, val loss : 0.584399844470777\n",
            "\n",
            "Epoch : 405, train loss : 0.5897298204176354\n",
            "Epoch : 405, val loss : 0.5857977208338286\n",
            "\n",
            "Epoch : 406, train loss : 0.5892429039333806\n",
            "Epoch : 406, val loss : 0.6112592063452068\n",
            "\n",
            "Epoch : 407, train loss : 0.5898801534464863\n",
            "Epoch : 407, val loss : 0.5994589187596975\n",
            "\n",
            "Epoch : 408, train loss : 0.5888908592137424\n",
            "Epoch : 408, val loss : 0.5929875687548988\n",
            "\n",
            "Epoch : 409, train loss : 0.5904549679972907\n",
            "Epoch : 409, val loss : 0.5857554689833994\n",
            "\n",
            "Epoch : 410, train loss : 0.5906688421061546\n",
            "Epoch : 410, val loss : 0.5800510723339884\n",
            "\n",
            "Epoch : 411, train loss : 0.5896935049331549\n",
            "Epoch : 411, val loss : 0.5844686596017136\n",
            "\n",
            "Epoch : 412, train loss : 0.5882440339435232\n",
            "Epoch : 412, val loss : 0.6025457554741909\n",
            "\n",
            "Epoch : 413, train loss : 0.5879491782549652\n",
            "Epoch : 413, val loss : 0.5889735566942317\n",
            "\n",
            "Epoch : 414, train loss : 0.5891740482864958\n",
            "Epoch : 414, val loss : 0.5898459208639044\n",
            "\n",
            "Epoch : 415, train loss : 0.5882876453977643\n",
            "Epoch : 415, val loss : 0.601453258803016\n",
            "\n",
            "Epoch : 416, train loss : 0.5879041397210323\n",
            "Epoch : 416, val loss : 0.5853088639284436\n",
            "\n",
            "Epoch : 417, train loss : 0.588045170993516\n",
            "Epoch : 417, val loss : 0.5903660843246862\n",
            "\n",
            "Epoch : 418, train loss : 0.5886340020280896\n",
            "Epoch : 418, val loss : 0.5866686090042716\n",
            "\n",
            "Epoch : 419, train loss : 0.5875661391200443\n",
            "Epoch : 419, val loss : 0.5922223157004307\n",
            "\n",
            "Epoch : 420, train loss : 0.5878262776317019\n",
            "Epoch : 420, val loss : 0.5904683282500818\n",
            "\n",
            "Epoch : 421, train loss : 0.5877569379228532\n",
            "Epoch : 421, val loss : 0.5896183989549938\n",
            "\n",
            "Epoch : 422, train loss : 0.5876568472746649\n",
            "Epoch : 422, val loss : 0.5920743848148147\n",
            "\n",
            "Epoch : 423, train loss : 0.5872435118212845\n",
            "Epoch : 423, val loss : 0.5811890865627088\n",
            "\n",
            "Epoch : 424, train loss : 0.5862949866237064\n",
            "Epoch : 424, val loss : 0.5930283414690117\n",
            "\n",
            "Epoch : 425, train loss : 0.586906276327191\n",
            "Epoch : 425, val loss : 0.5909891175596338\n",
            "\n",
            "Epoch : 426, train loss : 0.5868191905093916\n",
            "Epoch : 426, val loss : 0.581992993229314\n",
            "\n",
            "Epoch : 427, train loss : 0.5862416957363937\n",
            "Epoch : 427, val loss : 0.603092814746656\n",
            "\n",
            "Epoch : 428, train loss : 0.5861087031436687\n",
            "Epoch : 428, val loss : 0.5812905320995734\n",
            "\n",
            "Epoch : 429, train loss : 0.5857393271995311\n",
            "Epoch : 429, val loss : 0.5887691033513923\n",
            "\n",
            "Epoch : 430, train loss : 0.5858961773641181\n",
            "Epoch : 430, val loss : 0.581839908110468\n",
            "\n",
            "Epoch : 431, train loss : 0.5853665041201037\n",
            "Epoch : 431, val loss : 0.5935443028023367\n",
            "\n",
            "Epoch : 432, train loss : 0.5846748091957785\n",
            "Epoch : 432, val loss : 0.5786477876336951\n",
            "\n",
            "Epoch : 433, train loss : 0.5850877248879635\n",
            "Epoch : 433, val loss : 0.5899786996214014\n",
            "\n",
            "Epoch : 434, train loss : 0.5922269720019716\n",
            "Epoch : 434, val loss : 0.5829412247005261\n",
            "\n",
            "Epoch : 435, train loss : 0.585483351801381\n",
            "Epoch : 435, val loss : 0.588010958935085\n",
            "\n",
            "Epoch : 436, train loss : 0.5852694493351558\n",
            "Epoch : 436, val loss : 0.5844946189930564\n",
            "\n",
            "Epoch : 437, train loss : 0.5844166638273183\n",
            "Epoch : 437, val loss : 0.5834730612604242\n",
            "\n",
            "Epoch : 438, train loss : 0.5847819478222817\n",
            "Epoch : 438, val loss : 0.5947690904140472\n",
            "\n",
            "Epoch : 439, train loss : 0.5839435716470079\n",
            "Epoch : 439, val loss : 0.5850680690062674\n",
            "\n",
            "Epoch : 440, train loss : 0.5841036731546575\n",
            "Epoch : 440, val loss : 0.5867731602568376\n",
            "\n",
            "Epoch : 441, train loss : 0.5861325123093345\n",
            "Epoch : 441, val loss : 0.5980231071773328\n",
            "\n",
            "Epoch : 442, train loss : 0.5837912776253441\n",
            "Epoch : 442, val loss : 0.5876525671858538\n",
            "\n",
            "Epoch : 443, train loss : 0.5854304761597601\n",
            "Epoch : 443, val loss : 0.5830227058184774\n",
            "\n",
            "Epoch : 444, train loss : 0.5837026102976366\n",
            "Epoch : 444, val loss : 0.5807439057450545\n",
            "\n",
            "Epoch : 445, train loss : 0.5842189889965635\n",
            "Epoch : 445, val loss : 0.5846543986546365\n",
            "\n",
            "Epoch : 446, train loss : 0.5830690277345251\n",
            "Epoch : 446, val loss : 0.5884632311369242\n",
            "\n",
            "Epoch : 447, train loss : 0.5833031222675783\n",
            "Epoch : 447, val loss : 0.5827837366806834\n",
            "\n",
            "Epoch : 448, train loss : 0.5820999842701536\n",
            "Epoch : 448, val loss : 0.5823479279091484\n",
            "\n",
            "Epoch : 449, train loss : 0.5832824154333637\n",
            "Epoch : 449, val loss : 0.5861411282890723\n",
            "\n",
            "Epoch : 450, train loss : 0.5820364831071914\n",
            "Epoch : 450, val loss : 0.5937706859488235\n",
            "\n",
            "Epoch : 451, train loss : 0.5830497562885286\n",
            "Epoch : 451, val loss : 0.5845260102497905\n",
            "\n",
            "Epoch : 452, train loss : 0.5826464282743858\n",
            "Epoch : 452, val loss : 0.5833149834683068\n",
            "\n",
            "Epoch : 453, train loss : 0.5822330256303155\n",
            "Epoch : 453, val loss : 0.5871115656275498\n",
            "\n",
            "Epoch : 454, train loss : 0.5820553815726077\n",
            "Epoch : 454, val loss : 0.5909258064470793\n",
            "\n",
            "Epoch : 455, train loss : 0.582107843232877\n",
            "Epoch : 455, val loss : 0.5897357652061863\n",
            "\n",
            "Epoch : 456, train loss : 0.5822244463544907\n",
            "Epoch : 456, val loss : 0.5774184653633518\n",
            "\n",
            "Epoch : 457, train loss : 0.5809971735332949\n",
            "Epoch : 457, val loss : 0.5905074681106368\n",
            "\n",
            "Epoch : 458, train loss : 0.5810326760465447\n",
            "Epoch : 458, val loss : 0.577992153795142\n",
            "\n",
            "Epoch : 459, train loss : 0.5811526831352352\n",
            "Epoch : 459, val loss : 0.5755304819659183\n",
            "\n",
            "Epoch : 460, train loss : 0.5814070705211527\n",
            "Epoch : 460, val loss : 0.5884703005615033\n",
            "\n",
            "Epoch : 461, train loss : 0.5805920183658599\n",
            "Epoch : 461, val loss : 0.5745255319695723\n",
            "\n",
            "Epoch : 462, train loss : 0.5801199613195479\n",
            "Epoch : 462, val loss : 0.5850396924897244\n",
            "\n",
            "Epoch : 463, train loss : 0.5796070104295559\n",
            "Epoch : 463, val loss : 0.5952562548612292\n",
            "\n",
            "Epoch : 464, train loss : 0.5814366069707003\n",
            "Epoch : 464, val loss : 0.5815998064844231\n",
            "\n",
            "Epoch : 465, train loss : 0.5803044617176052\n",
            "Epoch : 465, val loss : 0.5815803753702264\n",
            "\n",
            "Epoch : 466, train loss : 0.5808762320966429\n",
            "Epoch : 466, val loss : 0.5768670549518184\n",
            "\n",
            "Epoch : 467, train loss : 0.5788907914450675\n",
            "Epoch : 467, val loss : 0.5923998010785957\n",
            "\n",
            "Epoch : 468, train loss : 0.5791486525174343\n",
            "Epoch : 468, val loss : 0.5854495073619642\n",
            "\n",
            "Epoch : 469, train loss : 0.5813289980093638\n",
            "Epoch : 469, val loss : 0.5825817130113903\n",
            "\n",
            "Epoch : 470, train loss : 0.5798620112014538\n",
            "Epoch : 470, val loss : 0.5786649678882799\n",
            "\n",
            "Epoch : 471, train loss : 0.5799280565796476\n",
            "Epoch : 471, val loss : 0.5825158545845432\n",
            "\n",
            "Epoch : 472, train loss : 0.5793641433571324\n",
            "Epoch : 472, val loss : 0.5864980738413961\n",
            "\n",
            "Epoch : 473, train loss : 0.5792780142841916\n",
            "Epoch : 473, val loss : 0.5804287195205688\n",
            "\n",
            "Epoch : 474, train loss : 0.579847600243308\n",
            "Epoch : 474, val loss : 0.5841937253349705\n",
            "\n",
            "Epoch : 475, train loss : 0.5790676051920106\n",
            "Epoch : 475, val loss : 0.5785239432987413\n",
            "\n",
            "Epoch : 476, train loss : 0.5786316418286525\n",
            "Epoch : 476, val loss : 0.5751294556416963\n",
            "\n",
            "Epoch : 477, train loss : 0.5860047206734165\n",
            "Epoch : 477, val loss : 0.5823786948856554\n",
            "\n",
            "Epoch : 478, train loss : 0.579166620789152\n",
            "Epoch : 478, val loss : 0.5847286440824209\n",
            "\n",
            "Epoch : 479, train loss : 0.5778122190273168\n",
            "Epoch : 479, val loss : 0.5766131768101139\n",
            "\n",
            "Epoch : 480, train loss : 0.5789765777009905\n",
            "Epoch : 480, val loss : 0.6021722712014851\n",
            "\n",
            "Epoch : 481, train loss : 0.5776593517173421\n",
            "Epoch : 481, val loss : 0.5801472867790021\n",
            "\n",
            "Epoch : 482, train loss : 0.5785914487910989\n",
            "Epoch : 482, val loss : 0.5904941652950486\n",
            "\n",
            "Epoch : 483, train loss : 0.5771741924863871\n",
            "Epoch : 483, val loss : 0.5793039751680273\n",
            "\n",
            "Epoch : 484, train loss : 0.5783055762449901\n",
            "Epoch : 484, val loss : 0.575382958901556\n",
            "\n",
            "Epoch : 485, train loss : 0.5781345174168098\n",
            "Epoch : 485, val loss : 0.5770844487767471\n",
            "\n",
            "Epoch : 486, train loss : 0.5781308831590598\n",
            "Epoch : 486, val loss : 0.57368571350449\n",
            "\n",
            "Epoch : 487, train loss : 0.5767588655153909\n",
            "Epoch : 487, val loss : 0.5776116942104541\n",
            "\n",
            "Epoch : 488, train loss : 0.5767114420731864\n",
            "Epoch : 488, val loss : 0.586037963628769\n",
            "\n",
            "Epoch : 489, train loss : 0.5763676117766988\n",
            "Epoch : 489, val loss : 0.5759821769438292\n",
            "\n",
            "Epoch : 490, train loss : 0.5765122247464731\n",
            "Epoch : 490, val loss : 0.5765650570392608\n",
            "\n",
            "Epoch : 491, train loss : 0.578941052790844\n",
            "Epoch : 491, val loss : 0.606926458446603\n",
            "\n",
            "Epoch : 492, train loss : 0.5770057611393209\n",
            "Epoch : 492, val loss : 0.5754477852269224\n",
            "\n",
            "Epoch : 493, train loss : 0.5754044095675152\n",
            "Epoch : 493, val loss : 0.5713661736563632\n",
            "\n",
            "Epoch : 494, train loss : 0.575643668572108\n",
            "Epoch : 494, val loss : 0.5759584590008384\n",
            "\n",
            "Epoch : 495, train loss : 0.5755448930191268\n",
            "Epoch : 495, val loss : 0.5787167266795511\n",
            "\n",
            "Epoch : 496, train loss : 0.5751263553445989\n",
            "Epoch : 496, val loss : 0.5795078026620966\n",
            "\n",
            "Epoch : 497, train loss : 0.5742087658607602\n",
            "Epoch : 497, val loss : 0.5780628966657739\n",
            "\n",
            "Epoch : 498, train loss : 0.5755325387824664\n",
            "Epoch : 498, val loss : 0.5735825080620616\n",
            "\n",
            "Epoch : 499, train loss : 0.5753599376389471\n",
            "Epoch : 499, val loss : 0.5775851654378991\n",
            "\n",
            "Epoch : 500, train loss : 0.5745733560937822\n",
            "Epoch : 500, val loss : 0.5762344724253605\n",
            "\n",
            "Epoch : 501, train loss : 0.5746707807887683\n",
            "Epoch : 501, val loss : 0.5760095056734587\n",
            "\n",
            "Epoch : 502, train loss : 0.575037690003713\n",
            "Epoch : 502, val loss : 0.5781893102746261\n",
            "\n",
            "Epoch : 503, train loss : 0.5754121868899371\n",
            "Epoch : 503, val loss : 0.5797065606242732\n",
            "\n",
            "Epoch : 504, train loss : 0.574921830856439\n",
            "Epoch : 504, val loss : 0.5678391425233139\n",
            "\n",
            "Epoch : 505, train loss : 0.5751721830079051\n",
            "Epoch : 505, val loss : 0.5720445968602833\n",
            "\n",
            "Epoch : 506, train loss : 0.5743019635027105\n",
            "Epoch : 506, val loss : 0.5698380125196356\n",
            "\n",
            "Epoch : 507, train loss : 0.5747173612767998\n",
            "Epoch : 507, val loss : 0.5722618730444655\n",
            "\n",
            "Epoch : 508, train loss : 0.5753025226520773\n",
            "Epoch : 508, val loss : 0.574913435860684\n",
            "\n",
            "Epoch : 509, train loss : 0.57355243527528\n",
            "Epoch : 509, val loss : 0.5717986806442864\n",
            "\n",
            "Epoch : 510, train loss : 0.5733193717219611\n",
            "Epoch : 510, val loss : 0.5711331038098585\n",
            "\n",
            "Epoch : 511, train loss : 0.5735183546037382\n",
            "Epoch : 511, val loss : 0.571681254788449\n",
            "\n",
            "Epoch : 512, train loss : 0.5732225403641207\n",
            "Epoch : 512, val loss : 0.5810553729534148\n",
            "\n",
            "Epoch : 513, train loss : 0.5730323502511692\n",
            "Epoch : 513, val loss : 0.5758693641737888\n",
            "\n",
            "Epoch : 514, train loss : 0.5742467818838177\n",
            "Epoch : 514, val loss : 0.5825954534505544\n",
            "\n",
            "Epoch : 515, train loss : 0.5732862840999255\n",
            "Epoch : 515, val loss : 0.5721187920946824\n",
            "\n",
            "Epoch : 516, train loss : 0.5728748151750277\n",
            "Epoch : 516, val loss : 0.5808465982738295\n",
            "\n",
            "Epoch : 517, train loss : 0.5721054611784037\n",
            "Epoch : 517, val loss : 0.5763956386792033\n",
            "\n",
            "Epoch : 518, train loss : 0.5730335587804968\n",
            "Epoch : 518, val loss : 0.5742035445414091\n",
            "\n",
            "Epoch : 519, train loss : 0.5726070133122533\n",
            "Epoch : 519, val loss : 0.5774405002593995\n",
            "\n",
            "Epoch : 520, train loss : 0.5723769626834174\n",
            "Epoch : 520, val loss : 0.5734400435497885\n",
            "\n",
            "Epoch : 521, train loss : 0.5720990215287061\n",
            "Epoch : 521, val loss : 0.573345234519557\n",
            "\n",
            "Epoch : 522, train loss : 0.571852401350484\n",
            "Epoch : 522, val loss : 0.5803803472142469\n",
            "\n",
            "Epoch : 523, train loss : 0.5713459984822707\n",
            "Epoch : 523, val loss : 0.574625755611219\n",
            "\n",
            "Epoch : 524, train loss : 0.5711473622105339\n",
            "Epoch : 524, val loss : 0.5750485893927123\n",
            "\n",
            "Epoch : 525, train loss : 0.5718398173650109\n",
            "Epoch : 525, val loss : 0.5731142602468793\n",
            "\n",
            "Epoch : 526, train loss : 0.5710842235521838\n",
            "Epoch : 526, val loss : 0.5846674285436931\n",
            "\n",
            "Epoch : 527, train loss : 0.5722787589737863\n",
            "Epoch : 527, val loss : 0.5703027060157375\n",
            "\n",
            "Epoch : 528, train loss : 0.570581655249451\n",
            "Epoch : 528, val loss : 0.5947543915949369\n",
            "\n",
            "Epoch : 529, train loss : 0.5701528505845503\n",
            "Epoch : 529, val loss : 0.5878807243547941\n",
            "\n",
            "Epoch : 530, train loss : 0.5713641275059093\n",
            "Epoch : 530, val loss : 0.5766502148226688\n",
            "\n",
            "Epoch : 531, train loss : 0.5709282521045567\n",
            "Epoch : 531, val loss : 0.5739851970421641\n",
            "\n",
            "Epoch : 532, train loss : 0.5706289925358512\n",
            "Epoch : 532, val loss : 0.5759369730949403\n",
            "\n",
            "Epoch : 533, train loss : 0.5700881923689984\n",
            "Epoch : 533, val loss : 0.5723449274113305\n",
            "\n",
            "Epoch : 534, train loss : 0.5708922946091852\n",
            "Epoch : 534, val loss : 0.5867230907866828\n",
            "\n",
            "Epoch : 535, train loss : 0.5701915365276916\n",
            "Epoch : 535, val loss : 0.5637357031044207\n",
            "\n",
            "Epoch : 536, train loss : 0.5697223155787494\n",
            "Epoch : 536, val loss : 0.5662106699065158\n",
            "\n",
            "Epoch : 537, train loss : 0.5699888697176267\n",
            "Epoch : 537, val loss : 0.5759355425834656\n",
            "\n",
            "Epoch : 538, train loss : 0.5697485425255512\n",
            "Epoch : 538, val loss : 0.5673166074250873\n",
            "\n",
            "Epoch : 539, train loss : 0.569723640066205\n",
            "Epoch : 539, val loss : 0.578361401432439\n",
            "\n",
            "Epoch : 540, train loss : 0.5695127534143851\n",
            "Epoch : 540, val loss : 0.5635603431024049\n",
            "\n",
            "Epoch : 541, train loss : 0.5689592820225342\n",
            "Epoch : 541, val loss : 0.5982230933089004\n",
            "\n",
            "Epoch : 542, train loss : 0.5688264745654484\n",
            "Epoch : 542, val loss : 0.5667616834765987\n",
            "\n",
            "Epoch : 543, train loss : 0.5694162473534095\n",
            "Epoch : 543, val loss : 0.5797631928795263\n",
            "\n",
            "Epoch : 544, train loss : 0.5691475414868558\n",
            "Epoch : 544, val loss : 0.5730303933745936\n",
            "\n",
            "Epoch : 545, train loss : 0.5688526610533392\n",
            "Epoch : 545, val loss : 0.5852291458531429\n",
            "\n",
            "Epoch : 546, train loss : 0.5682959115866455\n",
            "Epoch : 546, val loss : 0.5753287829850849\n",
            "\n",
            "Epoch : 547, train loss : 0.5690415516044154\n",
            "Epoch : 547, val loss : 0.5711059366401873\n",
            "\n",
            "Epoch : 548, train loss : 0.5675282928076657\n",
            "Epoch : 548, val loss : 0.5790828560528003\n",
            "\n",
            "Epoch : 549, train loss : 0.5683560463515194\n",
            "Epoch : 549, val loss : 0.565458013823158\n",
            "\n",
            "Epoch : 550, train loss : 0.5680216866912265\n",
            "Epoch : 550, val loss : 0.5719982777771198\n",
            "\n",
            "Epoch : 551, train loss : 0.5678161604837936\n",
            "Epoch : 551, val loss : 0.5729217592038607\n",
            "\n",
            "Epoch : 552, train loss : 0.5659057821288253\n",
            "Epoch : 552, val loss : 0.568103584804033\n",
            "\n",
            "Epoch : 553, train loss : 0.568761220664689\n",
            "Epoch : 553, val loss : 0.5664692348555516\n",
            "\n",
            "Epoch : 554, train loss : 0.5676193797227108\n",
            "Epoch : 554, val loss : 0.576697162891689\n",
            "\n",
            "Epoch : 555, train loss : 0.5706323999347114\n",
            "Epoch : 555, val loss : 0.5694462970683448\n",
            "\n",
            "Epoch : 556, train loss : 0.5667220404653835\n",
            "Epoch : 556, val loss : 0.5633718387076729\n",
            "\n",
            "Epoch : 557, train loss : 0.5670806472951714\n",
            "Epoch : 557, val loss : 0.5664840905289902\n",
            "\n",
            "Epoch : 558, train loss : 0.5670165215477796\n",
            "Epoch : 558, val loss : 0.5674319016306022\n",
            "\n",
            "Epoch : 559, train loss : 0.5660716297048511\n",
            "Epoch : 559, val loss : 0.5644294509762213\n",
            "\n",
            "Epoch : 560, train loss : 0.5670070001573277\n",
            "Epoch : 560, val loss : 0.5958907761071858\n",
            "\n",
            "Epoch : 561, train loss : 0.5675445157470128\n",
            "Epoch : 561, val loss : 0.571919864729831\n",
            "\n",
            "Epoch : 562, train loss : 0.5666583646427502\n",
            "Epoch : 562, val loss : 0.568827991422854\n",
            "\n",
            "Epoch : 563, train loss : 0.5652192999016155\n",
            "Epoch : 563, val loss : 0.5650507305797777\n",
            "\n",
            "Epoch : 564, train loss : 0.5661157524946959\n",
            "Epoch : 564, val loss : 0.5678395189737019\n",
            "\n",
            "Epoch : 565, train loss : 0.5654324166702501\n",
            "Epoch : 565, val loss : 0.569651966032229\n",
            "\n",
            "Epoch : 566, train loss : 0.5683065607692254\n",
            "Epoch : 566, val loss : 0.5688974449509068\n",
            "\n",
            "Epoch : 567, train loss : 0.5657688868768288\n",
            "Epoch : 567, val loss : 0.5771081714253677\n",
            "\n",
            "Epoch : 568, train loss : 0.5654950763240003\n",
            "Epoch : 568, val loss : 0.5660477013964402\n",
            "\n",
            "Epoch : 569, train loss : 0.566022512226394\n",
            "Epoch : 569, val loss : 0.5637579324998354\n",
            "\n",
            "Epoch : 570, train loss : 0.564331258607633\n",
            "Epoch : 570, val loss : 0.5662985004876788\n",
            "\n",
            "Epoch : 571, train loss : 0.564970480853861\n",
            "Epoch : 571, val loss : 0.5743745220334906\n",
            "\n",
            "Epoch : 572, train loss : 0.5646655209136731\n",
            "Epoch : 572, val loss : 0.5681600319711786\n",
            "\n",
            "Epoch : 573, train loss : 0.5645801432204971\n",
            "Epoch : 573, val loss : 0.5697151406815176\n",
            "\n",
            "Epoch : 574, train loss : 0.5642006404472122\n",
            "Epoch : 574, val loss : 0.5701680340264974\n",
            "\n",
            "Epoch : 575, train loss : 0.5645539045333863\n",
            "Epoch : 575, val loss : 0.5656958504727012\n",
            "\n",
            "Epoch : 576, train loss : 0.5643321765191626\n",
            "Epoch : 576, val loss : 0.5695253218475143\n",
            "\n",
            "Epoch : 577, train loss : 0.5651581924973116\n",
            "Epoch : 577, val loss : 0.5699554917059446\n",
            "\n",
            "Epoch : 578, train loss : 0.5642370912161743\n",
            "Epoch : 578, val loss : 0.5716006834256021\n",
            "\n",
            "Epoch : 579, train loss : 0.5648705643234831\n",
            "Epoch : 579, val loss : 0.5719799336634184\n",
            "\n",
            "Epoch : 580, train loss : 0.5646247574777317\n",
            "Epoch : 580, val loss : 0.5603688688654649\n",
            "\n",
            "Epoch : 581, train loss : 0.5634307108142156\n",
            "Epoch : 581, val loss : 0.5673288138289201\n",
            "\n",
            "Epoch : 582, train loss : 0.5641468053514307\n",
            "Epoch : 582, val loss : 0.5642201210323134\n",
            "\n",
            "Epoch : 583, train loss : 0.5630804316564041\n",
            "Epoch : 583, val loss : 0.5642581268360741\n",
            "\n",
            "Epoch : 584, train loss : 0.5621590820225804\n",
            "Epoch : 584, val loss : 0.5582610682437296\n",
            "\n",
            "Epoch : 585, train loss : 0.5628827730814611\n",
            "Epoch : 585, val loss : 0.5800669177582388\n",
            "\n",
            "Epoch : 586, train loss : 0.5643407763856833\n",
            "Epoch : 586, val loss : 0.5727069283786572\n",
            "\n",
            "Epoch : 587, train loss : 0.5621519840124881\n",
            "Epoch : 587, val loss : 0.5609488173534996\n",
            "\n",
            "Epoch : 588, train loss : 0.5623851440169594\n",
            "Epoch : 588, val loss : 0.5555537546935834\n",
            "\n",
            "Epoch : 589, train loss : 0.5632626822500517\n",
            "Epoch : 589, val loss : 0.5537906383213244\n",
            "\n",
            "Epoch : 590, train loss : 0.5626302583651106\n",
            "Epoch : 590, val loss : 0.5681873591322646\n",
            "\n",
            "Epoch : 591, train loss : 0.5629618370171751\n",
            "Epoch : 591, val loss : 0.5693176231886211\n",
            "\n",
            "Epoch : 592, train loss : 0.5623224056128298\n",
            "Epoch : 592, val loss : 0.5623216660399186\n",
            "\n",
            "Epoch : 593, train loss : 0.5623376252073226\n",
            "Epoch : 593, val loss : 0.5644397029751227\n",
            "\n",
            "Epoch : 594, train loss : 0.5623075342539584\n",
            "Epoch : 594, val loss : 0.5569998841536672\n",
            "\n",
            "Epoch : 595, train loss : 0.5621084507667657\n",
            "Epoch : 595, val loss : 0.5603610434030232\n",
            "\n",
            "Epoch : 596, train loss : 0.5622029217806733\n",
            "Epoch : 596, val loss : 0.5600900665709847\n",
            "\n",
            "Epoch : 597, train loss : 0.5617004855112595\n",
            "Epoch : 597, val loss : 0.5645488390797062\n",
            "\n",
            "Epoch : 598, train loss : 0.5614458575393213\n",
            "Epoch : 598, val loss : 0.5669276949606443\n",
            "\n",
            "Epoch : 599, train loss : 0.5617609464760984\n",
            "Epoch : 599, val loss : 0.5604494135630759\n",
            "\n",
            "Epoch : 600, train loss : 0.5600452722022028\n",
            "Epoch : 600, val loss : 0.5647643757493872\n",
            "\n",
            "Epoch : 601, train loss : 0.5611466767209944\n",
            "Epoch : 601, val loss : 0.5630076292314028\n",
            "\n",
            "Epoch : 602, train loss : 0.5610348132523622\n",
            "Epoch : 602, val loss : 0.557580999637905\n",
            "\n",
            "Epoch : 603, train loss : 0.5614903563802891\n",
            "Epoch : 603, val loss : 0.5627208148178301\n",
            "\n",
            "Epoch : 604, train loss : 0.5613666258075022\n",
            "Epoch : 604, val loss : 0.5683476030826569\n",
            "\n",
            "Epoch : 605, train loss : 0.5606897215048473\n",
            "Epoch : 605, val loss : 0.5700032914939679\n",
            "\n",
            "Epoch : 606, train loss : 0.5591999845071273\n",
            "Epoch : 606, val loss : 0.5578057216970542\n",
            "\n",
            "Epoch : 607, train loss : 0.5588651711290534\n",
            "Epoch : 607, val loss : 0.5571850365714023\n",
            "\n",
            "Epoch : 608, train loss : 0.5614058879288758\n",
            "Epoch : 608, val loss : 0.5541872021399048\n",
            "\n",
            "Epoch : 609, train loss : 0.560482390902259\n",
            "Epoch : 609, val loss : 0.5516782450048547\n",
            "\n",
            "Epoch : 610, train loss : 0.5602484914389524\n",
            "Epoch : 610, val loss : 0.5672961724431891\n",
            "\n",
            "Epoch : 611, train loss : 0.5603683558377357\n",
            "Epoch : 611, val loss : 0.5550610548571536\n",
            "\n",
            "Epoch : 612, train loss : 0.5578402134505186\n",
            "Epoch : 612, val loss : 0.5660363812195628\n",
            "\n",
            "Epoch : 613, train loss : 0.5606661989833372\n",
            "Epoch : 613, val loss : 0.5557790163316225\n",
            "\n",
            "Epoch : 614, train loss : 0.5603093232169294\n",
            "Epoch : 614, val loss : 0.5572033101006557\n",
            "\n",
            "Epoch : 615, train loss : 0.5594283746950556\n",
            "Epoch : 615, val loss : 0.5600585341453551\n",
            "\n",
            "Epoch : 616, train loss : 0.5587363971002174\n",
            "Epoch : 616, val loss : 0.5590366187848543\n",
            "\n",
            "Epoch : 617, train loss : 0.5591599814819566\n",
            "Epoch : 617, val loss : 0.5604276374766699\n",
            "\n",
            "Epoch : 618, train loss : 0.5586672412626672\n",
            "Epoch : 618, val loss : 0.5646167789634906\n",
            "\n",
            "Epoch : 619, train loss : 0.5581185584718529\n",
            "Epoch : 619, val loss : 0.5745345322709334\n",
            "\n",
            "Epoch : 620, train loss : 0.5589476787682734\n",
            "Epoch : 620, val loss : 0.5628527324450644\n",
            "\n",
            "Epoch : 621, train loss : 0.5587651888529458\n",
            "Epoch : 621, val loss : 0.5632955024116918\n",
            "\n",
            "Epoch : 622, train loss : 0.5576933524825355\n",
            "Epoch : 622, val loss : 0.5725765981172259\n",
            "\n",
            "Epoch : 623, train loss : 0.5577090133320203\n",
            "Epoch : 623, val loss : 0.5629702022201137\n",
            "\n",
            "Epoch : 624, train loss : 0.558967352635933\n",
            "Epoch : 624, val loss : 0.5662481204459542\n",
            "\n",
            "Epoch : 625, train loss : 0.5580596127293325\n",
            "Epoch : 625, val loss : 0.5591680125186319\n",
            "\n",
            "Epoch : 626, train loss : 0.5572622920527606\n",
            "Epoch : 626, val loss : 0.5570406803959294\n",
            "\n",
            "Epoch : 627, train loss : 0.5580462593020813\n",
            "Epoch : 627, val loss : 0.5694576846925836\n",
            "\n",
            "Epoch : 628, train loss : 0.5587534221735869\n",
            "Epoch : 628, val loss : 0.5498803948101244\n",
            "\n",
            "Epoch : 629, train loss : 0.5589587563818151\n",
            "Epoch : 629, val loss : 0.561288678332379\n",
            "\n",
            "Epoch : 630, train loss : 0.5571815098776961\n",
            "Epoch : 630, val loss : 0.5554804550973993\n",
            "\n",
            "Epoch : 631, train loss : 0.5565572722391651\n",
            "Epoch : 631, val loss : 0.5581426840079459\n",
            "\n",
            "Epoch : 632, train loss : 0.5572242285266068\n",
            "Epoch : 632, val loss : 0.5638913324004726\n",
            "\n",
            "Epoch : 633, train loss : 0.5571552030967943\n",
            "Epoch : 633, val loss : 0.5605202034900062\n",
            "\n",
            "Epoch : 634, train loss : 0.558235816522078\n",
            "Epoch : 634, val loss : 0.5532691572841845\n",
            "\n",
            "Epoch : 635, train loss : 0.557517932581179\n",
            "Epoch : 635, val loss : 0.5615624377602025\n",
            "\n",
            "Epoch : 636, train loss : 0.5564314750107849\n",
            "Epoch : 636, val loss : 0.5572921727832996\n",
            "\n",
            "Epoch : 637, train loss : 0.5566055507370921\n",
            "Epoch : 637, val loss : 0.5588947738471783\n",
            "\n",
            "Epoch : 638, train loss : 0.555894759748921\n",
            "Epoch : 638, val loss : 0.5608058336533998\n",
            "\n",
            "Epoch : 639, train loss : 0.5561217197866151\n",
            "Epoch : 639, val loss : 0.558085659616872\n",
            "\n",
            "Epoch : 640, train loss : 0.555145144101345\n",
            "Epoch : 640, val loss : 0.5610407590866088\n",
            "\n",
            "Epoch : 641, train loss : 0.5557275553544362\n",
            "Epoch : 641, val loss : 0.5650701052264163\n",
            "\n",
            "Epoch : 642, train loss : 0.5636037143793973\n",
            "Epoch : 642, val loss : 0.5575229509880668\n",
            "\n",
            "Epoch : 643, train loss : 0.5561692436536154\n",
            "Epoch : 643, val loss : 0.5553876773307197\n",
            "\n",
            "Epoch : 644, train loss : 0.5563792113101841\n",
            "Epoch : 644, val loss : 0.5572868240507025\n",
            "\n",
            "Epoch : 645, train loss : 0.559852975245678\n",
            "Epoch : 645, val loss : 0.5630306240759398\n",
            "\n",
            "Epoch : 646, train loss : 0.5554891667582772\n",
            "Epoch : 646, val loss : 0.5523813272777357\n",
            "\n",
            "Epoch : 647, train loss : 0.5567299765167811\n",
            "Epoch : 647, val loss : 0.5792773491457888\n",
            "\n",
            "Epoch : 648, train loss : 0.555546437429659\n",
            "Epoch : 648, val loss : 0.5562754791033895\n",
            "\n",
            "Epoch : 649, train loss : 0.5563018946936635\n",
            "Epoch : 649, val loss : 0.5547036308991281\n",
            "\n",
            "Epoch : 650, train loss : 0.5544837827032265\n",
            "Epoch : 650, val loss : 0.5569196092455011\n",
            "\n",
            "Epoch : 651, train loss : 0.5544680832010326\n",
            "Epoch : 651, val loss : 0.5565764182492307\n",
            "\n",
            "Epoch : 652, train loss : 0.5564758669246328\n",
            "Epoch : 652, val loss : 0.5558990396951375\n",
            "\n",
            "Epoch : 653, train loss : 0.5535830577214557\n",
            "Epoch : 653, val loss : 0.5608549808201035\n",
            "\n",
            "Epoch : 654, train loss : 0.5537336362130717\n",
            "Epoch : 654, val loss : 0.5727908046622026\n",
            "\n",
            "Epoch : 655, train loss : 0.5538174282420766\n",
            "Epoch : 655, val loss : 0.5586798285183153\n",
            "\n",
            "Epoch : 656, train loss : 0.5552091286037908\n",
            "Epoch : 656, val loss : 0.5566925861333546\n",
            "\n",
            "Epoch : 657, train loss : 0.5551088965300355\n",
            "Epoch : 657, val loss : 0.5537829446165184\n",
            "\n",
            "Epoch : 658, train loss : 0.5545486773505353\n",
            "Epoch : 658, val loss : 0.5572201939005601\n",
            "\n",
            "Epoch : 659, train loss : 0.5537793116136026\n",
            "Epoch : 659, val loss : 0.5579330591779005\n",
            "\n",
            "Epoch : 660, train loss : 0.5532032625241715\n",
            "Epoch : 660, val loss : 0.555923055661352\n",
            "\n",
            "Epoch : 661, train loss : 0.5534794798403078\n",
            "Epoch : 661, val loss : 0.5615257620811461\n",
            "\n",
            "Epoch : 662, train loss : 0.5531010226769881\n",
            "Epoch : 662, val loss : 0.5535153479952561\n",
            "\n",
            "Epoch : 663, train loss : 0.5533579367579837\n",
            "Epoch : 663, val loss : 0.5509176003305535\n",
            "\n",
            "Epoch : 664, train loss : 0.5541443942171158\n",
            "Epoch : 664, val loss : 0.550943037396983\n",
            "\n",
            "Epoch : 665, train loss : 0.5532190021240347\n",
            "Epoch : 665, val loss : 0.5469833832038077\n",
            "\n",
            "Epoch : 666, train loss : 0.5528782125675318\n",
            "Epoch : 666, val loss : 0.5550024242777573\n",
            "\n",
            "Epoch : 667, train loss : 0.5536816593372459\n",
            "Epoch : 667, val loss : 0.5585072730716907\n",
            "\n",
            "Epoch : 668, train loss : 0.552765693809047\n",
            "Epoch : 668, val loss : 0.5605477957349074\n",
            "\n",
            "Epoch : 669, train loss : 0.5536114396470966\n",
            "Epoch : 669, val loss : 0.5537197683986865\n",
            "\n",
            "Epoch : 670, train loss : 0.5524613268447642\n",
            "Epoch : 670, val loss : 0.5551307452352424\n",
            "\n",
            "Epoch : 671, train loss : 0.5520656452034457\n",
            "Epoch : 671, val loss : 0.5519523291211378\n",
            "\n",
            "Epoch : 672, train loss : 0.55271165136135\n",
            "Epoch : 672, val loss : 0.5627771349329699\n",
            "\n",
            "Epoch : 673, train loss : 0.5533141515471717\n",
            "Epoch : 673, val loss : 0.5572073789019334\n",
            "\n",
            "Epoch : 674, train loss : 0.5533333709745698\n",
            "Epoch : 674, val loss : 0.5509139095482072\n",
            "\n",
            "Epoch : 675, train loss : 0.551641551292304\n",
            "Epoch : 675, val loss : 0.5525948420951241\n",
            "\n",
            "Epoch : 676, train loss : 0.551163563041976\n",
            "Epoch : 676, val loss : 0.5516962283536008\n",
            "\n",
            "Epoch : 677, train loss : 0.5517173922423164\n",
            "Epoch : 677, val loss : 0.5493682826820173\n",
            "\n",
            "Epoch : 678, train loss : 0.551253055261843\n",
            "Epoch : 678, val loss : 0.556300069156446\n",
            "\n",
            "Epoch : 679, train loss : 0.5522618544824193\n",
            "Epoch : 679, val loss : 0.5512883035760175\n",
            "\n",
            "Epoch : 680, train loss : 0.5534758670763535\n",
            "Epoch : 680, val loss : 0.5634262734337857\n",
            "\n",
            "Epoch : 681, train loss : 0.5527620830319145\n",
            "Epoch : 681, val loss : 0.5502072917787653\n",
            "\n",
            "Epoch : 682, train loss : 0.5511710165124949\n",
            "Epoch : 682, val loss : 0.5490491986274719\n",
            "\n",
            "Epoch : 683, train loss : 0.5509141833493207\n",
            "Epoch : 683, val loss : 0.5515145831986477\n",
            "\n",
            "Epoch : 684, train loss : 0.5509150942166646\n",
            "Epoch : 684, val loss : 0.5532429155550506\n",
            "\n",
            "Epoch : 685, train loss : 0.5499487956364951\n",
            "Epoch : 685, val loss : 0.5537062334386926\n",
            "\n",
            "Epoch : 686, train loss : 0.5494181053204971\n",
            "Epoch : 686, val loss : 0.5534435730231435\n",
            "\n",
            "Epoch : 687, train loss : 0.5505522402850067\n",
            "Epoch : 687, val loss : 0.5483527858006326\n",
            "\n",
            "Epoch : 688, train loss : 0.5499121552163905\n",
            "Epoch : 688, val loss : 0.5580897629261017\n",
            "\n",
            "Epoch : 689, train loss : 0.5506724764000286\n",
            "Epoch : 689, val loss : 0.5608519927451485\n",
            "\n",
            "Epoch : 690, train loss : 0.5500409218398007\n",
            "Epoch : 690, val loss : 0.5475647590662305\n",
            "\n",
            "Epoch : 691, train loss : 0.549914042516188\n",
            "Epoch : 691, val loss : 0.5556460634658211\n",
            "\n",
            "Epoch : 692, train loss : 0.5498818527568472\n",
            "Epoch : 692, val loss : 0.557809098770744\n",
            "\n",
            "Epoch : 693, train loss : 0.5500019483494037\n",
            "Epoch : 693, val loss : 0.5565390445684131\n",
            "\n",
            "Epoch : 694, train loss : 0.5507257194230052\n",
            "Epoch : 694, val loss : 0.5515176183299014\n",
            "\n",
            "Epoch : 695, train loss : 0.5496009613528398\n",
            "Epoch : 695, val loss : 0.551294417757737\n",
            "\n",
            "Epoch : 696, train loss : 0.5497308756365921\n",
            "Epoch : 696, val loss : 0.546675653834092\n",
            "\n",
            "Epoch : 697, train loss : 0.5490509652730192\n",
            "Epoch : 697, val loss : 0.5468289052185259\n",
            "\n",
            "Epoch : 698, train loss : 0.5489188877019019\n",
            "Epoch : 698, val loss : 0.5526538459878219\n",
            "\n",
            "Epoch : 699, train loss : 0.5488865510983902\n",
            "Epoch : 699, val loss : 0.548194579387966\n",
            "\n",
            "Epoch : 700, train loss : 0.5502643635778719\n",
            "Epoch : 700, val loss : 0.560151236621957\n",
            "\n",
            "Epoch : 701, train loss : 0.5496001444079657\n",
            "Epoch : 701, val loss : 0.5541841638715643\n",
            "\n",
            "Epoch : 702, train loss : 0.5479442170172028\n",
            "Epoch : 702, val loss : 0.5557062782739337\n",
            "\n",
            "Epoch : 703, train loss : 0.549313937353365\n",
            "Epoch : 703, val loss : 0.5513756745739987\n",
            "\n",
            "Epoch : 704, train loss : 0.5492205126719042\n",
            "Epoch : 704, val loss : 0.5555036554211065\n",
            "\n",
            "Epoch : 705, train loss : 0.548116113019712\n",
            "Epoch : 705, val loss : 0.5547836736628883\n",
            "\n",
            "Epoch : 706, train loss : 0.5481221818562714\n",
            "Epoch : 706, val loss : 0.5497694109615526\n",
            "\n",
            "Epoch : 707, train loss : 0.5491022886651938\n",
            "Epoch : 707, val loss : 0.565176033659985\n",
            "\n",
            "Epoch : 708, train loss : 0.5491419846361335\n",
            "Epoch : 708, val loss : 0.5520113094856866\n",
            "\n",
            "Epoch : 709, train loss : 0.5478982746601104\n",
            "Epoch : 709, val loss : 0.5763114141790491\n",
            "\n",
            "Epoch : 710, train loss : 0.5477951569990677\n",
            "Epoch : 710, val loss : 0.5450173083104584\n",
            "\n",
            "Epoch : 711, train loss : 0.5474435876716269\n",
            "Epoch : 711, val loss : 0.5398849703763661\n",
            "\n",
            "Epoch : 712, train loss : 0.547918086160313\n",
            "Epoch : 712, val loss : 0.554424419214851\n",
            "\n",
            "Epoch : 713, train loss : 0.5471808303486219\n",
            "Epoch : 713, val loss : 0.5488842412045128\n",
            "\n",
            "Epoch : 714, train loss : 0.5483231815424834\n",
            "Epoch : 714, val loss : 0.5411679556495266\n",
            "\n",
            "Epoch : 715, train loss : 0.5472651497884232\n",
            "Epoch : 715, val loss : 0.5527886199323755\n",
            "\n",
            "Epoch : 716, train loss : 0.5481777010541974\n",
            "Epoch : 716, val loss : 0.5681552243860144\n",
            "\n",
            "Epoch : 717, train loss : 0.546950751181805\n",
            "Epoch : 717, val loss : 0.5534253779210543\n",
            "\n",
            "Epoch : 718, train loss : 0.547395383227955\n",
            "Epoch : 718, val loss : 0.5515610371765338\n",
            "\n",
            "Epoch : 719, train loss : 0.546438988411065\n",
            "Epoch : 719, val loss : 0.5523784380210074\n",
            "\n",
            "Epoch : 720, train loss : 0.5458191132906715\n",
            "Epoch : 720, val loss : 0.551115743423763\n",
            "\n",
            "Epoch : 721, train loss : 0.5469764534271127\n",
            "Epoch : 721, val loss : 0.5394968312037619\n",
            "\n",
            "Epoch : 722, train loss : 0.5461716530901014\n",
            "Epoch : 722, val loss : 0.5733586439960882\n",
            "\n",
            "Epoch : 723, train loss : 0.5452950513724123\n",
            "Epoch : 723, val loss : 0.5588931409936202\n",
            "\n",
            "Epoch : 724, train loss : 0.547876103357835\n",
            "Epoch : 724, val loss : 0.552413882393586\n",
            "\n",
            "Epoch : 725, train loss : 0.5463986797766252\n",
            "Epoch : 725, val loss : 0.5456662554489939\n",
            "\n",
            "Epoch : 726, train loss : 0.5452571251175621\n",
            "Epoch : 726, val loss : 0.5356874497313249\n",
            "\n",
            "Epoch : 727, train loss : 0.5456879630233299\n",
            "Epoch : 727, val loss : 0.5492831092131765\n",
            "\n",
            "Epoch : 728, train loss : 0.5456208682421481\n",
            "Epoch : 728, val loss : 0.5787541113401714\n",
            "\n",
            "Epoch : 729, train loss : 0.5460558459614262\n",
            "Epoch : 729, val loss : 0.5484230345801303\n",
            "\n",
            "Epoch : 730, train loss : 0.5449386766462613\n",
            "Epoch : 730, val loss : 0.5554232958116029\n",
            "\n",
            "Epoch : 731, train loss : 0.5440990523858504\n",
            "Epoch : 731, val loss : 0.5491482333133095\n",
            "\n",
            "Epoch : 732, train loss : 0.5450242644006558\n",
            "Epoch : 732, val loss : 0.5489481781658373\n",
            "\n",
            "Epoch : 733, train loss : 0.5449652417139577\n",
            "Epoch : 733, val loss : 0.5725261412168804\n",
            "\n",
            "Epoch : 734, train loss : 0.5447834526047566\n",
            "Epoch : 734, val loss : 0.5509469979687742\n",
            "\n",
            "Epoch : 735, train loss : 0.5447045456279406\n",
            "Epoch : 735, val loss : 0.5513067621933786\n",
            "\n",
            "Epoch : 736, train loss : 0.5447392761707307\n",
            "Epoch : 736, val loss : 0.55023613728975\n",
            "\n",
            "Epoch : 737, train loss : 0.5442995140046785\n",
            "Epoch : 737, val loss : 0.5459995489371451\n",
            "\n",
            "Epoch : 738, train loss : 0.5436331517768629\n",
            "Epoch : 738, val loss : 0.5411527439167626\n",
            "\n",
            "Epoch : 739, train loss : 0.5441997163223495\n",
            "Epoch : 739, val loss : 0.5545020511275842\n",
            "\n",
            "Epoch : 740, train loss : 0.5441921660394382\n",
            "Epoch : 740, val loss : 0.5468209574097082\n",
            "\n",
            "Epoch : 741, train loss : 0.5478619266640056\n",
            "Epoch : 741, val loss : 0.5486268856023487\n",
            "\n",
            "Epoch : 742, train loss : 0.5438899909005022\n",
            "Epoch : 742, val loss : 0.5507352289400603\n",
            "\n",
            "Epoch : 743, train loss : 0.5426547352111702\n",
            "Epoch : 743, val loss : 0.5511723552879536\n",
            "\n",
            "Epoch : 744, train loss : 0.5430582192811098\n",
            "Epoch : 744, val loss : 0.5476852718152497\n",
            "\n",
            "Epoch : 745, train loss : 0.543412910446976\n",
            "Epoch : 745, val loss : 0.5495319899759795\n",
            "\n",
            "Epoch : 746, train loss : 0.5435743738304484\n",
            "Epoch : 746, val loss : 0.5564977432552137\n",
            "\n",
            "Epoch : 747, train loss : 0.5430026961095403\n",
            "Epoch : 747, val loss : 0.5475051999092103\n",
            "\n",
            "Epoch : 748, train loss : 0.5430451391321239\n",
            "Epoch : 748, val loss : 0.5421425010028638\n",
            "\n",
            "Epoch : 749, train loss : 0.5436189156590088\n",
            "Epoch : 749, val loss : 0.5408047970972564\n",
            "\n",
            "Epoch : 750, train loss : 0.5436248735948042\n",
            "Epoch : 750, val loss : 0.5455042766897302\n",
            "\n",
            "Epoch : 751, train loss : 0.5440020293900458\n",
            "Epoch : 751, val loss : 0.5508389692557485\n",
            "\n",
            "Epoch : 752, train loss : 0.5431021737329889\n",
            "Epoch : 752, val loss : 0.5441499587736632\n",
            "\n",
            "Epoch : 753, train loss : 0.5428272890322137\n",
            "Epoch : 753, val loss : 0.5493918861213484\n",
            "\n",
            "Epoch : 754, train loss : 0.5407008665980715\n",
            "Epoch : 754, val loss : 0.53799968957901\n",
            "\n",
            "Epoch : 755, train loss : 0.5420546369119124\n",
            "Epoch : 755, val loss : 0.5375299312566457\n",
            "\n",
            "Epoch : 756, train loss : 0.542505150672161\n",
            "Epoch : 756, val loss : 0.5456589225091433\n",
            "\n",
            "Epoch : 757, train loss : 0.5421383268905408\n",
            "Epoch : 757, val loss : 0.5471969874281631\n",
            "\n",
            "Epoch : 758, train loss : 0.5439738828124421\n",
            "Epoch : 758, val loss : 0.5511005567876917\n",
            "\n",
            "Epoch : 759, train loss : 0.5412909126642977\n",
            "Epoch : 759, val loss : 0.5585565519960303\n",
            "\n",
            "Epoch : 760, train loss : 0.5417289630933244\n",
            "Epoch : 760, val loss : 0.5504024201317838\n",
            "\n",
            "Epoch : 761, train loss : 0.5413397415117784\n",
            "Epoch : 761, val loss : 0.5437153483691969\n",
            "\n",
            "Epoch : 762, train loss : 0.5415715204946924\n",
            "Epoch : 762, val loss : 0.544494567733062\n",
            "\n",
            "Epoch : 763, train loss : 0.5415773371855415\n",
            "Epoch : 763, val loss : 0.5487377220078518\n",
            "\n",
            "Epoch : 764, train loss : 0.5414708477078063\n",
            "Epoch : 764, val loss : 0.5455222161192642\n",
            "\n",
            "Epoch : 765, train loss : 0.5417626064835173\n",
            "Epoch : 765, val loss : 0.5498400565825011\n",
            "\n",
            "Epoch : 766, train loss : 0.5418497304121653\n",
            "Epoch : 766, val loss : 0.5567272368230318\n",
            "\n",
            "Epoch : 767, train loss : 0.5400112968502622\n",
            "Epoch : 767, val loss : 0.5461930522793217\n",
            "\n",
            "Epoch : 768, train loss : 0.5416740271178158\n",
            "Epoch : 768, val loss : 0.5544267343847376\n",
            "\n",
            "Epoch : 769, train loss : 0.5415696785305487\n",
            "Epoch : 769, val loss : 0.5712684502727109\n",
            "\n",
            "Epoch : 770, train loss : 0.5409983351375116\n",
            "Epoch : 770, val loss : 0.555663420965797\n",
            "\n",
            "Epoch : 771, train loss : 0.5405499998367195\n",
            "Epoch : 771, val loss : 0.5479817657094254\n",
            "\n",
            "Epoch : 772, train loss : 0.5406462347868716\n",
            "Epoch : 772, val loss : 0.5422862579948023\n",
            "\n",
            "Epoch : 773, train loss : 0.5404971196795955\n",
            "Epoch : 773, val loss : 0.5353218771909413\n",
            "\n",
            "Epoch : 774, train loss : 0.5402878911206217\n",
            "Epoch : 774, val loss : 0.5571543621389489\n",
            "\n",
            "Epoch : 775, train loss : 0.5405131379763283\n",
            "Epoch : 775, val loss : 0.547575089492296\n",
            "\n",
            "Epoch : 776, train loss : 0.5400817191962041\n",
            "Epoch : 776, val loss : 0.5401280020412645\n",
            "\n",
            "Epoch : 777, train loss : 0.5421738348223945\n",
            "Epoch : 777, val loss : 0.5336721915947764\n",
            "\n",
            "Epoch : 778, train loss : 0.5411443538738017\n",
            "Epoch : 778, val loss : 0.5492601128000962\n",
            "\n",
            "Epoch : 779, train loss : 0.5405676693627328\n",
            "Epoch : 779, val loss : 0.5579984674328252\n",
            "\n",
            "Epoch : 780, train loss : 0.5398604188904613\n",
            "Epoch : 780, val loss : 0.5369706059757032\n",
            "\n",
            "Epoch : 781, train loss : 0.5396229796337358\n",
            "Epoch : 781, val loss : 0.545966520121223\n",
            "\n",
            "Epoch : 782, train loss : 0.5393580378908102\n",
            "Epoch : 782, val loss : 0.5379906343786339\n",
            "\n",
            "Epoch : 783, train loss : 0.5395326374155103\n",
            "Epoch : 783, val loss : 0.5429272902639289\n",
            "\n",
            "Epoch : 784, train loss : 0.5392038574724487\n",
            "Epoch : 784, val loss : 0.5440714704362969\n",
            "\n",
            "Epoch : 785, train loss : 0.5390657605546895\n",
            "Epoch : 785, val loss : 0.5748404436989835\n",
            "\n",
            "Epoch : 786, train loss : 0.5393470673850089\n",
            "Epoch : 786, val loss : 0.5452881439736015\n",
            "\n",
            "Epoch : 787, train loss : 0.5403858143271821\n",
            "Epoch : 787, val loss : 0.5419744159045973\n",
            "\n",
            "Epoch : 788, train loss : 0.5384371459484101\n",
            "Epoch : 788, val loss : 0.5426867368974184\n",
            "\n",
            "Epoch : 789, train loss : 0.5388834853967029\n",
            "Epoch : 789, val loss : 0.5463048360849682\n",
            "\n",
            "Epoch : 790, train loss : 0.5374159059741281\n",
            "Epoch : 790, val loss : 0.5408109442183845\n",
            "\n",
            "Epoch : 791, train loss : 0.5378440139871657\n",
            "Epoch : 791, val loss : 0.5488923788070679\n",
            "\n",
            "Epoch : 792, train loss : 0.5383410538687852\n",
            "Epoch : 792, val loss : 0.5501343529475363\n",
            "\n",
            "Epoch : 793, train loss : 0.5395199959928337\n",
            "Epoch : 793, val loss : 0.537341174326445\n",
            "\n",
            "Epoch : 794, train loss : 0.5384146377895817\n",
            "Epoch : 794, val loss : 0.5458807459003047\n",
            "\n",
            "Epoch : 795, train loss : 0.538709237539407\n",
            "Epoch : 795, val loss : 0.5410552181695638\n",
            "\n",
            "Epoch : 796, train loss : 0.5388329500501803\n",
            "Epoch : 796, val loss : 0.5497610286662453\n",
            "\n",
            "Epoch : 797, train loss : 0.5425680055762783\n",
            "Epoch : 797, val loss : 0.5373132636672571\n",
            "\n",
            "Epoch : 798, train loss : 0.5370419034452147\n",
            "Epoch : 798, val loss : 0.5411441106545297\n",
            "\n",
            "Epoch : 799, train loss : 0.5391081616734011\n",
            "Epoch : 799, val loss : 0.5456005507393887\n",
            "\n",
            "Epoch : 800, train loss : 0.5381118266871481\n",
            "Epoch : 800, val loss : 0.5488623851223996\n",
            "0: 13.382003784179688 / 165: 0.08110305323745265\n",
            "1: 11.02829647064209 / 165: 0.14794121366558655\n",
            "2: 5.504019737243652 / 165: 0.1812989090428208\n",
            "3: 7.835084915161133 / 165: 0.2287842721650095\n",
            "4: 10.76263427734375 / 165: 0.29401235869436554\n",
            "5: 8.953612327575684 / 165: 0.3482766758311879\n",
            "6: 7.499244689941406 / 165: 0.39372664364901455\n",
            "7: 10.896038055419922 / 165: 0.4597632379242868\n",
            "8: 7.696144104003906 / 165: 0.5064065355243105\n",
            "9: 7.368635177612305 / 165: 0.5510649305401426\n",
            "10: 8.842025756835938 / 165: 0.6046529654300574\n",
            "11: 9.404233932495117 / 165: 0.6616483225966945\n",
            "12: 7.33229923248291 / 165: 0.7060864997632575\n",
            "13: 13.993741989135742 / 165: 0.7908970572731712\n",
            "14: 5.478473663330078 / 165: 0.8240999279600201\n",
            "15: 3.8611979484558105 / 165: 0.8475011276476311\n",
            "16: 5.334620952606201 / 165: 0.8798321637240323\n",
            "17: 5.087220191955566 / 165: 0.9106638012510357\n",
            "18: 10.222105979919434 / 165: 0.9726159587050929\n",
            "19: 7.434081554412842 / 165: 1.017670998428807\n",
            "20: 7.294243812561035 / 165: 1.0618785366867527\n",
            "21: 8.890395164489746 / 165: 1.115759719501842\n",
            "22: 10.837701797485352 / 165: 1.181442760698723\n",
            "23: 6.661327362060547 / 165: 1.221814441680908\n",
            "24: 8.63331413269043 / 165: 1.2741375576366076\n",
            "25: 5.187873840332031 / 165: 1.3055792172749836\n",
            "26: 6.842158317565918 / 165: 1.3470468434420497\n",
            "27: 7.162372589111328 / 165: 1.3904551621639365\n",
            "28: 5.708020210266113 / 165: 1.425049224044337\n",
            "29: 11.1927490234375 / 165: 1.492884066610625\n",
            "30: 6.840206623077393 / 165: 1.5343398643262454\n",
            "31: 6.822281837463379 / 165: 1.5756870269775385\n",
            "32: 7.437615394592285 / 165: 1.6207634839144613\n",
            "33: 8.927326202392578 / 165: 1.6748684912016891\n",
            "34: 5.225586891174316 / 165: 1.7065387147845639\n",
            "35: 6.825139045715332 / 165: 1.7479031938495053\n",
            "36: 4.980635643005371 / 165: 1.7780888644131743\n",
            "37: 6.916330814361572 / 165: 1.8200060208638504\n",
            "38: 6.6818037033081055 / 165: 1.8605018008838996\n",
            "39: 7.067521095275879 / 165: 1.9033352620673898\n",
            "40: 12.131938934326172 / 165: 1.9768621646996696\n",
            "41: 8.86801528930664 / 165: 2.0306077119075887\n",
            "42: 5.169643878936768 / 165: 2.061938886931448\n",
            "43: 5.037153244018555 / 165: 2.092467088410348\n",
            "44: 5.010009288787842 / 165: 2.122830781069668\n",
            "45: 12.291593551635742 / 165: 2.197325287443218\n",
            "46: 8.558258056640625 / 165: 2.2491935180895246\n",
            "47: 6.546408176422119 / 165: 2.28886871915875\n",
            "48: 5.378459930419922 / 165: 2.3214654460097797\n",
            "49: 3.3300588130950928 / 165: 2.3416476206345984\n",
            "50: 6.720198631286621 / 165: 2.3823760971878505\n",
            "51: 6.559481143951416 / 165: 2.4221305283633137\n",
            "52: 3.232043504714966 / 165: 2.441718670816132\n",
            "53: 8.260702133178711 / 165: 2.491783532229336\n",
            "54: 6.477471828460693 / 165: 2.5310409372503098\n",
            "55: 6.650756359100342 / 165: 2.5713485515478878\n",
            "56: 7.8970627784729 / 165: 2.619209538084087\n",
            "57: 8.13890266418457 / 165: 2.668536220897327\n",
            "58: 3.583221673965454 / 165: 2.690252715891057\n",
            "59: 6.490085601806641 / 165: 2.7295865680232185\n",
            "60: 4.824599266052246 / 165: 2.75882656357505\n",
            "61: 3.2654953002929688 / 165: 2.778617444182886\n",
            "62: 5.180408000946045 / 165: 2.810013856309832\n",
            "63: 10.191436767578125 / 165: 2.8717801397496996\n",
            "64: 8.044589042663574 / 165: 2.9205352248567515\n",
            "65: 9.732885360717773 / 165: 2.9795224088611016\n",
            "66: 3.3202672004699707 / 165: 2.9996452403791016\n",
            "67: 6.5631208419799805 / 165: 3.0394217303304956\n",
            "68: 3.344369649887085 / 165: 3.059690637299508\n",
            "69: 7.609394550323486 / 165: 3.1058081800287414\n",
            "70: 7.883159637451172 / 165: 3.153584905104203\n",
            "71: 11.216007232666016 / 165: 3.2215607065143\n",
            "72: 7.883570194244385 / 165: 3.269339919812751\n",
            "73: 4.7262163162231445 / 165: 3.297983655062588\n",
            "74: 6.149201393127441 / 165: 3.3352515422936633\n",
            "75: 6.594484329223633 / 165: 3.3752181139859276\n",
            "76: 6.391180992126465 / 165: 3.4139525442412397\n",
            "77: 7.931199073791504 / 165: 3.462020417415734\n",
            "78: 6.640491485595703 / 165: 3.5022658203587382\n",
            "79: 7.870749473571777 / 165: 3.5499673323197793\n",
            "80: 6.285910606384277 / 165: 3.58806376023726\n",
            "81: 7.561425685882568 / 165: 3.633890582575942\n",
            "82: 1.4937724699848332e-05 / 165: 3.633890673107607\n",
            "83: 6.120760917663574 / 165: 3.6709861938207196\n",
            "84: 8.709997177124023 / 165: 3.723774055500259\n",
            "85: 6.258092880249023 / 165: 3.761701891138132\n",
            "86: 7.330749034881592 / 165: 3.8061306731677176\n",
            "87: 5.783850193023682 / 165: 3.8411843107011943\n",
            "88: 6.032898426055908 / 165: 3.8777473314651694\n",
            "89: 5.975799560546875 / 165: 3.9139642984987866\n",
            "90: 6.184627532958984 / 165: 3.9514468896076287\n",
            "91: 4.542757034301758 / 165: 3.978978750421579\n",
            "92: 5.821885585784912 / 165: 4.014262905486942\n",
            "93: 5.793517112731934 / 165: 4.04937513041259\n",
            "94: 5.845190525054932 / 165: 4.084800527534135\n",
            "95: 4.188656806945801 / 165: 4.11018632636411\n",
            "96: 4.5366411209106445 / 165: 4.137681121036295\n",
            "97: 6.02726936340332 / 165: 4.1742100262690425\n",
            "98: 4.558876037597656 / 165: 4.201839578012058\n",
            "99: 3.8549327850341797 / 165: 4.225202807012265\n",
            "100: 4.286190032958984 / 165: 4.251179716302926\n",
            "101: 7.373232841491699 / 165: 4.29586597594833\n",
            "102: 4.499385833740234 / 165: 4.323134981001301\n",
            "103: 5.902925968170166 / 165: 4.358910289899302\n",
            "104: 5.98101282119751 / 165: 4.395158852452014\n",
            "105: 6.953690528869629 / 165: 4.43730243141486\n",
            "106: 2.719940662384033 / 165: 4.4537869202777935\n",
            "107: 8.588997840881348 / 165: 4.505841452646772\n",
            "108: 2.7396678924560547 / 165: 4.522445500479839\n",
            "109: 7.204819679260254 / 165: 4.566111074293538\n",
            "110: 5.9108099937438965 / 165: 4.601934165164713\n",
            "111: 2.7366394996643066 / 165: 4.618519859102072\n",
            "112: 7.010928153991699 / 165: 4.661010332762628\n",
            "113: 7.139726638793945 / 165: 4.704281403300773\n",
            "114: 6.801445960998535 / 165: 4.745502287912886\n",
            "115: 5.491547584533691 / 165: 4.778784394485817\n",
            "116: 4.219363212585449 / 165: 4.804356292743911\n",
            "117: 5.507521152496338 / 165: 4.837735208819646\n",
            "118: 4.145134925842285 / 165: 4.862857238673236\n",
            "119: 5.463268280029297 / 165: 4.895967955521899\n",
            "120: 8.111183166503906 / 165: 4.945126641379498\n",
            "121: 8.016422271728516 / 165: 4.993711018783913\n",
            "122: 1.3722403049468994 / 165: 5.002027626692683\n",
            "123: 4.187361717224121 / 165: 5.027405576494041\n",
            "124: 5.602661609649658 / 165: 5.061361101401008\n",
            "125: 5.326360702514648 / 165: 5.093642075355643\n",
            "126: 2.6892614364624023 / 165: 5.109940629516021\n",
            "127: 7.9963698387146 / 165: 5.1584034770233815\n",
            "128: 5.344510555267334 / 165: 5.190794450085608\n",
            "129: 5.084295272827148 / 165: 5.2216083608300154\n",
            "130: 7.942117214202881 / 165: 5.269742404552457\n",
            "131: 3.7708616256713867 / 165: 5.292596111374708\n",
            "132: 1.306593894958496 / 165: 5.3005148622532445\n",
            "133: 2.6156673431396484 / 165: 5.316367391605606\n",
            "134: 4.056274890899658 / 165: 5.340950875792877\n",
            "135: 5.196122646331787 / 165: 5.372442528194887\n",
            "136: 6.224771499633789 / 165: 5.410168416071456\n",
            "137: 7.672616958618164 / 165: 5.456669124911566\n",
            "138: 7.81169319152832 / 165: 5.504012720011738\n",
            "139: 6.26295280456543 / 165: 5.541970009736376\n",
            "140: 3.9742627143859863 / 165: 5.566056450429625\n",
            "141: 3.854492425918579 / 165: 5.589417010586707\n",
            "142: 5.079907417297363 / 165: 5.620204328267297\n",
            "143: 5.192261695861816 / 165: 5.65167258096949\n",
            "144: 5.2519683837890625 / 165: 5.683502692386393\n",
            "145: 4.081050872802734 / 165: 5.708236334039744\n",
            "146: 6.212767601013184 / 165: 5.745889471015581\n",
            "147: 5.737788677215576 / 165: 5.780663947847191\n",
            "148: 3.7204904556274414 / 165: 5.803212374850993\n",
            "149: 3.58923602104187 / 165: 5.824965320433065\n",
            "150: 2.263406991958618 / 165: 5.838682938566147\n",
            "151: 3.6616997718811035 / 165: 5.860875058395729\n",
            "152: 8.133749961853027 / 165: 5.91017051270999\n",
            "153: 3.8546595573425293 / 165: 5.933532085784793\n",
            "154: 6.256192207336426 / 165: 5.971448402192893\n",
            "155: 3.8181819915771484 / 165: 5.994588899111542\n",
            "156: 4.2481689453125 / 165: 6.0203353775679815\n",
            "157: 5.750514030456543 / 165: 6.055186977752567\n",
            "158: 4.549519062042236 / 165: 6.082759820552823\n",
            "159: 3.547849655151367 / 165: 6.104261939674953\n",
            "160: 7.39896297454834 / 165: 6.1491041395207\n",
            "161: 4.841041564941406 / 165: 6.17844378536883\n",
            "162: 4.856090545654297 / 165: 6.207874637160674\n",
            "163: 2.498408794403076 / 165: 6.223016508641905\n",
            "164: 4.507757186889648 / 165: 6.250336249168509\n",
            "\n",
            "Epoch : 1, train loss : 6.250336249168509\n",
            "Epoch : 1, val loss : 4.199366230713694\n",
            "\n",
            "Epoch : 2, train loss : 3.167129203796415\n",
            "Epoch : 2, val loss : 1.5158611426227968\n",
            "\n",
            "Epoch : 3, train loss : 0.932546844536608\n",
            "Epoch : 3, val loss : 0.6922553056164791\n",
            "\n",
            "Epoch : 4, train loss : 0.6946026968233512\n",
            "Epoch : 4, val loss : 0.7042239433840701\n",
            "\n",
            "Epoch : 5, train loss : 0.6945099538022823\n",
            "Epoch : 5, val loss : 0.6951763441688136\n",
            "\n",
            "Epoch : 6, train loss : 0.6952005122647139\n",
            "Epoch : 6, val loss : 0.6955290719082482\n",
            "\n",
            "Epoch : 7, train loss : 0.6959424502921828\n",
            "Epoch : 7, val loss : 0.6960882513146651\n",
            "\n",
            "Epoch : 8, train loss : 0.6948812965190775\n",
            "Epoch : 8, val loss : 0.6956849600139416\n",
            "\n",
            "Epoch : 9, train loss : 0.6940822175054842\n",
            "Epoch : 9, val loss : 0.6875456853916772\n",
            "\n",
            "Epoch : 10, train loss : 0.6944286967768816\n",
            "Epoch : 10, val loss : 0.7193419650981301\n",
            "\n",
            "Epoch : 11, train loss : 0.6955293102697895\n",
            "Epoch : 11, val loss : 0.7025584798110157\n",
            "\n",
            "Epoch : 12, train loss : 0.695496715921344\n",
            "Epoch : 12, val loss : 0.7004469225281164\n",
            "\n",
            "Epoch : 13, train loss : 0.694967818621433\n",
            "Epoch : 13, val loss : 0.7001179017518697\n",
            "\n",
            "Epoch : 14, train loss : 0.6948866118084303\n",
            "Epoch : 14, val loss : 0.6960906323633697\n",
            "\n",
            "Epoch : 15, train loss : 0.6952537074233545\n",
            "Epoch : 15, val loss : 0.6993547144689057\n",
            "\n",
            "Epoch : 16, train loss : 0.6954927505868855\n",
            "Epoch : 16, val loss : 0.7051357595544112\n",
            "\n",
            "Epoch : 17, train loss : 0.695167895519372\n",
            "Epoch : 17, val loss : 0.6926303788235312\n",
            "\n",
            "Epoch : 18, train loss : 0.6943532997911622\n",
            "Epoch : 18, val loss : 0.7052252731825176\n",
            "\n",
            "Epoch : 19, train loss : 0.694888514099699\n",
            "Epoch : 19, val loss : 0.6940815448760986\n",
            "\n",
            "Epoch : 20, train loss : 0.6976370948733702\n",
            "Epoch : 20, val loss : 0.6982863231709129\n",
            "\n",
            "Epoch : 21, train loss : 0.6951700340617788\n",
            "Epoch : 21, val loss : 0.6978261000231692\n",
            "\n",
            "Epoch : 22, train loss : 0.6933414495352538\n",
            "Epoch : 22, val loss : 0.6921083080141167\n",
            "\n",
            "Epoch : 23, train loss : 0.6947866512067389\n",
            "Epoch : 23, val loss : 0.6885942973588641\n",
            "\n",
            "Epoch : 24, train loss : 0.695429622404503\n",
            "Epoch : 24, val loss : 0.7084749497865377\n",
            "\n",
            "Epoch : 25, train loss : 0.6953328862334742\n",
            "Epoch : 25, val loss : 0.7005439526156374\n",
            "\n",
            "Epoch : 26, train loss : 0.6962913563757235\n",
            "Epoch : 26, val loss : 0.6833674782200864\n",
            "\n",
            "Epoch : 27, train loss : 0.694920298908696\n",
            "Epoch : 27, val loss : 0.695030927658081\n",
            "\n",
            "Epoch : 28, train loss : 0.6939437360474556\n",
            "Epoch : 28, val loss : 0.6931770569399782\n",
            "\n",
            "Epoch : 29, train loss : 0.6946929841330557\n",
            "Epoch : 29, val loss : 0.6997204893513731\n",
            "\n",
            "Epoch : 30, train loss : 0.6940349683617101\n",
            "Epoch : 30, val loss : 0.6880352842180352\n",
            "\n",
            "Epoch : 31, train loss : 0.6945784568786622\n",
            "Epoch : 31, val loss : 0.694401850825862\n",
            "\n",
            "Epoch : 32, train loss : 0.6959795861533193\n",
            "Epoch : 32, val loss : 0.7110742173696819\n",
            "\n",
            "Epoch : 33, train loss : 0.6922033570029521\n",
            "Epoch : 33, val loss : 0.6857430025150901\n",
            "\n",
            "Epoch : 34, train loss : 0.6937044512141832\n",
            "Epoch : 34, val loss : 0.6895478932481063\n",
            "\n",
            "Epoch : 35, train loss : 0.6934217984026125\n",
            "Epoch : 35, val loss : 0.7010947873717861\n",
            "\n",
            "Epoch : 36, train loss : 0.6942688280885869\n",
            "Epoch : 36, val loss : 0.7036826171373064\n",
            "\n",
            "Epoch : 37, train loss : 0.6944977630268442\n",
            "Epoch : 37, val loss : 0.6926945981226469\n",
            "\n",
            "Epoch : 38, train loss : 0.6939897425246963\n",
            "Epoch : 38, val loss : 0.6985265267522711\n",
            "\n",
            "Epoch : 39, train loss : 0.6938667680277971\n",
            "Epoch : 39, val loss : 0.7051208238852652\n",
            "\n",
            "Epoch : 40, train loss : 0.694030696334261\n",
            "Epoch : 40, val loss : 0.7017165391068709\n",
            "\n",
            "Epoch : 41, train loss : 0.693675317186298\n",
            "Epoch : 41, val loss : 0.692322872186962\n",
            "\n",
            "Epoch : 42, train loss : 0.693591623234026\n",
            "Epoch : 42, val loss : 0.6972218689165619\n",
            "\n",
            "Epoch : 43, train loss : 0.6946028427644212\n",
            "Epoch : 43, val loss : 0.6959387189463566\n",
            "\n",
            "Epoch : 44, train loss : 0.6956692128470456\n",
            "Epoch : 44, val loss : 0.6932032798465929\n",
            "\n",
            "Epoch : 45, train loss : 0.6949188781507086\n",
            "Epoch : 45, val loss : 0.6924555019328468\n",
            "\n",
            "Epoch : 46, train loss : 0.6937589016827671\n",
            "Epoch : 46, val loss : 0.710125872963353\n",
            "\n",
            "Epoch : 47, train loss : 0.6927513603008155\n",
            "Epoch : 47, val loss : 0.6829562124453092\n",
            "\n",
            "Epoch : 48, train loss : 0.6950683210835312\n",
            "Epoch : 48, val loss : 0.6968201398849487\n",
            "\n",
            "Epoch : 49, train loss : 0.6942889083515512\n",
            "Epoch : 49, val loss : 0.6839869179223712\n",
            "\n",
            "Epoch : 50, train loss : 0.6934777003346067\n",
            "Epoch : 50, val loss : 0.7079249996888008\n",
            "\n",
            "Epoch : 51, train loss : 0.6946122628269775\n",
            "Epoch : 51, val loss : 0.6943357335893732\n",
            "\n",
            "Epoch : 52, train loss : 0.6949530088540278\n",
            "Epoch : 52, val loss : 0.7002230663048594\n",
            "\n",
            "Epoch : 53, train loss : 0.6929266673145871\n",
            "Epoch : 53, val loss : 0.690313016113482\n",
            "\n",
            "Epoch : 54, train loss : 0.6927857529033318\n",
            "Epoch : 54, val loss : 0.695239559600228\n",
            "\n",
            "Epoch : 55, train loss : 0.6943246978701969\n",
            "Epoch : 55, val loss : 0.6962648473287883\n",
            "\n",
            "Epoch : 56, train loss : 0.6919205918456567\n",
            "Epoch : 56, val loss : 0.7108407867582222\n",
            "\n",
            "Epoch : 57, train loss : 0.6950377749674254\n",
            "Epoch : 57, val loss : 0.6944490263336582\n",
            "\n",
            "Epoch : 58, train loss : 0.6940184195836384\n",
            "Epoch : 58, val loss : 0.7042231685236882\n",
            "\n",
            "Epoch : 59, train loss : 0.6927597793665804\n",
            "Epoch : 59, val loss : 0.6903529920076068\n",
            "\n",
            "Epoch : 60, train loss : 0.692795009324045\n",
            "Epoch : 60, val loss : 0.7003556144864935\n",
            "\n",
            "Epoch : 61, train loss : 0.6948185014002248\n",
            "Epoch : 61, val loss : 0.6815049899251837\n",
            "\n",
            "Epoch : 62, train loss : 0.6955858725489991\n",
            "Epoch : 62, val loss : 0.6925788427654065\n",
            "\n",
            "Epoch : 63, train loss : 0.6934694929556413\n",
            "Epoch : 63, val loss : 0.7074064047713029\n",
            "\n",
            "Epoch : 64, train loss : 0.6925948182741798\n",
            "Epoch : 64, val loss : 0.6837905896337408\n",
            "\n",
            "Epoch : 65, train loss : 0.6935694987123662\n",
            "Epoch : 65, val loss : 0.7027942977453534\n",
            "\n",
            "Epoch : 66, train loss : 0.693741964571404\n",
            "Epoch : 66, val loss : 0.6892575025558472\n",
            "\n",
            "Epoch : 67, train loss : 0.6936996925960885\n",
            "Epoch : 67, val loss : 0.6883740048659476\n",
            "\n",
            "Epoch : 68, train loss : 0.6929280328028127\n",
            "Epoch : 68, val loss : 0.6879859880397194\n",
            "\n",
            "Epoch : 69, train loss : 0.6939272031639561\n",
            "Epoch : 69, val loss : 0.6908777230664305\n",
            "\n",
            "Epoch : 70, train loss : 0.6932183742523191\n",
            "Epoch : 70, val loss : 0.688784310692235\n",
            "\n",
            "Epoch : 71, train loss : 0.6936557643341293\n",
            "Epoch : 71, val loss : 0.705272238505514\n",
            "\n",
            "Epoch : 72, train loss : 0.6923298474514121\n",
            "Epoch : 72, val loss : 0.6825638695767052\n",
            "\n",
            "Epoch : 73, train loss : 0.692391036857258\n",
            "Epoch : 73, val loss : 0.7078957934128611\n",
            "\n",
            "Epoch : 74, train loss : 0.6930507833307442\n",
            "Epoch : 74, val loss : 0.691371522451702\n",
            "\n",
            "Epoch : 75, train loss : 0.6940299626552701\n",
            "Epoch : 75, val loss : 0.6886931908758064\n",
            "\n",
            "Epoch : 76, train loss : 0.6935738137274078\n",
            "Epoch : 76, val loss : 0.6844577789306641\n",
            "\n",
            "Epoch : 77, train loss : 0.692099918379928\n",
            "Epoch : 77, val loss : 0.7207901728780647\n",
            "\n",
            "Epoch : 78, train loss : 0.6950309706456735\n",
            "Epoch : 78, val loss : 0.6960507756785342\n",
            "\n",
            "Epoch : 79, train loss : 0.6935736912669558\n",
            "Epoch : 79, val loss : 0.699331101618315\n",
            "\n",
            "Epoch : 80, train loss : 0.691774304707845\n",
            "Epoch : 80, val loss : 0.7000979781150819\n",
            "\n",
            "Epoch : 81, train loss : 0.6926739439819798\n",
            "Epoch : 81, val loss : 0.6915528899744937\n",
            "\n",
            "Epoch : 82, train loss : 0.6928402362447792\n",
            "Epoch : 82, val loss : 0.687180271274165\n",
            "\n",
            "Epoch : 83, train loss : 0.6931179996692775\n",
            "Epoch : 83, val loss : 0.6954190323227329\n",
            "\n",
            "Epoch : 84, train loss : 0.6927780635429149\n",
            "Epoch : 84, val loss : 0.6926950530001991\n",
            "\n",
            "Epoch : 85, train loss : 0.6933201428615684\n",
            "Epoch : 85, val loss : 0.6982773981596295\n",
            "\n",
            "Epoch : 86, train loss : 0.6924561565572568\n",
            "Epoch : 86, val loss : 0.6910831551802785\n",
            "\n",
            "Epoch : 87, train loss : 0.69390399203156\n",
            "Epoch : 87, val loss : 0.6870510295817724\n",
            "\n",
            "Epoch : 88, train loss : 0.6921017816572478\n",
            "Epoch : 88, val loss : 0.69020844760694\n",
            "\n",
            "Epoch : 89, train loss : 0.6932551051631118\n",
            "Epoch : 89, val loss : 0.6892224079684207\n",
            "\n",
            "Epoch : 90, train loss : 0.6938145688085842\n",
            "Epoch : 90, val loss : 0.687648926910601\n",
            "\n",
            "Epoch : 91, train loss : 0.6919852448232249\n",
            "Epoch : 91, val loss : 0.6919774005287573\n",
            "\n",
            "Epoch : 92, train loss : 0.6919940070672475\n",
            "Epoch : 92, val loss : 0.696759958016245\n",
            "\n",
            "Epoch : 93, train loss : 0.6931126926884504\n",
            "Epoch : 93, val loss : 0.6858978585193032\n",
            "\n",
            "Epoch : 94, train loss : 0.6921130928126251\n",
            "Epoch : 94, val loss : 0.6941208557078714\n",
            "\n",
            "Epoch : 95, train loss : 0.6944353016940028\n",
            "Epoch : 95, val loss : 0.6913725420048362\n",
            "\n",
            "Epoch : 96, train loss : 0.6929788083741163\n",
            "Epoch : 96, val loss : 0.6917059421539306\n",
            "\n",
            "Epoch : 97, train loss : 0.6918971047256932\n",
            "Epoch : 97, val loss : 0.6855880147532412\n",
            "\n",
            "Epoch : 98, train loss : 0.6926614450685908\n",
            "Epoch : 98, val loss : 0.6979257746746665\n",
            "\n",
            "Epoch : 99, train loss : 0.6917367646188445\n",
            "Epoch : 99, val loss : 0.6873572751095421\n",
            "\n",
            "Epoch : 100, train loss : 0.6925707914612511\n",
            "Epoch : 100, val loss : 0.6977919120537607\n",
            "\n",
            "Epoch : 101, train loss : 0.6915837829763236\n",
            "Epoch : 101, val loss : 0.706114185483832\n",
            "\n",
            "Epoch : 102, train loss : 0.6894325498378635\n",
            "Epoch : 102, val loss : 0.6880510010217366\n",
            "\n",
            "Epoch : 103, train loss : 0.6921306465611312\n",
            "Epoch : 103, val loss : 0.7050715810374211\n",
            "\n",
            "Epoch : 104, train loss : 0.6928179823991029\n",
            "Epoch : 104, val loss : 0.7001983146918447\n",
            "\n",
            "Epoch : 105, train loss : 0.6901719190857629\n",
            "Epoch : 105, val loss : 0.6807046407147459\n",
            "\n",
            "Epoch : 106, train loss : 0.6928405017563787\n",
            "Epoch : 106, val loss : 0.6887180052305523\n",
            "\n",
            "Epoch : 107, train loss : 0.693408458521872\n",
            "Epoch : 107, val loss : 0.6926030171544929\n",
            "\n",
            "Epoch : 108, train loss : 0.692928878466288\n",
            "Epoch : 108, val loss : 0.7000481580433092\n",
            "\n",
            "Epoch : 109, train loss : 0.6925197016109119\n",
            "Epoch : 109, val loss : 0.6957252935359353\n",
            "\n",
            "Epoch : 110, train loss : 0.6920183239561141\n",
            "Epoch : 110, val loss : 0.6946537243692499\n",
            "\n",
            "Epoch : 111, train loss : 0.691580242099184\n",
            "Epoch : 111, val loss : 0.6976292917602941\n",
            "\n",
            "Epoch : 112, train loss : 0.6930582064570802\n",
            "Epoch : 112, val loss : 0.7108512489419235\n",
            "\n",
            "Epoch : 113, train loss : 0.6939524975689977\n",
            "Epoch : 113, val loss : 0.6921644932345339\n",
            "\n",
            "Epoch : 114, train loss : 0.6930608084707547\n",
            "Epoch : 114, val loss : 0.6980447675052442\n",
            "\n",
            "Epoch : 115, train loss : 0.6917555314121827\n",
            "Epoch : 115, val loss : 0.6974594436193768\n",
            "\n",
            "Epoch : 116, train loss : 0.6937964049252595\n",
            "Epoch : 116, val loss : 0.6858315154125816\n",
            "\n",
            "Epoch : 117, train loss : 0.6921677755587026\n",
            "Epoch : 117, val loss : 0.6865905586041904\n",
            "\n",
            "Epoch : 118, train loss : 0.6926881013494549\n",
            "Epoch : 118, val loss : 0.6883815275995353\n",
            "\n",
            "Epoch : 119, train loss : 0.692114650119435\n",
            "Epoch : 119, val loss : 0.69162770007786\n",
            "\n",
            "Epoch : 120, train loss : 0.6910099217385957\n",
            "Epoch : 120, val loss : 0.6827109393320586\n",
            "\n",
            "Epoch : 121, train loss : 0.6911581844994517\n",
            "Epoch : 121, val loss : 0.7038834345968146\n",
            "\n",
            "Epoch : 122, train loss : 0.6909843556808702\n",
            "Epoch : 122, val loss : 0.6959368241460699\n",
            "\n",
            "Epoch : 123, train loss : 0.692342433784947\n",
            "Epoch : 123, val loss : 0.7007357572254382\n",
            "\n",
            "Epoch : 124, train loss : 0.6927357666420217\n",
            "Epoch : 124, val loss : 0.6898721205560785\n",
            "\n",
            "Epoch : 125, train loss : 0.6900476983099274\n",
            "Epoch : 125, val loss : 0.6856392810219212\n",
            "\n",
            "Epoch : 126, train loss : 0.6921433000853568\n",
            "Epoch : 126, val loss : 0.6981947892590572\n",
            "\n",
            "Epoch : 127, train loss : 0.6907281424060011\n",
            "Epoch : 127, val loss : 0.7052872337793048\n",
            "\n",
            "Epoch : 128, train loss : 0.6908768357652607\n",
            "Epoch : 128, val loss : 0.6899570791344894\n",
            "\n",
            "Epoch : 129, train loss : 0.6915732893076809\n",
            "Epoch : 129, val loss : 0.6948519254985608\n",
            "\n",
            "Epoch : 130, train loss : 0.691659310731021\n",
            "Epoch : 130, val loss : 0.6924718147829959\n",
            "\n",
            "Epoch : 131, train loss : 0.6909812670765496\n",
            "Epoch : 131, val loss : 0.7086878513035021\n",
            "\n",
            "Epoch : 132, train loss : 0.6920613798228182\n",
            "Epoch : 132, val loss : 0.7049517506047298\n",
            "\n",
            "Epoch : 133, train loss : 0.6912820888288094\n",
            "Epoch : 133, val loss : 0.6893270894100793\n",
            "\n",
            "Epoch : 134, train loss : 0.6919945976950908\n",
            "Epoch : 134, val loss : 0.6876089290568703\n",
            "\n",
            "Epoch : 135, train loss : 0.6906275406028287\n",
            "Epoch : 135, val loss : 0.6902992537147121\n",
            "\n",
            "Epoch : 136, train loss : 0.6907888426925197\n",
            "Epoch : 136, val loss : 0.7108105201470225\n",
            "\n",
            "Epoch : 137, train loss : 0.6933346853111726\n",
            "Epoch : 137, val loss : 0.7028381479413883\n",
            "\n",
            "Epoch : 138, train loss : 0.6913028337738731\n",
            "Epoch : 138, val loss : 0.7122562245318764\n",
            "\n",
            "Epoch : 139, train loss : 0.6916075316342443\n",
            "Epoch : 139, val loss : 0.6912481502482766\n",
            "\n",
            "Epoch : 140, train loss : 0.6907708110231339\n",
            "Epoch : 140, val loss : 0.6994175440386722\n",
            "\n",
            "Epoch : 141, train loss : 0.69244315515865\n",
            "Epoch : 141, val loss : 0.6892375851932324\n",
            "\n",
            "Epoch : 142, train loss : 0.6929709954695265\n",
            "Epoch : 142, val loss : 0.6949456741935329\n",
            "\n",
            "Epoch : 143, train loss : 0.6909373864983066\n",
            "Epoch : 143, val loss : 0.6936567927661694\n",
            "\n",
            "Epoch : 144, train loss : 0.6914768453800316\n",
            "Epoch : 144, val loss : 0.6893156515924552\n",
            "\n",
            "Epoch : 145, train loss : 0.6915992765715624\n",
            "Epoch : 145, val loss : 0.6920386646923268\n",
            "\n",
            "Epoch : 146, train loss : 0.6909629825389743\n",
            "Epoch : 146, val loss : 0.6877534797317103\n",
            "\n",
            "Epoch : 147, train loss : 0.6910310167254826\n",
            "Epoch : 147, val loss : 0.6969552008729232\n",
            "\n",
            "Epoch : 148, train loss : 0.6905917854020086\n",
            "Epoch : 148, val loss : 0.6831444407764233\n",
            "\n",
            "Epoch : 149, train loss : 0.6905477877819182\n",
            "Epoch : 149, val loss : 0.6898690744450219\n",
            "\n",
            "Epoch : 150, train loss : 0.6903348507303181\n",
            "Epoch : 150, val loss : 0.6909314990043641\n",
            "\n",
            "Epoch : 151, train loss : 0.693036379958644\n",
            "Epoch : 151, val loss : 0.6948244854023582\n",
            "\n",
            "Epoch : 152, train loss : 0.6921493486924601\n",
            "Epoch : 152, val loss : 0.6889861978982624\n",
            "\n",
            "Epoch : 153, train loss : 0.6915671247424499\n",
            "Epoch : 153, val loss : 0.7081789593947561\n",
            "\n",
            "Epoch : 154, train loss : 0.6903421669295339\n",
            "Epoch : 154, val loss : 0.6825443286644786\n",
            "\n",
            "Epoch : 155, train loss : 0.6915479970700813\n",
            "Epoch : 155, val loss : 0.6900276322113842\n",
            "\n",
            "Epoch : 156, train loss : 0.691398072603977\n",
            "Epoch : 156, val loss : 0.6934186314281664\n",
            "\n",
            "Epoch : 157, train loss : 0.6898523117556715\n",
            "Epoch : 157, val loss : 0.6989869098914296\n",
            "\n",
            "Epoch : 158, train loss : 0.6908596620415196\n",
            "Epoch : 158, val loss : 0.6894684277082745\n",
            "\n",
            "Epoch : 159, train loss : 0.6896973497939831\n",
            "Epoch : 159, val loss : 0.7078274864899486\n",
            "\n",
            "Epoch : 160, train loss : 0.6914305343772426\n",
            "Epoch : 160, val loss : 0.6931547993107846\n",
            "\n",
            "Epoch : 161, train loss : 0.6916917233756092\n",
            "Epoch : 161, val loss : 0.7156493914754768\n",
            "\n",
            "Epoch : 162, train loss : 0.6903439807169364\n",
            "Epoch : 162, val loss : 0.7018222338274904\n",
            "\n",
            "Epoch : 163, train loss : 0.6901936386570783\n",
            "Epoch : 163, val loss : 0.6912753519259002\n",
            "\n",
            "Epoch : 164, train loss : 0.6913052103736185\n",
            "Epoch : 164, val loss : 0.6973124836620531\n",
            "\n",
            "Epoch : 165, train loss : 0.6913432106827245\n",
            "Epoch : 165, val loss : 0.7015935935472187\n",
            "\n",
            "Epoch : 166, train loss : 0.6910082510023405\n",
            "Epoch : 166, val loss : 0.689915026489057\n",
            "\n",
            "Epoch : 167, train loss : 0.6908087318593804\n",
            "Epoch : 167, val loss : 0.6835933892350446\n",
            "\n",
            "Epoch : 168, train loss : 0.6897990981737769\n",
            "Epoch : 168, val loss : 0.7079632752820064\n",
            "\n",
            "Epoch : 169, train loss : 0.6882286956816008\n",
            "Epoch : 169, val loss : 0.6881595253944398\n",
            "\n",
            "Epoch : 170, train loss : 0.6902435421943665\n",
            "Epoch : 170, val loss : 0.7017006340779756\n",
            "\n",
            "Epoch : 171, train loss : 0.6917044484254081\n",
            "Epoch : 171, val loss : 0.6904339727602506\n",
            "\n",
            "Epoch : 172, train loss : 0.692447047161333\n",
            "Epoch : 172, val loss : 0.6902100632065221\n",
            "\n",
            "Epoch : 173, train loss : 0.6891688086769797\n",
            "Epoch : 173, val loss : 0.6842039415710849\n",
            "\n",
            "Epoch : 174, train loss : 0.691395210497307\n",
            "Epoch : 174, val loss : 0.6907308383991844\n",
            "\n",
            "Epoch : 175, train loss : 0.6899127266623757\n",
            "Epoch : 175, val loss : 0.6963567921989842\n",
            "\n",
            "Epoch : 176, train loss : 0.6916727925791886\n",
            "Epoch : 176, val loss : 0.6905553246799269\n",
            "\n",
            "Epoch : 177, train loss : 0.69028911337708\n",
            "Epoch : 177, val loss : 0.6872489107282539\n",
            "\n",
            "Epoch : 178, train loss : 0.689451595508691\n",
            "Epoch : 178, val loss : 0.6967986288823579\n",
            "\n",
            "Epoch : 179, train loss : 0.6896601593855654\n",
            "Epoch : 179, val loss : 0.6849634333660728\n",
            "\n",
            "Epoch : 180, train loss : 0.691025265419122\n",
            "Epoch : 180, val loss : 0.6862076646403261\n",
            "\n",
            "Epoch : 181, train loss : 0.6908360640207929\n",
            "Epoch : 181, val loss : 0.6893758836545442\n",
            "\n",
            "Epoch : 182, train loss : 0.691921106613044\n",
            "Epoch : 182, val loss : 0.6962531083508542\n",
            "\n",
            "Epoch : 183, train loss : 0.6911507660692388\n",
            "Epoch : 183, val loss : 0.6914560449750797\n",
            "\n",
            "Epoch : 184, train loss : 0.6903333270188541\n",
            "Epoch : 184, val loss : 0.6968980682523628\n",
            "\n",
            "Epoch : 185, train loss : 0.6908195806272104\n",
            "Epoch : 185, val loss : 0.6882222388920031\n",
            "\n",
            "Epoch : 186, train loss : 0.6889843738440313\n",
            "Epoch : 186, val loss : 0.6951995993915356\n",
            "\n",
            "Epoch : 187, train loss : 0.6902543335249928\n",
            "Epoch : 187, val loss : 0.6824349039479307\n",
            "\n",
            "Epoch : 188, train loss : 0.6891622359102423\n",
            "Epoch : 188, val loss : 0.6852038659547506\n",
            "\n",
            "Epoch : 189, train loss : 0.6902905684528932\n",
            "Epoch : 189, val loss : 0.7035993557227284\n",
            "\n",
            "Epoch : 190, train loss : 0.6915252364043033\n",
            "Epoch : 190, val loss : 0.6896696216181705\n",
            "\n",
            "Epoch : 191, train loss : 0.6901366905732587\n",
            "Epoch : 191, val loss : 0.6885681089602019\n",
            "\n",
            "Epoch : 192, train loss : 0.6917757298007156\n",
            "Epoch : 192, val loss : 0.6833123219640632\n",
            "\n",
            "Epoch : 193, train loss : 0.691660745938619\n",
            "Epoch : 193, val loss : 0.6976499777091176\n",
            "\n",
            "Epoch : 194, train loss : 0.6892024672392648\n",
            "Epoch : 194, val loss : 0.7040178838529084\n",
            "\n",
            "Epoch : 195, train loss : 0.6885104659831882\n",
            "Epoch : 195, val loss : 0.7011763610337911\n",
            "\n",
            "Epoch : 196, train loss : 0.6914342208342119\n",
            "Epoch : 196, val loss : 0.7059901450809679\n",
            "\n",
            "Epoch : 197, train loss : 0.6887820059602908\n",
            "Epoch : 197, val loss : 0.6881475260383205\n",
            "\n",
            "Epoch : 198, train loss : 0.6901937094601717\n",
            "Epoch : 198, val loss : 0.6894591795770746\n",
            "\n",
            "Epoch : 199, train loss : 0.6897083318594731\n",
            "Epoch : 199, val loss : 0.6932546308166102\n",
            "\n",
            "Epoch : 200, train loss : 0.6901396234830224\n",
            "Epoch : 200, val loss : 0.687736966108021\n",
            "\n",
            "Epoch : 201, train loss : 0.689906519470793\n",
            "Epoch : 201, val loss : 0.689890626229738\n",
            "\n",
            "Epoch : 202, train loss : 0.6900075525948496\n",
            "Epoch : 202, val loss : 0.6903994208888002\n",
            "\n",
            "Epoch : 203, train loss : 0.6909977331305994\n",
            "Epoch : 203, val loss : 0.6991939042743883\n",
            "\n",
            "Epoch : 204, train loss : 0.689235311565977\n",
            "Epoch : 204, val loss : 0.6901331167472037\n",
            "\n",
            "Epoch : 205, train loss : 0.6893344864700782\n",
            "Epoch : 205, val loss : 0.6895878314971924\n",
            "\n",
            "Epoch : 206, train loss : 0.6893559477545999\n",
            "Epoch : 206, val loss : 0.6945759685415972\n",
            "\n",
            "Epoch : 207, train loss : 0.6905918020190617\n",
            "Epoch : 207, val loss : 0.6951372466589275\n",
            "\n",
            "Epoch : 208, train loss : 0.6890867374160071\n",
            "Epoch : 208, val loss : 0.6861863104920639\n",
            "\n",
            "Epoch : 209, train loss : 0.6890594460747455\n",
            "Epoch : 209, val loss : 0.6947536499876724\n",
            "\n",
            "Epoch : 210, train loss : 0.6910112142562868\n",
            "Epoch : 210, val loss : 0.7025464679065503\n",
            "\n",
            "Epoch : 211, train loss : 0.690113207065698\n",
            "Epoch : 211, val loss : 0.6899737590237667\n",
            "\n",
            "Epoch : 212, train loss : 0.6905096870480163\n",
            "Epoch : 212, val loss : 0.6845862363514146\n",
            "\n",
            "Epoch : 213, train loss : 0.6895153150413976\n",
            "Epoch : 213, val loss : 0.7036246029954207\n",
            "\n",
            "Epoch : 214, train loss : 0.6903642957860776\n",
            "Epoch : 214, val loss : 0.6999738655592265\n",
            "\n",
            "Epoch : 215, train loss : 0.6895797357414707\n",
            "Epoch : 215, val loss : 0.6980588216530649\n",
            "\n",
            "Epoch : 216, train loss : 0.689703418630542\n",
            "Epoch : 216, val loss : 0.6946674522600677\n",
            "\n",
            "Epoch : 217, train loss : 0.6892947099425578\n",
            "Epoch : 217, val loss : 0.7117687244164316\n",
            "\n",
            "Epoch : 218, train loss : 0.6897397398948668\n",
            "Epoch : 218, val loss : 0.6889657723276239\n",
            "\n",
            "Epoch : 219, train loss : 0.688932191964352\n",
            "Epoch : 219, val loss : 0.6933551775781731\n",
            "\n",
            "Epoch : 220, train loss : 0.6897424842372084\n",
            "Epoch : 220, val loss : 0.685656560094733\n",
            "\n",
            "Epoch : 221, train loss : 0.6891835172971086\n",
            "Epoch : 221, val loss : 0.7032329502858615\n",
            "\n",
            "Epoch : 222, train loss : 0.687498847282294\n",
            "Epoch : 222, val loss : 0.6786857027756541\n",
            "\n",
            "Epoch : 223, train loss : 0.6904829588803377\n",
            "Epoch : 223, val loss : 0.6866371066946733\n",
            "\n",
            "Epoch : 224, train loss : 0.6894725683963654\n",
            "Epoch : 224, val loss : 0.6829278092635305\n",
            "\n",
            "Epoch : 225, train loss : 0.6892040790933552\n",
            "Epoch : 225, val loss : 0.6900361399901539\n",
            "\n",
            "Epoch : 226, train loss : 0.6884285735361501\n",
            "Epoch : 226, val loss : 0.7125090015561958\n",
            "\n",
            "Epoch : 227, train loss : 0.6888173153906157\n",
            "Epoch : 227, val loss : 0.6921947410232142\n",
            "\n",
            "Epoch : 228, train loss : 0.6914178508700747\n",
            "Epoch : 228, val loss : 0.692571288660953\n",
            "\n",
            "Epoch : 229, train loss : 0.6884877894863936\n",
            "Epoch : 229, val loss : 0.687362777559381\n",
            "\n",
            "Epoch : 230, train loss : 0.6888764478943561\n",
            "Epoch : 230, val loss : 0.6859708553866336\n",
            "\n",
            "Epoch : 231, train loss : 0.6891264409729927\n",
            "Epoch : 231, val loss : 0.6901422017499022\n",
            "\n",
            "Epoch : 232, train loss : 0.6890082081158955\n",
            "Epoch : 232, val loss : 0.699711329058597\n",
            "\n",
            "Epoch : 233, train loss : 0.6886445601781209\n",
            "Epoch : 233, val loss : 0.6941012802876924\n",
            "\n",
            "Epoch : 234, train loss : 0.6891161647709932\n",
            "Epoch : 234, val loss : 0.6941835158749632\n",
            "\n",
            "Epoch : 235, train loss : 0.6891138817324781\n",
            "Epoch : 235, val loss : 0.692829197958896\n",
            "\n",
            "Epoch : 236, train loss : 0.6874474207560222\n",
            "Epoch : 236, val loss : 0.6977829368490922\n",
            "\n",
            "Epoch : 237, train loss : 0.6890125625061262\n",
            "Epoch : 237, val loss : 0.6844975634625085\n",
            "\n",
            "Epoch : 238, train loss : 0.688825884371093\n",
            "Epoch : 238, val loss : 0.6835734781466033\n",
            "\n",
            "Epoch : 239, train loss : 0.6892951372897989\n",
            "Epoch : 239, val loss : 0.6851593820672287\n",
            "\n",
            "Epoch : 240, train loss : 0.6884945952531064\n",
            "Epoch : 240, val loss : 0.6843219242597882\n",
            "\n",
            "Epoch : 241, train loss : 0.6893743352456524\n",
            "Epoch : 241, val loss : 0.687951966335899\n",
            "\n",
            "Epoch : 242, train loss : 0.6880126866427334\n",
            "Epoch : 242, val loss : 0.6818268581440574\n",
            "\n",
            "Epoch : 243, train loss : 0.6894359072049462\n",
            "Epoch : 243, val loss : 0.6818500004316632\n",
            "\n",
            "Epoch : 244, train loss : 0.6862888878042046\n",
            "Epoch : 244, val loss : 0.6962739072347943\n",
            "\n",
            "Epoch : 245, train loss : 0.6894267407330598\n",
            "Epoch : 245, val loss : 0.6952105070415295\n",
            "\n",
            "Epoch : 246, train loss : 0.6900105313821273\n",
            "Epoch : 246, val loss : 0.6880700023550737\n",
            "\n",
            "Epoch : 247, train loss : 0.6904965165889622\n",
            "Epoch : 247, val loss : 0.6894117311427469\n",
            "\n",
            "Epoch : 248, train loss : 0.688470327131676\n",
            "Epoch : 248, val loss : 0.6937551467042222\n",
            "\n",
            "Epoch : 249, train loss : 0.6891182733304572\n",
            "Epoch : 249, val loss : 0.6869280557883413\n",
            "\n",
            "Epoch : 250, train loss : 0.6885175639932807\n",
            "Epoch : 250, val loss : 0.6997141963557195\n",
            "\n",
            "Epoch : 251, train loss : 0.6890202941316548\n",
            "Epoch : 251, val loss : 0.6873206778576498\n",
            "\n",
            "Epoch : 252, train loss : 0.6906132362105631\n",
            "Epoch : 252, val loss : 0.7000370496197751\n",
            "\n",
            "Epoch : 253, train loss : 0.688709320805289\n",
            "Epoch : 253, val loss : 0.6952105886057803\n",
            "\n",
            "Epoch : 254, train loss : 0.6881500204404196\n",
            "Epoch : 254, val loss : 0.6983486006134435\n",
            "\n",
            "Epoch : 255, train loss : 0.6885602358615761\n",
            "Epoch : 255, val loss : 0.6932842417767173\n",
            "\n",
            "Epoch : 256, train loss : 0.6887490962490891\n",
            "Epoch : 256, val loss : 0.6865804603225306\n",
            "\n",
            "Epoch : 257, train loss : 0.6884082743615814\n",
            "Epoch : 257, val loss : 0.6993973380640933\n",
            "\n",
            "Epoch : 258, train loss : 0.6881539308663569\n",
            "Epoch : 258, val loss : 0.6965620800068505\n",
            "\n",
            "Epoch : 259, train loss : 0.6885772155992913\n",
            "Epoch : 259, val loss : 0.6875228819094207\n",
            "\n",
            "Epoch : 260, train loss : 0.6878317110466239\n",
            "Epoch : 260, val loss : 0.684884039979232\n",
            "\n",
            "Epoch : 261, train loss : 0.6894877708319469\n",
            "Epoch : 261, val loss : 0.6756099274283962\n",
            "\n",
            "Epoch : 262, train loss : 0.6876729072946495\n",
            "Epoch : 262, val loss : 0.689245089104301\n",
            "\n",
            "Epoch : 263, train loss : 0.6870393673578898\n",
            "Epoch : 263, val loss : 0.7231014370918274\n",
            "\n",
            "Epoch : 264, train loss : 0.6884408268061553\n",
            "Epoch : 264, val loss : 0.6911120383363021\n",
            "\n",
            "Epoch : 265, train loss : 0.6882684816013681\n",
            "Epoch : 265, val loss : 0.6834476150964435\n",
            "\n",
            "Epoch : 266, train loss : 0.6877029166077122\n",
            "Epoch : 266, val loss : 0.6984372107606185\n",
            "\n",
            "Epoch : 267, train loss : 0.6877845099478053\n",
            "Epoch : 267, val loss : 0.6992906143790798\n",
            "\n",
            "Epoch : 268, train loss : 0.6889872139150444\n",
            "Epoch : 268, val loss : 0.688768634670659\n",
            "\n",
            "Epoch : 269, train loss : 0.6877181923750677\n",
            "Epoch : 269, val loss : 0.6913130942143891\n",
            "\n",
            "Epoch : 270, train loss : 0.6875040451685588\n",
            "Epoch : 270, val loss : 0.6908347167466817\n",
            "\n",
            "Epoch : 271, train loss : 0.687357702038505\n",
            "Epoch : 271, val loss : 0.6925319025391027\n",
            "\n",
            "Epoch : 272, train loss : 0.6866696881525441\n",
            "Epoch : 272, val loss : 0.680378678597902\n",
            "\n",
            "Epoch : 273, train loss : 0.6883026108597263\n",
            "Epoch : 273, val loss : 0.6916312198889882\n",
            "\n",
            "Epoch : 274, train loss : 0.6890587109507934\n",
            "Epoch : 274, val loss : 0.6926537469813698\n",
            "\n",
            "Epoch : 275, train loss : 0.6883245775193879\n",
            "Epoch : 275, val loss : 0.6980739267248856\n",
            "\n",
            "Epoch : 276, train loss : 0.6885344241604657\n",
            "Epoch : 276, val loss : 0.7062778410158659\n",
            "\n",
            "Epoch : 277, train loss : 0.686392515355891\n",
            "Epoch : 277, val loss : 0.690489831723665\n",
            "\n",
            "Epoch : 278, train loss : 0.6899349591948768\n",
            "Epoch : 278, val loss : 0.704864298042498\n",
            "\n",
            "Epoch : 279, train loss : 0.6881212736621053\n",
            "Epoch : 279, val loss : 0.6854168019796674\n",
            "\n",
            "Epoch : 280, train loss : 0.6896226908221391\n",
            "Epoch : 280, val loss : 0.6949221429071928\n",
            "\n",
            "Epoch : 281, train loss : 0.6909986499584083\n",
            "Epoch : 281, val loss : 0.6899713215075042\n",
            "\n",
            "Epoch : 282, train loss : 0.6882597627061786\n",
            "Epoch : 282, val loss : 0.6995526269862525\n",
            "\n",
            "Epoch : 283, train loss : 0.6888469060262045\n",
            "Epoch : 283, val loss : 0.6839817636891415\n",
            "\n",
            "Epoch : 284, train loss : 0.6869802995161577\n",
            "Epoch : 284, val loss : 0.6922890481195951\n",
            "\n",
            "Epoch : 285, train loss : 0.6862441496415571\n",
            "Epoch : 285, val loss : 0.6781821564624184\n",
            "\n",
            "Epoch : 286, train loss : 0.6888693791447265\n",
            "Epoch : 286, val loss : 0.6999167925433107\n",
            "\n",
            "Epoch : 287, train loss : 0.6883715806585371\n",
            "Epoch : 287, val loss : 0.6912593527844079\n",
            "\n",
            "Epoch : 288, train loss : 0.6873763897202237\n",
            "Epoch : 288, val loss : 0.6867186113407737\n",
            "\n",
            "Epoch : 289, train loss : 0.6871373823194797\n",
            "Epoch : 289, val loss : 0.6841989818372223\n",
            "\n",
            "Epoch : 290, train loss : 0.6869421930024117\n",
            "Epoch : 290, val loss : 0.6930948809573525\n",
            "\n",
            "Epoch : 291, train loss : 0.6895004651763226\n",
            "Epoch : 291, val loss : 0.6888547727936193\n",
            "\n",
            "Epoch : 292, train loss : 0.688286189960711\n",
            "Epoch : 292, val loss : 0.694749327082383\n",
            "\n",
            "Epoch : 293, train loss : 0.6864936774427239\n",
            "Epoch : 293, val loss : 0.6906821978719611\n",
            "\n",
            "Epoch : 294, train loss : 0.6884763887434294\n",
            "Epoch : 294, val loss : 0.6844324814645866\n",
            "\n",
            "Epoch : 295, train loss : 0.6859406734957839\n",
            "Epoch : 295, val loss : 0.6892414751805757\n",
            "\n",
            "Epoch : 296, train loss : 0.6875090342579466\n",
            "Epoch : 296, val loss : 0.6884941176364295\n",
            "\n",
            "Epoch : 297, train loss : 0.6860476837013711\n",
            "Epoch : 297, val loss : 0.6901843767417105\n",
            "\n",
            "Epoch : 298, train loss : 0.6876318342757941\n",
            "Epoch : 298, val loss : 0.690623973545275\n",
            "\n",
            "Epoch : 299, train loss : 0.6874848846233255\n",
            "Epoch : 299, val loss : 0.6988100723216407\n",
            "\n",
            "Epoch : 300, train loss : 0.6869409727327751\n",
            "Epoch : 300, val loss : 0.6863420887997277\n",
            "\n",
            "Epoch : 301, train loss : 0.6878287918639905\n",
            "Epoch : 301, val loss : 0.686548286362698\n",
            "\n",
            "Epoch : 302, train loss : 0.6866247675635597\n",
            "Epoch : 302, val loss : 0.6842761071104754\n",
            "\n",
            "Epoch : 303, train loss : 0.6889005534576643\n",
            "Epoch : 303, val loss : 0.6919719827802558\n",
            "\n",
            "Epoch : 304, train loss : 0.687201917893959\n",
            "Epoch : 304, val loss : 0.6900420032049481\n",
            "\n",
            "Epoch : 305, train loss : 0.6870342211289839\n",
            "Epoch : 305, val loss : 0.6997485317681965\n",
            "\n",
            "Epoch : 306, train loss : 0.6878512927980134\n",
            "Epoch : 306, val loss : 0.712272418172736\n",
            "\n",
            "Epoch : 307, train loss : 0.6874128171891881\n",
            "Epoch : 307, val loss : 0.6876060868564404\n",
            "\n",
            "Epoch : 308, train loss : 0.6894868175188698\n",
            "Epoch : 308, val loss : 0.6923218024404425\n",
            "\n",
            "Epoch : 309, train loss : 0.6875913435762582\n",
            "Epoch : 309, val loss : 0.6784363075306541\n",
            "\n",
            "Epoch : 310, train loss : 0.6872441952878777\n",
            "Epoch : 310, val loss : 0.6834357506350468\n",
            "\n",
            "Epoch : 311, train loss : 0.6866053870230013\n",
            "Epoch : 311, val loss : 0.7096294572478846\n",
            "\n",
            "Epoch : 312, train loss : 0.6884242195071599\n",
            "Epoch : 312, val loss : 0.7024764795052378\n",
            "\n",
            "Epoch : 313, train loss : 0.6874342918395998\n",
            "Epoch : 313, val loss : 0.6837424133953296\n",
            "\n",
            "Epoch : 314, train loss : 0.6887759837237274\n",
            "Epoch : 314, val loss : 0.6852544326531259\n",
            "\n",
            "Epoch : 315, train loss : 0.6871064431739571\n",
            "Epoch : 315, val loss : 0.6908960373778094\n",
            "\n",
            "Epoch : 316, train loss : 0.6869018081462747\n",
            "Epoch : 316, val loss : 0.6874307927332427\n",
            "\n",
            "Epoch : 317, train loss : 0.6871971849239237\n",
            "Epoch : 317, val loss : 0.6968506417776409\n",
            "\n",
            "Epoch : 318, train loss : 0.6870719382257171\n",
            "Epoch : 318, val loss : 0.6973970971609416\n",
            "\n",
            "Epoch : 319, train loss : 0.6876433968544009\n",
            "Epoch : 319, val loss : 0.6855939469839396\n",
            "\n",
            "Epoch : 320, train loss : 0.6869067166790822\n",
            "Epoch : 320, val loss : 0.6875707475762618\n",
            "\n",
            "Epoch : 321, train loss : 0.6883178494193337\n",
            "Epoch : 321, val loss : 0.6924170192919281\n",
            "\n",
            "Epoch : 322, train loss : 0.6879656683314936\n",
            "Epoch : 322, val loss : 0.6840023398399353\n",
            "\n",
            "Epoch : 323, train loss : 0.6860745505853131\n",
            "Epoch : 323, val loss : 0.6940113086449473\n",
            "\n",
            "Epoch : 324, train loss : 0.6872673670450848\n",
            "Epoch : 324, val loss : 0.69441767115342\n",
            "\n",
            "Epoch : 325, train loss : 0.686396057316751\n",
            "Epoch : 325, val loss : 0.7019545247680262\n",
            "\n",
            "Epoch : 326, train loss : 0.6845454833724279\n",
            "Epoch : 326, val loss : 0.6722769549018458\n",
            "\n",
            "Epoch : 327, train loss : 0.6887747818773445\n",
            "Epoch : 327, val loss : 0.6827227567371569\n",
            "\n",
            "Epoch : 328, train loss : 0.6882698553981202\n",
            "Epoch : 328, val loss : 0.6854671365336369\n",
            "\n",
            "Epoch : 329, train loss : 0.6889223886258677\n",
            "Epoch : 329, val loss : 0.687407054399189\n",
            "\n",
            "Epoch : 330, train loss : 0.6864733526200956\n",
            "Epoch : 330, val loss : 0.7052690418143022\n",
            "\n",
            "Epoch : 331, train loss : 0.6854024955720612\n",
            "Epoch : 331, val loss : 0.6965215488484031\n",
            "\n",
            "Epoch : 332, train loss : 0.6872436823266926\n",
            "Epoch : 332, val loss : 0.6815263848555716\n",
            "\n",
            "Epoch : 333, train loss : 0.6884145584973422\n",
            "Epoch : 333, val loss : 0.6938794945415696\n",
            "\n",
            "Epoch : 334, train loss : 0.6878669666521479\n",
            "Epoch : 334, val loss : 0.6887682143010591\n",
            "\n",
            "Epoch : 335, train loss : 0.6869216637177904\n",
            "Epoch : 335, val loss : 0.6963267953772294\n",
            "\n",
            "Epoch : 336, train loss : 0.6870413213065176\n",
            "Epoch : 336, val loss : 0.6863323042267249\n",
            "\n",
            "Epoch : 337, train loss : 0.6879665627624048\n",
            "Epoch : 337, val loss : 0.6983664443618373\n",
            "\n",
            "Epoch : 338, train loss : 0.6862461953452139\n",
            "Epoch : 338, val loss : 0.6788038485928585\n",
            "\n",
            "Epoch : 339, train loss : 0.6878296776251361\n",
            "Epoch : 339, val loss : 0.6889719617994209\n",
            "\n",
            "Epoch : 340, train loss : 0.686666716951313\n",
            "Epoch : 340, val loss : 0.6923433071688603\n",
            "\n",
            "Epoch : 341, train loss : 0.6852794390736209\n",
            "Epoch : 341, val loss : 0.6833414529499255\n",
            "\n",
            "Epoch : 342, train loss : 0.6879093975731824\n",
            "Epoch : 342, val loss : 0.6980986908862467\n",
            "\n",
            "Epoch : 343, train loss : 0.6869930798357184\n",
            "Epoch : 343, val loss : 0.6894513870540417\n",
            "\n",
            "Epoch : 344, train loss : 0.6859278671669241\n",
            "Epoch : 344, val loss : 0.6825880125949257\n",
            "\n",
            "Epoch : 345, train loss : 0.6861773635401874\n",
            "Epoch : 345, val loss : 0.6815796431742216\n",
            "\n",
            "Epoch : 346, train loss : 0.6866868868018642\n",
            "Epoch : 346, val loss : 0.6893602201813145\n",
            "\n",
            "Epoch : 347, train loss : 0.6866590965877878\n",
            "Epoch : 347, val loss : 0.6850285028156482\n",
            "\n",
            "Epoch : 348, train loss : 0.6878962852738124\n",
            "Epoch : 348, val loss : 0.6858076421838057\n",
            "\n",
            "Epoch : 349, train loss : 0.685615486809702\n",
            "Epoch : 349, val loss : 0.6967595188241253\n",
            "\n",
            "Epoch : 350, train loss : 0.6859710158723771\n",
            "Epoch : 350, val loss : 0.6752917013670269\n",
            "\n",
            "Epoch : 351, train loss : 0.6865588166496971\n",
            "Epoch : 351, val loss : 0.7022744260336223\n",
            "\n",
            "Epoch : 352, train loss : 0.6862861532153508\n",
            "Epoch : 352, val loss : 0.6874359563777321\n",
            "\n",
            "Epoch : 353, train loss : 0.6861598170164861\n",
            "Epoch : 353, val loss : 0.6842754985156811\n",
            "\n",
            "Epoch : 354, train loss : 0.6859769225120547\n",
            "Epoch : 354, val loss : 0.6859125965519955\n",
            "\n",
            "Epoch : 355, train loss : 0.6859702110290528\n",
            "Epoch : 355, val loss : 0.6874693569384124\n",
            "\n",
            "Epoch : 356, train loss : 0.6854001919428504\n",
            "Epoch : 356, val loss : 0.6872863267597399\n",
            "\n",
            "Epoch : 357, train loss : 0.6858417825265363\n",
            "Epoch : 357, val loss : 0.6895790256951985\n",
            "\n",
            "Epoch : 358, train loss : 0.6843722249522355\n",
            "Epoch : 358, val loss : 0.6777997769807514\n",
            "\n",
            "Epoch : 359, train loss : 0.6867559086192736\n",
            "Epoch : 359, val loss : 0.6887496803936204\n",
            "\n",
            "Epoch : 360, train loss : 0.6852293559999174\n",
            "Epoch : 360, val loss : 0.6850202146329377\n",
            "\n",
            "Epoch : 361, train loss : 0.688307417522777\n",
            "Epoch : 361, val loss : 0.6920581773707741\n",
            "\n",
            "Epoch : 362, train loss : 0.6865218223947465\n",
            "Epoch : 362, val loss : 0.6857816420103375\n",
            "\n",
            "Epoch : 363, train loss : 0.6857064301317393\n",
            "Epoch : 363, val loss : 0.6914303208652297\n",
            "\n",
            "Epoch : 364, train loss : 0.6825971877936164\n",
            "Epoch : 364, val loss : 0.6762604368360418\n",
            "\n",
            "Epoch : 365, train loss : 0.6870812119859638\n",
            "Epoch : 365, val loss : 0.6793500875171862\n",
            "\n",
            "Epoch : 366, train loss : 0.6856154878934225\n",
            "Epoch : 366, val loss : 0.6951510749365154\n",
            "\n",
            "Epoch : 367, train loss : 0.687288051301783\n",
            "Epoch : 367, val loss : 0.686678582116177\n",
            "\n",
            "Epoch : 368, train loss : 0.6876666643402788\n",
            "Epoch : 368, val loss : 0.6851846512995268\n",
            "\n",
            "Epoch : 369, train loss : 0.6860984372370171\n",
            "Epoch : 369, val loss : 0.681171555268137\n",
            "\n",
            "Epoch : 370, train loss : 0.6856386462847394\n",
            "Epoch : 370, val loss : 0.7022177420164409\n",
            "\n",
            "Epoch : 371, train loss : 0.6854740901426833\n",
            "Epoch : 371, val loss : 0.6802326440811157\n",
            "\n",
            "Epoch : 372, train loss : 0.6874752947778412\n",
            "Epoch : 372, val loss : 0.6877558607804148\n",
            "\n",
            "Epoch : 373, train loss : 0.6869846380118165\n",
            "Epoch : 373, val loss : 0.6841271205952294\n",
            "\n",
            "Epoch : 374, train loss : 0.6853852687460004\n",
            "Epoch : 374, val loss : 0.6841402461654261\n",
            "\n",
            "Epoch : 375, train loss : 0.6860978957378503\n",
            "Epoch : 375, val loss : 0.6919278063272175\n",
            "\n",
            "Epoch : 376, train loss : 0.6852818163958461\n",
            "Epoch : 376, val loss : 0.685780600497597\n",
            "\n",
            "Epoch : 377, train loss : 0.6854992187384398\n",
            "Epoch : 377, val loss : 0.6813718111891496\n",
            "\n",
            "Epoch : 378, train loss : 0.6859679276292976\n",
            "Epoch : 378, val loss : 0.6887720070387188\n",
            "\n",
            "Epoch : 379, train loss : 0.6855733116467793\n",
            "Epoch : 379, val loss : 0.682144234054967\n",
            "\n",
            "Epoch : 380, train loss : 0.6856265140302259\n",
            "Epoch : 380, val loss : 0.6933596447894447\n",
            "\n",
            "Epoch : 381, train loss : 0.6870453025355484\n",
            "Epoch : 381, val loss : 0.6852149179107264\n",
            "\n",
            "Epoch : 382, train loss : 0.6861023245435771\n",
            "Epoch : 382, val loss : 0.683364472891155\n",
            "\n",
            "Epoch : 383, train loss : 0.6855811205777254\n",
            "Epoch : 383, val loss : 0.6891037319835864\n",
            "\n",
            "Epoch : 384, train loss : 0.6855466517535124\n",
            "Epoch : 384, val loss : 0.6841095591846265\n",
            "\n",
            "Epoch : 385, train loss : 0.6851714007782213\n",
            "Epoch : 385, val loss : 0.7067604284537465\n",
            "\n",
            "Epoch : 386, train loss : 0.684073513926882\n",
            "Epoch : 386, val loss : 0.6992559401612533\n",
            "\n",
            "Epoch : 387, train loss : 0.6868393428397896\n",
            "Epoch : 387, val loss : 0.6860713582289846\n",
            "\n",
            "Epoch : 388, train loss : 0.6856617692745093\n",
            "Epoch : 388, val loss : 0.6894601207030445\n",
            "\n",
            "Epoch : 389, train loss : 0.6868963884584833\n",
            "Epoch : 389, val loss : 0.6945878110433881\n",
            "\n",
            "Epoch : 390, train loss : 0.6850186459945914\n",
            "Epoch : 390, val loss : 0.705161336221193\n",
            "\n",
            "Epoch : 391, train loss : 0.6852897860787135\n",
            "Epoch : 391, val loss : 0.6894894085432354\n",
            "\n",
            "Epoch : 392, train loss : 0.6854397784579886\n",
            "Epoch : 392, val loss : 0.6945403437865411\n",
            "\n",
            "Epoch : 393, train loss : 0.6850260687596871\n",
            "Epoch : 393, val loss : 0.6956313283819902\n",
            "\n",
            "Epoch : 394, train loss : 0.6852212089480771\n",
            "Epoch : 394, val loss : 0.6839189937240199\n",
            "\n",
            "Epoch : 395, train loss : 0.6843584179878238\n",
            "Epoch : 395, val loss : 0.7094389796257019\n",
            "\n",
            "Epoch : 396, train loss : 0.6869461706190397\n",
            "Epoch : 396, val loss : 0.6873879652274282\n",
            "\n",
            "Epoch : 397, train loss : 0.684660589694977\n",
            "Epoch : 397, val loss : 0.7002166729224356\n",
            "\n",
            "Epoch : 398, train loss : 0.6853366873481055\n",
            "Epoch : 398, val loss : 0.6848839803745873\n",
            "\n",
            "Epoch : 399, train loss : 0.6857214981859379\n",
            "Epoch : 399, val loss : 0.691285296490318\n",
            "\n",
            "Epoch : 400, train loss : 0.6845826409079814\n",
            "Epoch : 400, val loss : 0.6872098728230125\n",
            "\n",
            "Epoch : 401, train loss : 0.6852517980517762\n",
            "Epoch : 401, val loss : 0.6963903088318675\n",
            "\n",
            "Epoch : 402, train loss : 0.6850689053535465\n",
            "Epoch : 402, val loss : 0.6900978402087562\n",
            "\n",
            "Epoch : 403, train loss : 0.6839704336542074\n",
            "Epoch : 403, val loss : 0.6840126420322218\n",
            "\n",
            "Epoch : 404, train loss : 0.6859741449356075\n",
            "Epoch : 404, val loss : 0.6831469880907158\n",
            "\n",
            "Epoch : 405, train loss : 0.6847261616677948\n",
            "Epoch : 405, val loss : 0.680944213741704\n",
            "\n",
            "Epoch : 406, train loss : 0.6863526785012448\n",
            "Epoch : 406, val loss : 0.6928014755249023\n",
            "\n",
            "Epoch : 407, train loss : 0.6862148740074852\n",
            "Epoch : 407, val loss : 0.6811386723267406\n",
            "\n",
            "Epoch : 408, train loss : 0.6850479902643145\n",
            "Epoch : 408, val loss : 0.6836627157110916\n",
            "\n",
            "Epoch : 409, train loss : 0.6852470394336817\n",
            "Epoch : 409, val loss : 0.6899932403313485\n",
            "\n",
            "Epoch : 410, train loss : 0.6850761922922997\n",
            "Epoch : 410, val loss : 0.6887470640634236\n",
            "\n",
            "Epoch : 411, train loss : 0.6841461495919658\n",
            "Epoch : 411, val loss : 0.6947371991057144\n",
            "\n",
            "Epoch : 412, train loss : 0.6853671940890226\n",
            "Epoch : 412, val loss : 0.6913481323342573\n",
            "\n",
            "Epoch : 413, train loss : 0.6844317251985721\n",
            "Epoch : 413, val loss : 0.6900552636698674\n",
            "\n",
            "Epoch : 414, train loss : 0.685834387215701\n",
            "Epoch : 414, val loss : 0.6913466610406573\n",
            "\n",
            "Epoch : 415, train loss : 0.6848269737128052\n",
            "Epoch : 415, val loss : 0.6858936454120435\n",
            "\n",
            "Epoch : 416, train loss : 0.6842983079679084\n",
            "Epoch : 416, val loss : 0.6829030294167368\n",
            "\n",
            "Epoch : 417, train loss : 0.6838846192215423\n",
            "Epoch : 417, val loss : 0.704753737700613\n",
            "\n",
            "Epoch : 418, train loss : 0.6870869835217793\n",
            "Epoch : 418, val loss : 0.6855849247229727\n",
            "\n",
            "Epoch : 419, train loss : 0.686047630960291\n",
            "Epoch : 419, val loss : 0.6852368273233111\n",
            "\n",
            "Epoch : 420, train loss : 0.6840857267379762\n",
            "Epoch : 420, val loss : 0.6959345497583089\n",
            "\n",
            "Epoch : 421, train loss : 0.6850002404415243\n",
            "Epoch : 421, val loss : 0.6977798593671697\n",
            "\n",
            "Epoch : 422, train loss : 0.6831488280585318\n",
            "Epoch : 422, val loss : 0.7040331332307114\n",
            "\n",
            "Epoch : 423, train loss : 0.6858779600172333\n",
            "Epoch : 423, val loss : 0.6937440727886403\n",
            "\n",
            "Epoch : 424, train loss : 0.6828987435861067\n",
            "Epoch : 424, val loss : 0.7126741221076565\n",
            "\n",
            "Epoch : 425, train loss : 0.6861946413011263\n",
            "Epoch : 425, val loss : 0.6848606279021814\n",
            "\n",
            "Epoch : 426, train loss : 0.6866798986088147\n",
            "Epoch : 426, val loss : 0.6871290081425717\n",
            "\n",
            "Epoch : 427, train loss : 0.6868978662924331\n",
            "Epoch : 427, val loss : 0.6876667047801771\n",
            "\n",
            "Epoch : 428, train loss : 0.6831372293558992\n",
            "Epoch : 428, val loss : 0.6822792166157773\n",
            "\n",
            "Epoch : 429, train loss : 0.6834259361931773\n",
            "Epoch : 429, val loss : 0.704027207274186\n",
            "\n",
            "Epoch : 430, train loss : 0.6847615238391993\n",
            "Epoch : 430, val loss : 0.6831421726628354\n",
            "\n",
            "Epoch : 431, train loss : 0.684515199516759\n",
            "Epoch : 431, val loss : 0.6888746430999355\n",
            "\n",
            "Epoch : 432, train loss : 0.684696195342324\n",
            "Epoch : 432, val loss : 0.6863334461262351\n",
            "\n",
            "Epoch : 433, train loss : 0.684768837148493\n",
            "Epoch : 433, val loss : 0.6842807782323735\n",
            "\n",
            "Epoch : 434, train loss : 0.6841090917587276\n",
            "Epoch : 434, val loss : 0.6891727667105826\n",
            "\n",
            "Epoch : 435, train loss : 0.6838594252412963\n",
            "Epoch : 435, val loss : 0.6868646960509452\n",
            "\n",
            "Epoch : 436, train loss : 0.6834927129023002\n",
            "Epoch : 436, val loss : 0.7061357780506736\n",
            "\n",
            "Epoch : 437, train loss : 0.6850661357243852\n",
            "Epoch : 437, val loss : 0.6848813358106112\n",
            "\n",
            "Epoch : 438, train loss : 0.6842534491510099\n",
            "Epoch : 438, val loss : 0.6914818318266617\n",
            "\n",
            "Epoch : 439, train loss : 0.6837948289784518\n",
            "Epoch : 439, val loss : 0.686820852129083\n",
            "\n",
            "Epoch : 440, train loss : 0.6839342586921919\n",
            "Epoch : 440, val loss : 0.6908616109898217\n",
            "\n",
            "Epoch : 441, train loss : 0.6852342222676129\n",
            "Epoch : 441, val loss : 0.6846405393198918\n",
            "\n",
            "Epoch : 442, train loss : 0.6845132690487485\n",
            "Epoch : 442, val loss : 0.7099768927222803\n",
            "\n",
            "Epoch : 443, train loss : 0.6863836552157546\n",
            "Epoch : 443, val loss : 0.6911128633900692\n",
            "\n",
            "Epoch : 444, train loss : 0.6854560552221354\n",
            "Epoch : 444, val loss : 0.6909154716290926\n",
            "\n",
            "Epoch : 445, train loss : 0.6850885221452427\n",
            "Epoch : 445, val loss : 0.6893999106005619\n",
            "\n",
            "Epoch : 446, train loss : 0.6839700789162607\n",
            "Epoch : 446, val loss : 0.6847462905080695\n",
            "\n",
            "Epoch : 447, train loss : 0.6841319123903912\n",
            "Epoch : 447, val loss : 0.6894699931144714\n",
            "\n",
            "Epoch : 448, train loss : 0.6844408580751126\n",
            "Epoch : 448, val loss : 0.694073150032445\n",
            "\n",
            "Epoch : 449, train loss : 0.6835198947877593\n",
            "Epoch : 449, val loss : 0.6819538913275067\n",
            "\n",
            "Epoch : 450, train loss : 0.68552122332833\n",
            "Epoch : 450, val loss : 0.6844470940138164\n",
            "\n",
            "Epoch : 451, train loss : 0.6849474715464043\n",
            "Epoch : 451, val loss : 0.6846786549216825\n",
            "\n",
            "Epoch : 452, train loss : 0.683925497893131\n",
            "Epoch : 452, val loss : 0.6822792009303444\n",
            "\n",
            "Epoch : 453, train loss : 0.6835129770365624\n",
            "Epoch : 453, val loss : 0.6804291574578536\n",
            "\n",
            "Epoch : 454, train loss : 0.6869458798206213\n",
            "Epoch : 454, val loss : 0.6825405108301262\n",
            "\n",
            "Epoch : 455, train loss : 0.6837799848932208\n",
            "Epoch : 455, val loss : 0.6912224135900797\n",
            "\n",
            "Epoch : 456, train loss : 0.683455222303217\n",
            "Epoch : 456, val loss : 0.6830965500128897\n",
            "\n",
            "Epoch : 457, train loss : 0.6834635044589187\n",
            "Epoch : 457, val loss : 0.6835010145839892\n",
            "\n",
            "Epoch : 458, train loss : 0.6846866325898604\n",
            "Epoch : 458, val loss : 0.6886157205230311\n",
            "\n",
            "Epoch : 459, train loss : 0.6826219703211927\n",
            "Epoch : 459, val loss : 0.6785761651239897\n",
            "\n",
            "Epoch : 460, train loss : 0.6831783500584692\n",
            "Epoch : 460, val loss : 0.6847232046880221\n",
            "\n",
            "Epoch : 461, train loss : 0.6830684820810949\n",
            "Epoch : 461, val loss : 0.6888422056248314\n",
            "\n",
            "Epoch : 462, train loss : 0.684337994185361\n",
            "Epoch : 462, val loss : 0.6753139903670863\n",
            "\n",
            "Epoch : 463, train loss : 0.6843196749687195\n",
            "Epoch : 463, val loss : 0.6934192212004411\n",
            "\n",
            "Epoch : 464, train loss : 0.6846443107633883\n",
            "Epoch : 464, val loss : 0.6810321870603058\n",
            "\n",
            "Epoch : 465, train loss : 0.6840281081922132\n",
            "Epoch : 465, val loss : 0.6904501475785907\n",
            "\n",
            "Epoch : 466, train loss : 0.6832467458464881\n",
            "Epoch : 466, val loss : 0.6915759099157234\n",
            "\n",
            "Epoch : 467, train loss : 0.6846276333837799\n",
            "Epoch : 467, val loss : 0.6776167122941269\n",
            "\n",
            "Epoch : 468, train loss : 0.6835066517194116\n",
            "Epoch : 468, val loss : 0.6890685370093897\n",
            "\n",
            "Epoch : 469, train loss : 0.68278013865153\n",
            "Epoch : 469, val loss : 0.6817571269838433\n",
            "\n",
            "Epoch : 470, train loss : 0.6840675780267423\n",
            "Epoch : 470, val loss : 0.6895423719757481\n",
            "\n",
            "Epoch : 471, train loss : 0.6840901006351815\n",
            "Epoch : 471, val loss : 0.6846281823359037\n",
            "\n",
            "Epoch : 472, train loss : 0.6844917976494995\n",
            "Epoch : 472, val loss : 0.7010014621835006\n",
            "\n",
            "Epoch : 473, train loss : 0.683083605405056\n",
            "Epoch : 473, val loss : 0.6816805601119996\n",
            "\n",
            "Epoch : 474, train loss : 0.6829928712411363\n",
            "Epoch : 474, val loss : 0.6831964850425721\n",
            "\n",
            "Epoch : 475, train loss : 0.683737310857484\n",
            "Epoch : 475, val loss : 0.6840806634802568\n",
            "\n",
            "Epoch : 476, train loss : 0.6831046906384554\n",
            "Epoch : 476, val loss : 0.6821046095145377\n",
            "\n",
            "Epoch : 477, train loss : 0.6844399918209424\n",
            "Epoch : 477, val loss : 0.6930279167074908\n",
            "\n",
            "Epoch : 478, train loss : 0.683360839251316\n",
            "Epoch : 478, val loss : 0.6874409161115946\n",
            "\n",
            "Epoch : 479, train loss : 0.6831413471337523\n",
            "Epoch : 479, val loss : 0.6835405293263888\n",
            "\n",
            "Epoch : 480, train loss : 0.6838800805987735\n",
            "Epoch : 480, val loss : 0.6856158532594381\n",
            "\n",
            "Epoch : 481, train loss : 0.683963800199104\n",
            "Epoch : 481, val loss : 0.6800519478948495\n",
            "\n",
            "Epoch : 482, train loss : 0.6838147708863926\n",
            "Epoch : 482, val loss : 0.6912375500327662\n",
            "\n",
            "Epoch : 483, train loss : 0.6836452018130907\n",
            "Epoch : 483, val loss : 0.6813998034125879\n",
            "\n",
            "Epoch : 484, train loss : 0.6853316841703473\n",
            "Epoch : 484, val loss : 0.6977020031527469\n",
            "\n",
            "Epoch : 485, train loss : 0.6841330513809667\n",
            "Epoch : 485, val loss : 0.6815942055300663\n",
            "\n",
            "Epoch : 486, train loss : 0.6843454761938614\n",
            "Epoch : 486, val loss : 0.696134206495787\n",
            "\n",
            "Epoch : 487, train loss : 0.6836519216046183\n",
            "Epoch : 487, val loss : 0.6869431294892963\n",
            "\n",
            "Epoch : 488, train loss : 0.6845507690400792\n",
            "Epoch : 488, val loss : 0.683151568237104\n",
            "\n",
            "Epoch : 489, train loss : 0.6839937376253532\n",
            "Epoch : 489, val loss : 0.6918895746532241\n",
            "\n",
            "Epoch : 490, train loss : 0.6827397216450087\n",
            "Epoch : 490, val loss : 0.683199882507324\n",
            "\n",
            "Epoch : 491, train loss : 0.6826022440736944\n",
            "Epoch : 491, val loss : 0.6969816904318957\n",
            "\n",
            "Epoch : 492, train loss : 0.6836091503952489\n",
            "Epoch : 492, val loss : 0.6876446008682251\n",
            "\n",
            "Epoch : 493, train loss : 0.6830843091011046\n",
            "Epoch : 493, val loss : 0.6940825675663195\n",
            "\n",
            "Epoch : 494, train loss : 0.6822016047708921\n",
            "Epoch : 494, val loss : 0.7098696576921564\n",
            "\n",
            "Epoch : 495, train loss : 0.6822874387105302\n",
            "Epoch : 495, val loss : 0.6804588719418173\n",
            "\n",
            "Epoch : 496, train loss : 0.6841469594926545\n",
            "Epoch : 496, val loss : 0.6853921538905093\n",
            "\n",
            "Epoch : 497, train loss : 0.6818529352997288\n",
            "Epoch : 497, val loss : 0.6800915410644129\n",
            "\n",
            "Epoch : 498, train loss : 0.6824567610567267\n",
            "Epoch : 498, val loss : 0.6876476877614072\n",
            "\n",
            "Epoch : 499, train loss : 0.684183014161659\n",
            "Epoch : 499, val loss : 0.6882518874971492\n",
            "\n",
            "Epoch : 500, train loss : 0.6833870226686651\n",
            "Epoch : 500, val loss : 0.6886545043242606\n",
            "\n",
            "Epoch : 501, train loss : 0.6832552750905356\n",
            "Epoch : 501, val loss : 0.6979302450230248\n",
            "\n",
            "Epoch : 502, train loss : 0.6854064009406352\n",
            "Epoch : 502, val loss : 0.6957307771632544\n",
            "\n",
            "Epoch : 503, train loss : 0.6832776802958865\n",
            "Epoch : 503, val loss : 0.6991222312575892\n",
            "\n",
            "Epoch : 504, train loss : 0.6832058725935038\n",
            "Epoch : 504, val loss : 0.6854828627485977\n",
            "\n",
            "Epoch : 505, train loss : 0.6843325748588098\n",
            "Epoch : 505, val loss : 0.6828486668436151\n",
            "\n",
            "Epoch : 506, train loss : 0.6825499411785241\n",
            "Epoch : 506, val loss : 0.6959629999963861\n",
            "\n",
            "Epoch : 507, train loss : 0.6823916540001381\n",
            "Epoch : 507, val loss : 0.6926275428972746\n",
            "\n",
            "Epoch : 508, train loss : 0.6835515607487072\n",
            "Epoch : 508, val loss : 0.6999908591571607\n",
            "\n",
            "Epoch : 509, train loss : 0.6820821408069494\n",
            "Epoch : 509, val loss : 0.6912134321112381\n",
            "\n",
            "Epoch : 510, train loss : 0.6837959083643824\n",
            "Epoch : 510, val loss : 0.6868551655819541\n",
            "\n",
            "Epoch : 511, train loss : 0.6826303749373465\n",
            "Epoch : 511, val loss : 0.6981128328724913\n",
            "\n",
            "Epoch : 512, train loss : 0.6844354412772435\n",
            "Epoch : 512, val loss : 0.6836227492282264\n",
            "\n",
            "Epoch : 513, train loss : 0.6838191877711904\n",
            "Epoch : 513, val loss : 0.694705727853273\n",
            "\n",
            "Epoch : 514, train loss : 0.6832222194382637\n",
            "Epoch : 514, val loss : 0.6781136142580133\n",
            "\n",
            "Epoch : 515, train loss : 0.6842645189978862\n",
            "Epoch : 515, val loss : 0.6799106942979913\n",
            "\n",
            "Epoch : 516, train loss : 0.6818566929210317\n",
            "Epoch : 516, val loss : 0.6855551380860179\n",
            "\n",
            "Epoch : 517, train loss : 0.6819891145735074\n",
            "Epoch : 517, val loss : 0.692390639530985\n",
            "\n",
            "Epoch : 518, train loss : 0.6830665909882744\n",
            "Epoch : 518, val loss : 0.6806697123929073\n",
            "\n",
            "Epoch : 519, train loss : 0.6831315365704622\n",
            "Epoch : 519, val loss : 0.6842299291962072\n",
            "\n",
            "Epoch : 520, train loss : 0.6841157537518131\n",
            "Epoch : 520, val loss : 0.6756220836388438\n",
            "\n",
            "Epoch : 521, train loss : 0.6826656059785319\n",
            "Epoch : 521, val loss : 0.6889960326646501\n",
            "\n",
            "Epoch : 522, train loss : 0.6828150200121337\n",
            "Epoch : 522, val loss : 0.688812914647554\n",
            "\n",
            "Epoch : 523, train loss : 0.6832728685754719\n",
            "Epoch : 523, val loss : 0.6839585806194104\n",
            "\n",
            "Epoch : 524, train loss : 0.683199255394213\n",
            "Epoch : 524, val loss : 0.6847487625322846\n",
            "\n",
            "Epoch : 525, train loss : 0.6818330378243423\n",
            "Epoch : 525, val loss : 0.6869754132471587\n",
            "\n",
            "Epoch : 526, train loss : 0.6824119672630771\n",
            "Epoch : 526, val loss : 0.7091326839045475\n",
            "\n",
            "Epoch : 527, train loss : 0.6823910550637681\n",
            "Epoch : 527, val loss : 0.6912493548895183\n",
            "\n",
            "Epoch : 528, train loss : 0.682265132485014\n",
            "Epoch : 528, val loss : 0.6969367924489474\n",
            "\n",
            "Epoch : 529, train loss : 0.6833321705009\n",
            "Epoch : 529, val loss : 0.6855415168561436\n",
            "\n",
            "Epoch : 530, train loss : 0.68245711218227\n",
            "Epoch : 530, val loss : 0.6767877089349847\n",
            "\n",
            "Epoch : 531, train loss : 0.6825635863072946\n",
            "Epoch : 531, val loss : 0.7050996422767639\n",
            "\n",
            "Epoch : 532, train loss : 0.6824822552276381\n",
            "Epoch : 532, val loss : 0.6827380468970852\n",
            "\n",
            "Epoch : 533, train loss : 0.6810679197311404\n",
            "Epoch : 533, val loss : 0.6953028879667584\n",
            "\n",
            "Epoch : 534, train loss : 0.6829319639639421\n",
            "Epoch : 534, val loss : 0.6844306274464255\n",
            "\n",
            "Epoch : 535, train loss : 0.6824518741983358\n"
          ]
        }
      ],
      "source": [
        "seedn = 0\n",
        "seeds = [72442,16007,15137,96512,19047,59485,75241,95430,72796,63453,26884,53675,\n",
        "         18008,15186,27656,31995,93321,89984,29108,75579,35223,13737,92478,17877,\n",
        "         68783,67243,71062,45080,43868,38000,73096,51761,64413,62026,50615,23993,\n",
        "         50152,22721,92064,87461,97294,59936,14695,15888,48874,37701,27120,60244,\n",
        "         97999,73735,81996,72191,77250,50393,23720,63282,19530,45563,98929,14856,\n",
        "         78783,75455,55985,89396,74140,74802,58912,14247,13741,41605,94482,20021,\n",
        "         94900,54095,56975,57805,76423,58744,22887,62985,29424,58566,19647,65836,\n",
        "         49274,99511,81839,78935,29560,97097,85628,87836,69055,19863,38173,80205,\n",
        "         25417,79727,92203,69116]\n",
        "\n",
        "for trial in range(11):\n",
        "  torch.manual_seed(seeds[seedn])\n",
        "  tname = \"NEW_trial\"+str(trial)\n",
        "\n",
        "  # FIT / EVALUATE MODELS: IMAGE CLASSIFICATION\n",
        "  # A\n",
        "  train_dataA = datasets.ImageFolder(traindirA,transform=train_transforms)\n",
        "  val_dataA = datasets.ImageFolder(valdirA,transform=val_transforms)\n",
        "\n",
        "  trainloaderA = torch.utils.data.DataLoader(train_dataA, shuffle = True, batch_size=8)\n",
        "  valloaderA = torch.utils.data.DataLoader(val_dataA, shuffle = True, batch_size=8)\n",
        "\n",
        "  modelAschema = vision_transformer.VitAttentionSchema().to(device)\n",
        "  modelAcontrol = vision_transformer.VitControl().to(device)\n",
        "\n",
        "  fitschema(modelAschema, trainloaderA, valloaderA, tname+\"A\")\n",
        "  fitcontrol(modelAcontrol, trainloaderA, valloaderA, tname+\"A\")\n",
        "\n",
        "  evaluate(modelAschema, valloaderA, tname+\"Aschema\", save_attn=False)\n",
        "  evaluate(modelAcontrol, valloaderA, tname+\"Acontrol\", save_attn=False)\n",
        "\n",
        "  del(train_dataA)\n",
        "  del(val_dataA)\n",
        "  del(trainloaderA)\n",
        "  del(valloaderA)\n",
        "\n",
        "  # B\n",
        "  train_dataB = datasets.ImageFolder(traindirB,transform=train_transforms)\n",
        "  val_dataB = datasets.ImageFolder(valdirB,transform=val_transforms)\n",
        "\n",
        "  trainloaderB = torch.utils.data.DataLoader(train_dataB, shuffle = True, batch_size=8)\n",
        "  valloaderB = torch.utils.data.DataLoader(val_dataB, shuffle = True, batch_size=8)\n",
        "\n",
        "  modelBschema = vision_transformer.VitAttentionSchema().to(device)\n",
        "  modelBcontrol = vision_transformer.VitControl().to(device)\n",
        "\n",
        "  fitschema(modelBschema, trainloaderB, valloaderB, tname+\"B\")\n",
        "  fitcontrol(modelBcontrol, trainloaderB, valloaderB, tname+\"B\")\n",
        "\n",
        "  evaluate(modelBschema, valloaderB, tname+\"Bschema\", save_attn=False)\n",
        "  evaluate(modelBcontrol, valloaderB, tname+\"Bcontrol\", save_attn=False)\n",
        "\n",
        "  del(train_dataB)\n",
        "  del(val_dataB)\n",
        "  del(trainloaderB)\n",
        "  del(valloaderB)\n",
        "\n",
        "  # C\n",
        "  train_dataC = datasets.ImageFolder(traindirC,transform=train_transforms)\n",
        "  val_dataC = datasets.ImageFolder(valdirC,transform=val_transforms)\n",
        "\n",
        "  trainloaderC = torch.utils.data.DataLoader(train_dataC, shuffle = True, batch_size=8)\n",
        "  valloaderC = torch.utils.data.DataLoader(val_dataC, shuffle = True, batch_size=8)\n",
        "\n",
        "  modelCschema = vision_transformer.VitAttentionSchema().to(device)\n",
        "  modelCcontrol = vision_transformer.VitControl().to(device)\n",
        "  fitschema(modelCschema, trainloaderC, valloaderC, tname+\"C\")\n",
        "  fitcontrol(modelCcontrol, trainloaderC, valloaderC, tname+\"C\")\n",
        "\n",
        "  evaluate(modelCschema, valloaderC, tname+\"Cschema\", save_attn=False)\n",
        "  evaluate(modelCcontrol, valloaderC, tname+\"Ccontrol\", save_attn=False)\n",
        "\n",
        "  del(train_dataC)\n",
        "  del(val_dataC)\n",
        "  del(trainloaderC)\n",
        "  del(valloaderC)\n",
        "\n",
        "  # FREEZE THE MODELS\n",
        "  freeze_models([modelAschema, modelAcontrol, modelBschema, modelBcontrol, modelCschema, modelCcontrol])\n",
        "\n",
        "  modelAschema_wts = deepcopy(modelAschema.state_dict())\n",
        "  modelAcontrol_wts = deepcopy(modelAcontrol.state_dict())\n",
        "  modelBschema_wts = deepcopy(modelBschema.state_dict())\n",
        "  modelBcontrol_wts = deepcopy(modelBcontrol.state_dict())\n",
        "  modelCschema_wts = deepcopy(modelCschema.state_dict())\n",
        "  modelCcontrol_wts = deepcopy(modelCcontrol.state_dict())\n",
        "\n",
        "  # FIT / EVALUATE MODELS: ATTENTION CLASSIFICATION\n",
        "  # SCHEMA\n",
        "  batch_size = 8\n",
        "  attn_transforms = transforms.Resize((256,256))\n",
        "\n",
        "  schemaAattn = torch.load(\"/content/drive/My Drive/networkattention/data/attentions/modelAschemaattn.pt\")\n",
        "  schemaAattn = attn_transforms(schemaAattn)\n",
        "  false_schemaAattn = torch.clone(schemaAattn)\n",
        "  indices = torch.randperm(false_schemaAattn.shape[-1])\n",
        "  false_schemaAattn = false_schemaAattn[:,:,indices] # false attention values are shuffled along last dimension\n",
        "\n",
        "  dataset_schemaAattn = TensorDataset(torch.cat((schemaAattn, false_schemaAattn),0),\n",
        "                                      torch.cat((torch.ones(730,), torch.zeros(730,)),0))\n",
        "  dataset_schemaAattn = torch.utils.data.random_split(dataset_schemaAattn, [0.9, 0.1])\n",
        "  schemaAattntrain = DataLoader(dataset_schemaAattn[0], batch_size, shuffle=True)\n",
        "  schemaAattnval = DataLoader(dataset_schemaAattn[1], batch_size, shuffle=True)\n",
        "\n",
        "  fitcontrol(modelCcontrol, schemaAattntrain, schemaAattnval, tname+\"CAcontrol_schemaattn\", n_epochs=800)\n",
        "  evaluate(modelCcontrol, schemaAattnval, tname+\"CAcontrol_schemaattn\", save_attn=False)\n",
        "  fitschema(modelCschema, schemaAattntrain, schemaAattnval, tname+\"CAschema_schemaattn\", n_epochs=800, policy_only=True)\n",
        "  evaluate(modelCschema, schemaAattnval, tname+\"CAschema_schemaattn\", save_attn=False)\n",
        "\n",
        "  seedn += 1\n",
        "  torch.manual_seed(seeds[seedn])\n",
        "\n",
        "  del(schemaAattn)\n",
        "  del(false_schemaAattn)\n",
        "  del(indices)\n",
        "  del(dataset_schemaAattn)\n",
        "  del(schemaAattntrain)\n",
        "  del(schemaAattnval)\n",
        "\n",
        "  schemaBattn = torch.load(\"/content/drive/My Drive/networkattention/data/attentions/modelBschemaattn.pt\")\n",
        "  schemaBattn = attn_transforms(schemaBattn)\n",
        "  false_schemaBattn = torch.clone(schemaBattn)\n",
        "  indices = torch.randperm(false_schemaBattn.shape[-1])\n",
        "  false_schemaBattn = false_schemaBattn[:,:,indices]\n",
        "\n",
        "  dataset_schemaBattn = TensorDataset(torch.cat((schemaBattn, false_schemaBattn),0),\n",
        "                                      torch.cat((torch.ones(730,), torch.zeros(730,)),0))\n",
        "  dataset_schemaBattn = torch.utils.data.random_split(dataset_schemaBattn, [0.9, 0.1])\n",
        "  schemaBattntrain = DataLoader(dataset_schemaBattn[0], batch_size, shuffle=True)\n",
        "  schemaBattnval = DataLoader(dataset_schemaBattn[1], batch_size, shuffle=True)\n",
        "\n",
        "  fitschema(modelAschema, schemaBattntrain, schemaBattnval, tname+\"ABschema_schemaattn\", n_epochs=800, policy_only=True)\n",
        "  fitcontrol(modelAcontrol, schemaBattntrain, schemaBattnval, tname+\"ABcontrol_schemaattn\", n_epochs=800)\n",
        "\n",
        "  evaluate(modelAschema, schemaBattnval, tname+\"ABschema_schemaattn\", save_attn=False)\n",
        "  evaluate(modelAcontrol, schemaBattnval, tname+\"ABcontrol_schemaattn\", save_attn=False)\n",
        "\n",
        "  seedn += 1\n",
        "  torch.manual_seed(seeds[seedn])\n",
        "\n",
        "  del(schemaBattn)\n",
        "  del(false_schemaBattn)\n",
        "  del(indices)\n",
        "  del(dataset_schemaBattn)\n",
        "  del(schemaBattntrain)\n",
        "  del(schemaBattnval)\n",
        "\n",
        "  schemaCattn = torch.load(\"/content/drive/My Drive/networkattention/data/attentions/modelCschemaattn.pt\")\n",
        "  schemaCattn = attn_transforms(schemaCattn)\n",
        "  false_schemaCattn = torch.clone(schemaCattn)\n",
        "  indices = torch.randperm(false_schemaCattn.shape[-1])\n",
        "  false_schemaCattn = false_schemaCattn[:,:,indices]\n",
        "\n",
        "  dataset_schemaCattn = TensorDataset(torch.cat((schemaCattn, false_schemaCattn),0),\n",
        "                                      torch.cat((torch.ones(730,), torch.zeros(730,)),0))\n",
        "  dataset_schemaCattn = torch.utils.data.random_split(dataset_schemaCattn, [0.9, 0.1])\n",
        "  schemaCattntrain = DataLoader(dataset_schemaCattn[0], batch_size, shuffle=True)\n",
        "  schemaCattnval = DataLoader(dataset_schemaCattn[1], batch_size, shuffle=True)\n",
        "\n",
        "  fitschema(modelBschema, schemaCattntrain, schemaCattnval, \"BCschema_schemaattn\", n_epochs=800, policy_only=True)\n",
        "  fitcontrol(modelBcontrol, schemaCattntrain, schemaCattnval, \"BCcontrol_schemaattn\", n_epochs=800)\n",
        "\n",
        "  evaluate(modelBschema, schemaCattnval, tname+\"BCschema_schemaattn\", save_attn=False)\n",
        "  evaluate(modelBcontrol, schemaCattnval, tname+\"BCcontrol_schemaattn\", save_attn=False)\n",
        "\n",
        "  seedn += 1\n",
        "  torch.manual_seed(seeds[seedn])\n",
        "\n",
        "  del(schemaCattn)\n",
        "  del(false_schemaCattn)\n",
        "  del(indices)\n",
        "  del(dataset_schemaCattn)\n",
        "  del(schemaCattntrain)\n",
        "  del(schemaCattnval)\n",
        "\n",
        "  # CONTROL\n",
        "  del(modelAschema)\n",
        "  del(modelAcontrol)\n",
        "  del(modelBschema)\n",
        "  del(modelBcontrol)\n",
        "  del(modelCschema)\n",
        "  del(modelCcontrol)\n",
        "\n",
        "  modelCschema = vision_transformer.VitAttentionSchema().to(device)\n",
        "  modelCcontrol = vision_transformer.VitControl().to(device)\n",
        "  modelCschema.load_state_dict(modelCschema_wts)\n",
        "  modelCcontrol.load_state_dict(modelCcontrol_wts)\n",
        "  freeze_models([modelCschema, modelCcontrol])\n",
        "\n",
        "  controlAattn = torch.load(\"/content/drive/My Drive/networkattention/data/attentions/modelAcontrolattn.pt\")\n",
        "  controlAattn = attn_transforms(controlAattn)\n",
        "  false_controlAattn = torch.clone(controlAattn)\n",
        "  indices = torch.randperm(false_controlAattn.shape[-1])\n",
        "  false_controlAattn = false_controlAattn[:,:,indices] # false attention values are shuffled along last dimension\n",
        "\n",
        "  dataset_controlAattn = TensorDataset(torch.cat((controlAattn, false_controlAattn),0),\n",
        "                                      torch.cat((torch.ones(730,), torch.zeros(730,)),0))\n",
        "  dataset_controlAattn = torch.utils.data.random_split(dataset_controlAattn, [0.9, 0.1])\n",
        "  controlAattntrain = DataLoader(dataset_controlAattn[0], batch_size, shuffle=True)\n",
        "  controlAattnval = DataLoader(dataset_controlAattn[1], batch_size, shuffle=True)\n",
        "\n",
        "  fitschema(modelCschema, controlAattntrain, controlAattnval, tname+\"CAs_controlattn\", n_epochs=800, policy_only=True)\n",
        "  fitcontrol(modelCcontrol, controlAattntrain, controlAattnval, tname+\"CAc_controlattn\", n_epochs=800)\n",
        "\n",
        "  evaluate(modelCschema, controlAattnval, tname+\"CAschema_controlattn\", save_attn=False)\n",
        "  evaluate(modelCcontrol, controlAattnval, tname+\"CAcontrol_controlattn\", save_attn=False)\n",
        "\n",
        "  seedn += 1\n",
        "  torch.manual_seed(seeds[seedn])\n",
        "\n",
        "  del(controlAattn)\n",
        "  del(false_controlAattn)\n",
        "  del(indices)\n",
        "  del(dataset_controlAattn)\n",
        "  del(controlAattntrain)\n",
        "  del(controlAattnval)\n",
        "  del(modelCschema)\n",
        "  del(modelCcontrol)\n",
        "  del(modelCschema_wts)\n",
        "  del(modelCcontrol_wts)\n",
        "\n",
        "  modelAschema = vision_transformer.VitAttentionSchema().to(device)\n",
        "  modelAcontrol = vision_transformer.VitControl().to(device)\n",
        "  modelAschema.load_state_dict(modelAschema_wts)\n",
        "  modelAcontrol.load_state_dict(modelAcontrol_wts)\n",
        "  freeze_models([modelAschema, modelAcontrol])\n",
        "\n",
        "  controlBattn = torch.load(\"/content/drive/My Drive/networkattention/data/attentions/modelBcontrolattn.pt\")\n",
        "  controlBattn = attn_transforms(controlBattn)\n",
        "  false_controlBattn = torch.clone(controlBattn)\n",
        "  indices = torch.randperm(false_controlBattn.shape[-1])\n",
        "  false_controlBattn = false_controlBattn[:,:,indices]\n",
        "\n",
        "  dataset_controlBattn = TensorDataset(torch.cat((controlBattn, false_controlBattn),0),\n",
        "                                      torch.cat((torch.ones(730,), torch.zeros(730,)),0))\n",
        "  dataset_controlBattn = torch.utils.data.random_split(dataset_controlBattn, [0.9, 0.1])\n",
        "  controlBattntrain = DataLoader(dataset_controlBattn[0], batch_size, shuffle=True)\n",
        "  controlBattnval = DataLoader(dataset_controlBattn[1], batch_size, shuffle=True)\n",
        "\n",
        "  fitschema(modelAschema, controlBattntrain, controlBattnval, tname+\"ABs_controlattn\", n_epochs=800, policy_only=True)\n",
        "  fitcontrol(modelAcontrol, controlBattntrain, controlBattnval, tname+\"ABc_controlattn\", n_epochs=800)\n",
        "\n",
        "  evaluate(modelAschema, controlBattnval, tname+\"ABschema_controlattn\", save_attn=False)\n",
        "  evaluate(modelAcontrol, controlBattnval, tname+\"ABcontrol_controlattn\", save_attn=False)\n",
        "\n",
        "  seedn += 1\n",
        "  torch.manual_seed(seeds[seedn])\n",
        "\n",
        "  del(controlBattn)\n",
        "  del(false_controlBattn)\n",
        "  del(indices)\n",
        "  del(dataset_controlBattn)\n",
        "  del(controlBattntrain)\n",
        "  del(controlBattnval)\n",
        "  del(modelAschema)\n",
        "  del(modelAcontrol)\n",
        "  del(modelAschema_wts)\n",
        "  del(modelAcontrol_wts)\n",
        "\n",
        "  modelBschema = vision_transformer.VitAttentionSchema().to(device)\n",
        "  modelBcontrol = vision_transformer.VitControl().to(device)\n",
        "  modelBschema.load_state_dict(modelBschema_wts)\n",
        "  modelBcontrol.load_state_dict(modelBcontrol_wts)\n",
        "  freeze_models([modelBschema, modelBcontrol])\n",
        "\n",
        "  controlCattn = torch.load(\"/content/drive/My Drive/networkattention/data/attentions/modelCcontrolattn.pt\")\n",
        "  controlCattn = attn_transforms(controlCattn)\n",
        "  false_controlCattn = torch.clone(controlCattn)\n",
        "  indices = torch.randperm(false_controlCattn.shape[-1])\n",
        "  false_controlCattn = false_controlCattn[:,:,indices]\n",
        "\n",
        "  dataset_controlCattn = TensorDataset(torch.cat((controlCattn, false_controlCattn),0),\n",
        "                                      torch.cat((torch.ones(730,), torch.zeros(730,)),0))\n",
        "  dataset_controlCattn = torch.utils.data.random_split(dataset_controlCattn, [0.9, 0.1])\n",
        "  controlCattntrain = DataLoader(dataset_controlCattn[0], batch_size, shuffle=True)\n",
        "  controlCattnval = DataLoader(dataset_controlCattn[1], batch_size, shuffle=True)\n",
        "\n",
        "  fitschema(modelBschema, controlCattntrain, controlCattnval, tname+\"BCs_controlattn\", n_epochs=800, policy_only=True)\n",
        "  fitcontrol(modelBcontrol, controlCattntrain, controlCattnval, tname+\"BCc_controlattn\", n_epochs=800)\n",
        "\n",
        "  evaluate(modelBschema, controlCattnval, tname+\"BCschema_controlattn\", save_attn=False)\n",
        "  evaluate(modelBcontrol, controlCattnval, tname+\"BCcontrol_controlattn\", save_attn=False)\n",
        "\n",
        "  seedn += 1\n",
        "  torch.manual_seed(seeds[seedn])\n",
        "\n",
        "  del(controlCattn)\n",
        "  del(false_controlCattn)\n",
        "  del(indices)\n",
        "  del(dataset_controlCattn)\n",
        "  del(controlCattntrain)\n",
        "  del(controlCattnval)\n",
        "  del(modelBschema)\n",
        "  del(modelBcontrol)\n",
        "  del(modelBschema_wts)\n",
        "  del(modelBcontrol_wts)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}