{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Class token attention task: the discrimination of genuine and artificially shuffled class tokens (Fig 4)"
      ],
      "metadata": {
        "id": "oigajfdyqD_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 uninstall --yes torch torchaudio torchvision torchtext torchdata\n",
        "!pip3 install torch torchvision torchdata\n",
        "!pip3 install torchrl"
      ],
      "metadata": {
        "id": "yyTz-3NgKimO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RLJF3R41eFA",
        "outputId": "2f71fc48-4ff9-42b7-9335-91542133300d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/networkattention')\n",
        "# vit model from https://github.com/facebookresearch/dino/blob/main/README.md\n",
        "import importlib\n",
        "import torch\n",
        "import torch.random\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.utils.data as data_utils\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import vision_transformer_clspred\n",
        "import fnmatch\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "# IMAGE CLASSIFICATION A, B, AND C DATA LOADERS\n",
        "\n",
        "traindirA = \"/content/drive/My Drive/networkattention/data/train/classificationA\"\n",
        "valdirA = \"/content/drive/My Drive/networkattention/data/val/classificationA\"\n",
        "\n",
        "traindirB = \"/content/drive/My Drive/networkattention/data/train/classificationB\"\n",
        "valdirB = \"/content/drive/My Drive/networkattention/data/val/classificationB\"\n",
        "\n",
        "traindirC = \"/content/drive/My Drive/networkattention/data/train/classificationC\"\n",
        "valdirC = \"/content/drive/My Drive/networkattention/data/val/classificationC\"\n",
        "\n",
        "train_transforms = transforms.Compose([transforms.Resize((256,256)),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       ])\n",
        "val_transforms = transforms.Compose([transforms.Resize((256,256)),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      ])\n",
        "\n",
        "\n",
        "def schematrain(model, x, y, optimizer):\n",
        "    pred_attn, h1m, policy = model.forward(x)\n",
        "    mse = torch.nn.MSELoss()\n",
        "    bce = torch.nn.BCEWithLogitsLoss()\n",
        "    pred_loss = 0.05*mse(pred_attn, h1m)\n",
        "    policy_loss = bce(policy, y)\n",
        "    total_loss = sum([pred_loss, policy_loss])\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    return total_loss\n",
        "\n",
        "def schematrain_policy(model, x, y, optimizer):\n",
        "    pred_attn, h1m, policy = model.forward(x)\n",
        "    bce = torch.nn.BCEWithLogitsLoss()\n",
        "    policy_loss = bce(policy, y)\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    return policy_loss\n",
        "\n",
        "def controltrain(model, x, y, optimizer):\n",
        "    h1, policy = model.forward(x)\n",
        "    bce = torch.nn.BCEWithLogitsLoss()\n",
        "    policy_loss = bce(policy, y)\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    return policy_loss\n",
        "\n",
        "def fitschema(model, trainloader, valloader, name=\"\", n_epochs=20, policy_only=False):\n",
        "  bce = torch.nn.BCEWithLogitsLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "  losses = []\n",
        "  val_losses = []\n",
        "\n",
        "  epoch_train_losses = []\n",
        "  epoch_val_losses = []\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "      epoch_loss = 0\n",
        "      for i, data in enumerate(trainloader): #iterate over batches\n",
        "          x_batch, y_batch = data\n",
        "          x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "          y_batch = y_batch.unsqueeze(1).float() #convert target to same nn output shape\n",
        "          model.train()\n",
        "          if policy_only:\n",
        "            loss = schematrain_policy(model, x_batch, y_batch, optimizer)\n",
        "          else:\n",
        "            loss = schematrain(model, x_batch, y_batch, optimizer)\n",
        "          epoch_loss += loss.item()/len(trainloader)\n",
        "          losses.append(loss.item())\n",
        "          if epoch == 0:\n",
        "            print(str(i)+\": \"+str(loss.item())+\" / \"+str(len(trainloader))+\": \"+str(epoch_loss))\n",
        "      epoch_train_losses.append(epoch_loss)\n",
        "      print('\\nEpoch : {}, train loss : {}'.format(epoch+1,epoch_loss))\n",
        "      with torch.no_grad():\n",
        "        cum_loss = 0\n",
        "        for x_batch, y_batch in valloader:\n",
        "          x_batch = x_batch.to(device)\n",
        "          y_batch = y_batch.unsqueeze(1).float() #convert target to same nn output shape\n",
        "          y_batch = y_batch.to(device)\n",
        "\n",
        "          #model to eval mode\n",
        "          model.eval()\n",
        "\n",
        "          _, _, policy = model(x_batch)\n",
        "          val_loss = bce(policy,y_batch)\n",
        "          cum_loss += val_loss.item()/len(valloader)\n",
        "          val_losses.append(val_loss.item())\n",
        "\n",
        "        epoch_val_losses.append(cum_loss)\n",
        "        print('Epoch : {}, val loss : {}'.format(epoch+1,cum_loss))\n",
        "\n",
        "        best_loss = min(epoch_val_losses)\n",
        "\n",
        "        #save best model\n",
        "        if cum_loss <= best_loss:\n",
        "          best_model_wts = model.state_dict()\n",
        "\n",
        "  model.load_state_dict(best_model_wts)\n",
        "\n",
        "  file = open(\"/content/drive/My Drive/networkattention/losscurves/\"+name+\"schema.txt\",\"w\")\n",
        "  for item in epoch_train_losses:\n",
        "    file.write(str(item)+\"\\n\")\n",
        "  file.close()\n",
        "\n",
        "def fitcontrol(model, trainloader, valloader, name=\"\", n_epochs=20):\n",
        "  bce = torch.nn.BCEWithLogitsLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "  losses = []\n",
        "  val_losses = []\n",
        "\n",
        "  epoch_train_losses = []\n",
        "  epoch_val_losses = []\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "      epoch_loss = 0\n",
        "      for i, data in enumerate(trainloader): #iterate over batches\n",
        "          x_batch, y_batch = data\n",
        "          x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "          y_batch = y_batch.unsqueeze(1).float() #convert target to same nn output shape\n",
        "          model.train()\n",
        "          loss = controltrain(model, x_batch, y_batch, optimizer)\n",
        "          epoch_loss += loss.item()/len(trainloader)\n",
        "          losses.append(loss.item())\n",
        "          if epoch == 0:\n",
        "            print(str(i)+\": \"+str(loss.item())+\" / \"+str(len(trainloader))+\": \"+str(epoch_loss))\n",
        "      epoch_train_losses.append(epoch_loss)\n",
        "      print('\\nEpoch : {}, train loss : {}'.format(epoch+1,epoch_loss))\n",
        "      with torch.no_grad():\n",
        "        cum_loss = 0\n",
        "        for x_batch, y_batch in valloader:\n",
        "          x_batch = x_batch.to(device)\n",
        "          y_batch = y_batch.unsqueeze(1).float() #convert target to same nn output shape\n",
        "          y_batch = y_batch.to(device)\n",
        "\n",
        "          #model to eval mode\n",
        "          model.eval()\n",
        "\n",
        "          _, policy = model(x_batch)\n",
        "          val_loss = bce(policy,y_batch)\n",
        "          cum_loss += val_loss.item()/len(valloader)\n",
        "          val_losses.append(val_loss.item())\n",
        "\n",
        "        epoch_val_losses.append(cum_loss)\n",
        "        print('Epoch : {}, val loss : {}'.format(epoch+1,cum_loss))\n",
        "\n",
        "        best_loss = min(epoch_val_losses)\n",
        "\n",
        "        #save best model\n",
        "        if cum_loss <= best_loss:\n",
        "          best_model_wts = model.state_dict()\n",
        "\n",
        "  model.load_state_dict(best_model_wts)\n",
        "\n",
        "  file = open(\"/content/drive/My Drive/networkattention/losscurves/\"+name+\"control.txt\",\"w\")\n",
        "  for item in epoch_train_losses:\n",
        "    file.write(str(item)+\"\\n\")\n",
        "  file.close()\n",
        "\n",
        "def evaluate(model, valloader, name=\"\", save_attn=False):\n",
        "  classifications = []\n",
        "  labels = []\n",
        "  attn_outputs = torch.empty((0,3,257,257)).to(device)\n",
        "  model.eval()\n",
        "  sigmoid = torch.nn.Sigmoid()\n",
        "  total_acc = 0\n",
        "  for i, data in enumerate(valloader):\n",
        "      accuracy = 0\n",
        "      x_batch, y_batch = data\n",
        "      x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "      y_batch = y_batch.unsqueeze(1).float()\n",
        "      outputs = model.forward(x_batch)\n",
        "      policy = outputs[-1]\n",
        "      policy = torch.round(sigmoid(policy))\n",
        "      accuracy = 1-(torch.sum(abs(policy - y_batch))/len(y_batch))\n",
        "      total_acc += accuracy.item()/len(valloader)\n",
        "      if save_attn:\n",
        "        attn = model.get_last_selfattention(x_batch)\n",
        "        attn_outputs = torch.cat((attn_outputs, attn.data[:,0:3,:,:]), 0)\n",
        "      for pol in policy:\n",
        "        classifications.append(pol.item())\n",
        "      for yb in y_batch:\n",
        "        labels.append(yb.item())\n",
        "\n",
        "  file = open(\"/content/drive/My Drive/networkattention/accuracies/acc\"+name+\".txt\",\"w\")\n",
        "  file.write(str(total_acc))\n",
        "  file.close()\n",
        "\n",
        "  file = open(\"/content/drive/My Drive/networkattention/classifications/\"+name+\"_classifications.txt\",\"w\")\n",
        "  file.write(str(classifications))\n",
        "  file.close()\n",
        "\n",
        "  file = open(\"/content/drive/My Drive/networkattention/classifications/\"+name+\"_labels.txt\",\"w\")\n",
        "  file.write(str(labels))\n",
        "  file.close()\n",
        "\n",
        "  if save_attn:\n",
        "    torch.save(attn_outputs, \"/content/drive/My Drive/networkattention/data/attentions/\"+name+\"attn.pt\")\n",
        "\n",
        "def freeze_models(models):\n",
        "  for i, model in enumerate(models):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    for param in model.policy.parameters():\n",
        "        param.requires_grad = True\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "schem = vision_transformer_clspred.VitAttentionSchema().to(device)\n",
        "cont = vision_transformer_clspred.VitControl()\n",
        "\n",
        "attn = torch.load(\"/content/drive/My Drive/networkattention/data/attentions/modelAschemaattn.pt\")\n",
        "\n",
        "clsattn = torch.nn.functional.interpolate(attn[:,:,0,:], size=(65536), mode=\"nearest\")\n",
        "cls = clsattn.reshape(730,3,256,256)\n",
        "print(cls.shape)\n",
        "schem(cls[0,...].unsqueeze(0))"
      ],
      "metadata": {
        "id": "zQ00gMID8ddK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tname = \"NEWCLStrial0\"\n",
        "torch.random.seed()\n",
        "# B\n",
        "train_dataB = datasets.ImageFolder(traindirB,transform=train_transforms)\n",
        "val_dataB = datasets.ImageFolder(valdirB,transform=val_transforms)\n",
        "\n",
        "trainloaderB = torch.utils.data.DataLoader(train_dataB, shuffle = True, batch_size=8)\n",
        "valloaderB = torch.utils.data.DataLoader(val_dataB, shuffle = True, batch_size=8)\n",
        "\n",
        "modelBschema = vision_transformer_clspred.VitAttentionSchema().to(device)\n",
        "modelBcontrol = vision_transformer_clspred.VitControl().to(device)\n",
        "\n",
        "fitschema(modelBschema, trainloaderB, valloaderB, tname+\"B\")\n",
        "fitcontrol(modelBcontrol, trainloaderB, valloaderB, tname+\"B\")\n",
        "\n",
        "evaluate(modelBschema, valloaderB, tname+\"Bschema\", save_attn=True)\n",
        "evaluate(modelBcontrol, valloaderB, tname+\"Bcontrol\", save_attn=True)\n",
        "\n",
        "del(train_dataB)\n",
        "del(val_dataB)\n",
        "del(trainloaderB)\n",
        "del(valloaderB)\n",
        "\n",
        "# C\n",
        "train_dataC = datasets.ImageFolder(traindirC,transform=train_transforms)\n",
        "val_dataC = datasets.ImageFolder(valdirC,transform=val_transforms)\n",
        "\n",
        "trainloaderC = torch.utils.data.DataLoader(train_dataC, shuffle = True, batch_size=8)\n",
        "valloaderC = torch.utils.data.DataLoader(val_dataC, shuffle = True, batch_size=8)\n",
        "\n",
        "modelCschema = vision_transformer_clspred.VitAttentionSchema().to(device)\n",
        "modelCcontrol = vision_transformer_clspred.VitControl().to(device)\n",
        "fitschema(modelCschema, trainloaderC, valloaderC, tname+\"C\")\n",
        "fitcontrol(modelCcontrol, trainloaderC, valloaderC, tname+\"C\")\n",
        "\n",
        "evaluate(modelCschema, valloaderC, tname+\"Cschema\", save_attn=True)\n",
        "evaluate(modelCcontrol, valloaderC, tname+\"Ccontrol\", save_attn=True)\n",
        "\n",
        "del(train_dataC)\n",
        "del(val_dataC)\n",
        "del(trainloaderC)\n",
        "del(valloaderC)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yhZpJMUDTyH",
        "outputId": "b6cc7599-a6e1-4439-ca01-683c23a0bc58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: 0.6646795272827148 / 239: 0.002781085888212196\n",
            "1: 0.6534203886985779 / 239: 0.005515062409963568\n",
            "2: 0.8286062479019165 / 239: 0.008982034158507152\n",
            "3: 0.6653600931167603 / 239: 0.011765967602510332\n",
            "4: 0.7489864826202393 / 239: 0.01489980225782514\n",
            "5: 0.6522353887557983 / 239: 0.017628820620820114\n",
            "6: 0.6897300481796265 / 239: 0.020514720403998467\n",
            "7: 0.7265724539756775 / 239: 0.02355477251268331\n",
            "8: 0.6764973998069763 / 239: 0.026385305566268984\n",
            "9: 0.6736147999763489 / 239: 0.029203777532697223\n",
            "10: 0.6753988862037659 / 239: 0.03202971429505608\n",
            "11: 0.6847952008247375 / 239: 0.03489496618135205\n",
            "12: 0.6776763200759888 / 239: 0.037730431955728574\n",
            "13: 0.6642135381698608 / 239: 0.040509568098698705\n",
            "14: 0.7013245820999146 / 239: 0.043443980576104206\n",
            "15: 0.679368257522583 / 239: 0.046286525586658946\n",
            "16: 0.7302337884902954 / 239: 0.04934189708661834\n",
            "17: 0.6957350969314575 / 239: 0.052252922596791804\n",
            "18: 0.5531455874443054 / 239: 0.05456733928065919\n",
            "19: 0.8296688199043274 / 239: 0.05803875693716265\n",
            "20: 0.672033429145813 / 239: 0.06085061228923718\n",
            "21: 0.677770733833313 / 239: 0.06368647310025523\n",
            "22: 0.6462149620056152 / 239: 0.06639030139316575\n",
            "23: 0.6966841220855713 / 239: 0.06930529771988363\n",
            "24: 0.6441333889961243 / 239: 0.07200041650229419\n",
            "25: 0.6649855375289917 / 239: 0.07478278276810588\n",
            "26: 0.6419550180435181 / 239: 0.07746878702770219\n",
            "27: 0.6647504568099976 / 239: 0.08025016969217916\n",
            "28: 0.6773478984832764 / 239: 0.08308426131763219\n",
            "29: 0.6930648684501648 / 239: 0.08598411432369983\n",
            "30: 0.6091737151145935 / 239: 0.0885329583199952\n",
            "31: 0.6835192441940308 / 239: 0.09139287147561877\n",
            "32: 0.7253904342651367 / 239: 0.09442797789513817\n",
            "33: 0.7898926138877869 / 239: 0.09773296791140505\n",
            "34: 0.8712331652641296 / 239: 0.1013782949627194\n",
            "35: 0.8082512021064758 / 239: 0.10476009915563353\n",
            "36: 0.8594658374786377 / 239: 0.10835619052583703\n",
            "37: 0.6855663657188416 / 239: 0.11122466904348909\n",
            "38: 0.5812351107597351 / 239: 0.1136566151136135\n",
            "39: 0.722339928150177 / 239: 0.11667895790922093\n",
            "40: 0.6829312443733215 / 239: 0.11953641081454863\n",
            "41: 0.6289836764335632 / 239: 0.12216814167828739\n",
            "42: 0.6479248404502869 / 239: 0.12487912427431369\n",
            "43: 0.6558459401130676 / 239: 0.1276232495467533\n",
            "44: 0.5738690495491028 / 239: 0.13002437527708427\n",
            "45: 0.7028363943099976 / 239: 0.13296511332859054\n",
            "46: 0.711574912071228 / 239: 0.13594241421591785\n",
            "47: 0.8257038593292236 / 239: 0.13939724207922005\n",
            "48: 0.6865724325180054 / 239: 0.14226993008138744\n",
            "49: 0.6348190307617188 / 239: 0.1449260766536122\n",
            "50: 0.6199692487716675 / 239: 0.1475200902467991\n",
            "51: 0.6132022738456726 / 239: 0.15008579013736678\n",
            "52: 0.758509635925293 / 239: 0.15325947062240985\n",
            "53: 0.66059809923172 / 239: 0.15602347940580616\n",
            "54: 0.6343953609466553 / 239: 0.15867785330098044\n",
            "55: 0.59535151720047 / 239: 0.16116886383319998\n",
            "56: 0.6471076011657715 / 239: 0.163876427017994\n",
            "57: 0.5893732905387878 / 239: 0.16634242404953703\n",
            "58: 0.789157509803772 / 239: 0.16964433831649842\n",
            "59: 0.6048202514648438 / 239: 0.17217496698371534\n",
            "60: 0.6958234906196594 / 239: 0.1750863623419566\n",
            "61: 0.7360842823982239 / 239: 0.17816621289592405\n",
            "62: 0.6490855813026428 / 239: 0.18088205214823636\n",
            "63: 0.73092120885849 / 239: 0.18394029988404595\n",
            "64: 0.500768780708313 / 239: 0.1860355667489343\n",
            "65: 0.6925503015518188 / 239: 0.18893326675542726\n",
            "66: 0.7524412274360657 / 239: 0.19208155640997146\n",
            "67: 0.602779746055603 / 239: 0.19460364739765182\n",
            "68: 0.6424410939216614 / 239: 0.19729168544753325\n",
            "69: 0.5925845503807068 / 239: 0.1997711187127245\n",
            "70: 0.752638041973114 / 239: 0.2029202318590555\n",
            "71: 0.7901692986488342 / 239: 0.20622637955214684\n",
            "72: 0.5884383916854858 / 239: 0.20868846487300663\n",
            "73: 0.6815383434295654 / 239: 0.21154008974091276\n",
            "74: 0.6170241236686707 / 239: 0.21412178063492393\n",
            "75: 0.6873374581336975 / 239: 0.21699766958109004\n",
            "76: 0.7101474404335022 / 239: 0.21996899778374068\n",
            "77: 0.5991169810295105 / 239: 0.22247576339474281\n",
            "78: 0.8076085448265076 / 239: 0.22585487864506293\n",
            "79: 0.6747403740882874 / 239: 0.22867806012660388\n",
            "80: 0.5720367431640625 / 239: 0.23107151930302255\n",
            "81: 0.734499454498291 / 239: 0.2341447387779108\n",
            "82: 0.5727288722991943 / 239: 0.23654109389213337\n",
            "83: 0.5718991160392761 / 239: 0.23893397722284165\n",
            "84: 0.6035734415054321 / 239: 0.24145938911198572\n",
            "85: 0.727026641368866 / 239: 0.24450134158633244\n",
            "86: 0.6258184313774109 / 239: 0.24711982874690738\n",
            "87: 0.5112600326538086 / 239: 0.2492589920634505\n",
            "88: 0.5460925698280334 / 239: 0.2515438982133586\n",
            "89: 0.6121789813041687 / 239: 0.2541053165451752\n",
            "90: 0.5953127145767212 / 239: 0.2565961647233205\n",
            "91: 0.5765027403831482 / 239: 0.2590083100805721\n",
            "92: 0.6923828721046448 / 239: 0.2619053095454451\n",
            "93: 0.44409653544425964 / 239: 0.2637634540452119\n",
            "94: 0.9967728853225708 / 239: 0.26793405189174985\n",
            "95: 0.5893009901046753 / 239: 0.2703997464110163\n",
            "96: 0.6410196423530579 / 239: 0.27308183696479477\n",
            "97: 0.4732106924057007 / 239: 0.27506179802088554\n",
            "98: 0.540359377861023 / 239: 0.2773227159198856\n",
            "99: 0.5834441184997559 / 239: 0.27976390470021933\n",
            "100: 0.5566748380661011 / 239: 0.28209308812309003\n",
            "101: 0.5522354245185852 / 239: 0.2844036965938791\n",
            "102: 0.5269031524658203 / 239: 0.2866083122945729\n",
            "103: 0.45302048325538635 / 239: 0.28850379548811006\n",
            "104: 0.4676150977611542 / 239: 0.29046034401430737\n",
            "105: 0.569695234298706 / 239: 0.2928440060825028\n",
            "106: 0.7784936428070068 / 239: 0.29610130165910115\n",
            "107: 0.86359703540802 / 239: 0.2997146783762895\n",
            "108: 0.5589689612388611 / 239: 0.3020534606408873\n",
            "109: 0.6276606917381287 / 239: 0.3046796560038083\n",
            "110: 0.45544692873954773 / 239: 0.306585291688911\n",
            "111: 0.43421322107315063 / 239: 0.30840208340888237\n",
            "112: 0.8691290616989136 / 239: 0.3120386066795891\n",
            "113: 0.5073917508125305 / 239: 0.3141615847164616\n",
            "114: 0.8314391374588013 / 239: 0.31764040955938544\n",
            "115: 0.8622278571128845 / 239: 0.32124805749709623\n",
            "116: 0.6298059821128845 / 239: 0.3238832289703719\n",
            "117: 0.6901595592498779 / 239: 0.32677092587099904\n",
            "118: 0.7763431668281555 / 239: 0.3300192236401545\n",
            "119: 0.49468597769737244 / 239: 0.33208903944641965\n",
            "120: 0.5767185688018799 / 239: 0.3345020878514484\n",
            "121: 0.7638934850692749 / 239: 0.33769829490194747\n",
            "122: 0.6512340903282166 / 239: 0.34042312373177264\n",
            "123: 0.7187955379486084 / 239: 0.34343063644285465\n",
            "124: 0.6484156250953674 / 239: 0.3461436725311198\n",
            "125: 0.4939470589160919 / 239: 0.348210396627003\n",
            "126: 0.6637016534805298 / 239: 0.3509873909930303\n",
            "127: 0.4188450872898102 / 239: 0.35273988089800856\n",
            "128: 0.5429468750953674 / 239: 0.3550116251452695\n",
            "129: 0.5720385313034058 / 239: 0.35740509180344276\n",
            "130: 0.4715891182422638 / 239: 0.35937826803039785\n",
            "131: 0.6346203088760376 / 239: 0.3620335831302976\n",
            "132: 0.775341272354126 / 239: 0.36527768887236506\n",
            "133: 0.5095372200012207 / 239: 0.36740964376776764\n",
            "134: 0.3229920566082001 / 239: 0.36876107496696514\n",
            "135: 0.4707694947719574 / 239: 0.37073082180701517\n",
            "136: 0.7684633731842041 / 239: 0.373946149728288\n",
            "137: 0.4817625880241394 / 239: 0.3759618927744141\n",
            "138: 0.5114754438400269 / 239: 0.37810195739299163\n",
            "139: 0.5695871710777283 / 239: 0.38048516731381893\n",
            "140: 0.7482942938804626 / 239: 0.383616105781938\n",
            "141: 0.5131750106811523 / 239: 0.3857632815588466\n",
            "142: 0.8203479647636414 / 239: 0.38919569982145596\n",
            "143: 0.5963387489318848 / 239: 0.391690841030376\n",
            "144: 0.5311612486839294 / 239: 0.39391327303323764\n",
            "145: 0.6024866700172424 / 239: 0.3964341377613433\n",
            "146: 0.5979710817337036 / 239: 0.3989361088146224\n",
            "147: 0.7749305367469788 / 239: 0.40217849599766414\n",
            "148: 0.6640503406524658 / 239: 0.4049569493058335\n",
            "149: 0.6100743412971497 / 239: 0.4075095616125161\n",
            "150: 0.597061276435852 / 239: 0.4100077259490678\n",
            "151: 0.5322906970977783 / 239: 0.4122348836775104\n",
            "152: 0.4221568703651428 / 239: 0.41400123041543985\n",
            "153: 0.7778614163398743 / 239: 0.41725588069301256\n",
            "154: 0.7014141082763672 / 239: 0.4201906677569304\n",
            "155: 0.5633170008659363 / 239: 0.4225476426559511\n",
            "156: 0.5477753281593323 / 239: 0.42483958963569723\n",
            "157: 0.7366170287132263 / 239: 0.4279216692537442\n",
            "158: 0.4690364599227905 / 239: 0.4298841649019567\n",
            "159: 0.5412598848342896 / 239: 0.4321488506125604\n",
            "160: 0.5310832858085632 / 239: 0.4343709564109226\n",
            "161: 0.3638897240161896 / 239: 0.43589350755743383\n",
            "162: 0.5589818954467773 / 239: 0.43823234394005633\n",
            "163: 0.8079686164855957 / 239: 0.4416129657663559\n",
            "164: 0.8439072370529175 / 239: 0.44514395839000825\n",
            "165: 0.3888828158378601 / 239: 0.44677108314246794\n",
            "166: 0.758650541305542 / 239: 0.44994535318977147\n",
            "167: 0.3974180519580841 / 239: 0.451608190227253\n",
            "168: 0.7234879732131958 / 239: 0.454635336558689\n",
            "169: 0.36553817987442017 / 239: 0.45616478501004637\n",
            "170: 0.5577473044395447 / 239: 0.45849845573991893\n",
            "171: 0.46481701731681824 / 239: 0.4604432968165583\n",
            "172: 0.5342087745666504 / 239: 0.46267847997374095\n",
            "173: 0.4627569019794464 / 239: 0.4646147013209353\n",
            "174: 1.1837362051010132 / 239: 0.46956757247198555\n",
            "175: 0.34687086939811707 / 239: 0.47101891502176846\n",
            "176: 0.5155472755432129 / 239: 0.47317601659307895\n",
            "177: 0.6707835793495178 / 239: 0.47598264244809785\n",
            "178: 0.9740037322044373 / 239: 0.48005797187154736\n",
            "179: 0.7171685099601746 / 239: 0.48305867693414223\n",
            "180: 0.5489920973777771 / 239: 0.4853557149984844\n",
            "181: 0.5222455263137817 / 239: 0.4875408427236467\n",
            "182: 0.6680833101272583 / 239: 0.4903361703810829\n",
            "183: 0.6463989019393921 / 239: 0.4930407682971473\n",
            "184: 0.7511836886405945 / 239: 0.4961837962830912\n",
            "185: 0.36299845576286316 / 239: 0.4977026182737308\n",
            "186: 0.45636889338493347 / 239: 0.49961211155149204\n",
            "187: 0.4754544794559479 / 239: 0.5016014608379186\n",
            "188: 0.49064233899116516 / 239: 0.5036543576537812\n",
            "189: 0.543645441532135 / 239: 0.5059290247731625\n",
            "190: 0.45047658681869507 / 239: 0.5078138640485546\n",
            "191: 0.5364084243774414 / 239: 0.5100582507614309\n",
            "192: 0.6752305030822754 / 239: 0.5128834829919007\n",
            "193: 0.6395100355148315 / 239: 0.5155592571990757\n",
            "194: 0.44983142614364624 / 239: 0.5174413970574173\n",
            "195: 0.6978588104248047 / 239: 0.5203613083981069\n",
            "196: 0.6085682511329651 / 239: 0.5229076190723034\n",
            "197: 0.7235900163650513 / 239: 0.5259351923625337\n",
            "198: 0.35805681347846985 / 239: 0.5274333380256235\n",
            "199: 0.44662415981292725 / 239: 0.5293020583595688\n",
            "200: 0.4171585738658905 / 239: 0.5310474917230243\n",
            "201: 0.534715473651886 / 239: 0.5332847949600615\n",
            "202: 0.45979511737823486 / 239: 0.5352086239030667\n",
            "203: 0.4369325637817383 / 239: 0.5370367936260028\n",
            "204: 0.42512810230255127 / 239: 0.5388155722967247\n",
            "205: 0.268615186214447 / 239: 0.5399394852097559\n",
            "206: 0.5358535051345825 / 239: 0.542181550084796\n",
            "207: 0.4685631990432739 / 239: 0.5441420655619644\n",
            "208: 0.7607725262641907 / 239: 0.5473252142074213\n",
            "209: 0.6511510610580444 / 239: 0.5500496956344424\n",
            "210: 0.37703076004981995 / 239: 0.5516272301953203\n",
            "211: 0.42841312289237976 / 239: 0.5534197537220666\n",
            "212: 0.4415297508239746 / 239: 0.5552671585372297\n",
            "213: 0.30724889039993286 / 239: 0.5565527187481081\n",
            "214: 0.49926841259002686 / 239: 0.558641707922125\n",
            "215: 0.15942694246768951 / 239: 0.5593087662588099\n",
            "216: 0.3310055136680603 / 239: 0.5606937265670444\n",
            "217: 0.42542266845703125 / 239: 0.5624737377321366\n",
            "218: 0.4196524918079376 / 239: 0.564229605898697\n",
            "219: 0.17836275696754456 / 239: 0.5649758935847536\n",
            "220: 0.23241811990737915 / 239: 0.5659483543375042\n",
            "221: 0.5629834532737732 / 239: 0.568303933639905\n",
            "222: 0.6167125105857849 / 239: 0.5708843207134856\n",
            "223: 0.5014947652816772 / 239: 0.5729826251707311\n",
            "224: 0.39864155650138855 / 239: 0.5746505814740841\n",
            "225: 0.6179253458976746 / 239: 0.5772360431724007\n",
            "226: 0.4344179332256317 / 239: 0.5790536914285749\n",
            "227: 0.37870490550994873 / 239: 0.580638230782173\n",
            "228: 0.48541051149368286 / 239: 0.5826692371064144\n",
            "229: 0.3506690561771393 / 239: 0.5841364716510886\n",
            "230: 0.21835151314735413 / 239: 0.585050076308609\n",
            "231: 0.8753339648246765 / 239: 0.5887125615170804\n",
            "232: 0.5295416712760925 / 239: 0.5909282170454323\n",
            "233: 0.275295615196228 / 239: 0.5920800815441613\n",
            "234: 0.22348929941654205 / 239: 0.5930151832153602\n",
            "235: 0.6139374375343323 / 239: 0.5955839591046251\n",
            "236: 0.4450448751449585 / 239: 0.5974460715529304\n",
            "237: 0.5652949810028076 / 239: 0.5998113225194693\n",
            "238: 0.4305008351802826 / 239: 0.601612581244073\n",
            "\n",
            "Epoch : 1, train loss : 0.601612581244073\n",
            "Epoch : 1, val loss : 0.44701639449466823\n",
            "\n",
            "Epoch : 2, train loss : 0.43054908610662196\n",
            "Epoch : 2, val loss : 0.4339845754692088\n",
            "\n",
            "Epoch : 3, train loss : 0.34766770350633813\n",
            "Epoch : 3, val loss : 0.3908180362828401\n",
            "\n",
            "Epoch : 4, train loss : 0.2885177972800562\n",
            "Epoch : 4, val loss : 0.337416928625949\n",
            "\n",
            "Epoch : 5, train loss : 0.25422522287706717\n",
            "Epoch : 5, val loss : 0.2919936348663886\n",
            "\n",
            "Epoch : 6, train loss : 0.2107626009093418\n",
            "Epoch : 6, val loss : 0.3101893403850819\n",
            "\n",
            "Epoch : 7, train loss : 0.19277034648479782\n",
            "Epoch : 7, val loss : 0.3149908988169678\n",
            "\n",
            "Epoch : 8, train loss : 0.14828718271277944\n",
            "Epoch : 8, val loss : 0.4070050139739382\n",
            "\n",
            "Epoch : 9, train loss : 0.1484226342855668\n",
            "Epoch : 9, val loss : 0.35525471768746886\n",
            "\n",
            "Epoch : 10, train loss : 0.10270788662416162\n",
            "Epoch : 10, val loss : 0.3965444604830776\n",
            "\n",
            "Epoch : 11, train loss : 0.11205025290813161\n",
            "Epoch : 11, val loss : 0.39008986533862416\n",
            "\n",
            "Epoch : 12, train loss : 0.09714229785248822\n",
            "Epoch : 12, val loss : 0.45443407029170374\n",
            "\n",
            "Epoch : 13, train loss : 0.07935634994817017\n",
            "Epoch : 13, val loss : 0.6817292285850272\n",
            "\n",
            "Epoch : 14, train loss : 0.1227451427761762\n",
            "Epoch : 14, val loss : 0.4674789000811237\n",
            "\n",
            "Epoch : 15, train loss : 0.05908486344286526\n",
            "Epoch : 15, val loss : 0.4726459735557792\n",
            "\n",
            "Epoch : 16, train loss : 0.043535747847843226\n",
            "Epoch : 16, val loss : 0.5816480323476794\n",
            "\n",
            "Epoch : 17, train loss : 0.04723316368015645\n",
            "Epoch : 17, val loss : 0.8860823157478789\n",
            "\n",
            "Epoch : 18, train loss : 0.09119537316757613\n",
            "Epoch : 18, val loss : 0.5095170412678273\n",
            "\n",
            "Epoch : 19, train loss : 0.029071218713097077\n",
            "Epoch : 19, val loss : 0.6890225683784356\n",
            "\n",
            "Epoch : 20, train loss : 0.018929772862237133\n",
            "Epoch : 20, val loss : 0.5922307616595429\n",
            "0: 0.6897212266921997 / 239: 0.0028858628731891203\n",
            "1: 1.05977201461792 / 239: 0.007320055402971212\n",
            "2: 0.7919831275939941 / 239: 0.010633792338510936\n",
            "3: 0.7002706527709961 / 239: 0.013563795069770334\n",
            "4: 0.5830225348472595 / 239: 0.016003219901767236\n",
            "5: 0.5688716769218445 / 239: 0.01838343612319755\n",
            "6: 0.7176929116249084 / 239: 0.02138633533501725\n",
            "7: 1.5388875007629395 / 239: 0.027825195170845446\n",
            "8: 0.7453904151916504 / 239: 0.030943983518927666\n",
            "9: 0.8535153269767761 / 239: 0.03451517735565058\n",
            "10: 0.6574143767356873 / 239: 0.037265865124419144\n",
            "11: 0.8093774914741516 / 239: 0.04065238182514781\n",
            "12: 0.7273097038269043 / 239: 0.043695518661243646\n",
            "13: 0.669572651386261 / 239: 0.04649707787206482\n",
            "14: 0.6074686050415039 / 239: 0.049038787516589945\n",
            "15: 0.8471653461456299 / 239: 0.05258341239586036\n",
            "16: 0.5625758171081543 / 239: 0.054937286107609964\n",
            "17: 0.9083757400512695 / 239: 0.058738021421632015\n",
            "18: 0.6947946548461914 / 239: 0.061645112027683024\n",
            "19: 0.44321340322494507 / 239: 0.06349956141356146\n",
            "20: 0.5673466920852661 / 239: 0.06587339694529898\n",
            "21: 0.6914894580841064 / 239: 0.06876665827619483\n",
            "22: 0.6686369180679321 / 239: 0.07156430228484727\n",
            "23: 0.6732050776481628 / 239: 0.07438105993191071\n",
            "24: 0.8631969690322876 / 239: 0.07799276273120899\n",
            "25: 0.5847799181938171 / 239: 0.08043954063160152\n",
            "26: 0.7610023617744446 / 239: 0.08362365093191301\n",
            "27: 0.5934202075004578 / 239: 0.08610658067040865\n",
            "28: 0.7030903100967407 / 239: 0.08904838113106446\n",
            "29: 0.6942721605300903 / 239: 0.09195328556842886\n",
            "30: 0.7202109098434448 / 239: 0.09496672033764829\n",
            "31: 0.6731534600257874 / 239: 0.09778326201139635\n",
            "32: 0.6120262742042542 / 239: 0.1003440414013723\n",
            "33: 0.7174715399742126 / 239: 0.1033460143719757\n",
            "34: 0.9403914213180542 / 239: 0.10728070651138179\n",
            "35: 0.8046919703483582 / 239: 0.11064761852120755\n",
            "36: 0.7731658220291138 / 239: 0.11388262196065992\n",
            "37: 0.7342941164970398 / 239: 0.11695498228073122\n",
            "38: 0.6576045751571655 / 239: 0.11970646585879467\n",
            "39: 0.595360517501831 / 239: 0.1221975140491789\n",
            "40: 0.6296266317367554 / 239: 0.12483193510247077\n",
            "41: 0.6753817796707153 / 239: 0.12765780028937754\n",
            "42: 0.6979900598526001 / 239: 0.13057826079085286\n",
            "43: 0.6629688143730164 / 239: 0.13335218888446382\n",
            "44: 0.7182145118713379 / 239: 0.13635727052409286\n",
            "45: 0.6895338892936707 / 239: 0.13924234955879441\n",
            "46: 0.7402360439300537 / 239: 0.14233957149992435\n",
            "47: 0.714564859867096 / 239: 0.14532938262907538\n",
            "48: 0.6109745502471924 / 239: 0.14788576150040256\n",
            "49: 0.6501639485359192 / 239: 0.15060611274950683\n",
            "50: 0.6660804748535156 / 239: 0.15339306034303618\n",
            "51: 0.7052079439163208 / 239: 0.15634372119624254\n",
            "52: 0.6785878539085388 / 239: 0.15918300091970922\n",
            "53: 0.7237858772277832 / 239: 0.16221139371145726\n",
            "54: 0.6865259408950806 / 239: 0.1650838871880057\n",
            "55: 0.6793496608734131 / 239: 0.16792635438831288\n",
            "56: 0.7001185417175293 / 239: 0.17085572067164984\n",
            "57: 0.6629499197006226 / 239: 0.17362956970805413\n",
            "58: 0.655445396900177 / 239: 0.1763720190674691\n",
            "59: 0.6358716487884521 / 239: 0.17903256989921995\n",
            "60: 0.6346572637557983 / 239: 0.18168803962204755\n",
            "61: 0.6169308423995972 / 239: 0.18426934021786176\n",
            "62: 0.6341849565505981 / 239: 0.18692283375991448\n",
            "63: 0.743695855140686 / 239: 0.19003453189857844\n",
            "64: 0.6759386658668518 / 239: 0.19286272715325145\n",
            "65: 0.725549578666687 / 239: 0.19589849944892795\n",
            "66: 0.7833481431007385 / 239: 0.1991761067422365\n",
            "67: 0.6507132649421692 / 239: 0.20189875638634597\n",
            "68: 0.5843535661697388 / 239: 0.20434375038705618\n",
            "69: 0.6445046663284302 / 239: 0.2070404226311082\n",
            "70: 0.6652891039848328 / 239: 0.20982405904945478\n",
            "71: 0.6838610172271729 / 239: 0.2126854022177693\n",
            "72: 0.6741482019424438 / 239: 0.21550610599158707\n",
            "73: 0.6734880208969116 / 239: 0.218324047501616\n",
            "74: 0.5142695307731628 / 239: 0.22047580286049953\n",
            "75: 0.7577914595603943 / 239: 0.22364647842351373\n",
            "76: 0.6754238605499268 / 239: 0.22647251968104481\n",
            "77: 0.5389899611473083 / 239: 0.22872770780300009\n",
            "78: 1.0373611450195312 / 239: 0.23306813100391863\n",
            "79: 0.7448306083679199 / 239: 0.23618457706403545\n",
            "80: 0.640759289264679 / 239: 0.2388655782743479\n",
            "81: 0.6627551913261414 / 239: 0.24163861254767904\n",
            "82: 0.5859231948852539 / 239: 0.2440901740325546\n",
            "83: 0.6341474056243896 / 239: 0.24674351045776124\n",
            "84: 0.6169906854629517 / 239: 0.24932506144296188\n",
            "85: 0.6050504446029663 / 239: 0.25185665326138434\n",
            "86: 0.6199464797973633 / 239: 0.2544505715868963\n",
            "87: 0.6492068767547607 / 239: 0.2571669183515606\n",
            "88: 0.6921851634979248 / 239: 0.2600630905837695\n",
            "89: 0.656690776348114 / 239: 0.26281075073585364\n",
            "90: 0.6771528124809265 / 239: 0.26564402610188265\n",
            "91: 0.5023318529129028 / 239: 0.2677458330178362\n",
            "92: 0.6065717339515686 / 239: 0.27028379006365866\n",
            "93: 0.6769084930419922 / 239: 0.27311604317262095\n",
            "94: 0.6221884489059448 / 239: 0.2757193421220182\n",
            "95: 0.5792168378829956 / 239: 0.2781428435357546\n",
            "96: 0.43252402544021606 / 239: 0.2799525674915714\n",
            "97: 0.6022350192070007 / 239: 0.2824723792874166\n",
            "98: 0.5104396939277649 / 239: 0.28460811022435284\n",
            "99: 0.4583592414855957 / 239: 0.28652593131843485\n",
            "100: 0.5169176459312439 / 239: 0.2886887666570593\n",
            "101: 0.5268200635910034 / 239: 0.2908930347055572\n",
            "102: 0.4364144802093506 / 239: 0.2927190367148014\n",
            "103: 1.6295180320739746 / 239: 0.2995371037946088\n",
            "104: 0.9722859263420105 / 239: 0.3036052457458306\n",
            "105: 0.5918750762939453 / 239: 0.30608171050019856\n",
            "106: 0.40530073642730713 / 239: 0.30777752948106596\n",
            "107: 0.6523914337158203 / 239: 0.31050720075184346\n",
            "108: 1.0830554962158203 / 239: 0.3150388137067214\n",
            "109: 0.9205589890480042 / 239: 0.31889052495796827\n",
            "110: 0.725987434387207 / 239: 0.3219281292859482\n",
            "111: 0.7847678065299988 / 239: 0.3252116765936051\n",
            "112: 0.5149345397949219 / 239: 0.32736621441701486\n",
            "113: 0.8351837396621704 / 239: 0.3308607070515846\n",
            "114: 0.5893493890762329 / 239: 0.33332660407700815\n",
            "115: 0.6102337837219238 / 239: 0.335879883506807\n",
            "116: 0.6698755621910095 / 239: 0.338682710126853\n",
            "117: 0.6914389133453369 / 239: 0.3415757599734862\n",
            "118: 0.7043470144271851 / 239: 0.3445228186112569\n",
            "119: 0.6087957620620728 / 239: 0.34707008121402705\n",
            "120: 0.6807209253311157 / 239: 0.34991828592252544\n",
            "121: 0.6549789905548096 / 239: 0.3526587837909556\n",
            "122: 0.633499026298523 / 239: 0.3553094073319536\n",
            "123: 0.6824867725372314 / 239: 0.35816500052248595\n",
            "124: 0.6158775091171265 / 239: 0.36074189386607225\n",
            "125: 0.6760371327400208 / 239: 0.3635705011160305\n",
            "126: 0.6701673269271851 / 239: 0.3663745485090313\n",
            "127: 0.5861151218414307 / 239: 0.3688269130355645\n",
            "128: 0.6842272281646729 / 239: 0.37168978846721584\n",
            "129: 0.6079622507095337 / 239: 0.3742335635747871\n",
            "130: 0.6933544874191284 / 239: 0.37713462837570394\n",
            "131: 0.5954899191856384 / 239: 0.37962621799572754\n",
            "132: 0.636921763420105 / 239: 0.3822911626125481\n",
            "133: 0.6262184977531433 / 239: 0.3849113236910131\n",
            "134: 0.5955009460449219 / 239: 0.38740295944852327\n",
            "135: 0.5114859938621521 / 239: 0.3895430682094528\n",
            "136: 0.6142836809158325 / 239: 0.39211329281579516\n",
            "137: 0.5742157697677612 / 239: 0.3945158692583381\n",
            "138: 0.5698937177658081 / 239: 0.3969003618012913\n",
            "139: 0.6752671003341675 / 239: 0.39972574715833803\n",
            "140: 0.672405481338501 / 239: 0.4025391592141477\n",
            "141: 0.6822142601013184 / 239: 0.4053936121852829\n",
            "142: 0.5220946669578552 / 239: 0.4075781086997509\n",
            "143: 0.689899206161499 / 239: 0.410464716256912\n",
            "144: 0.7639745473861694 / 239: 0.4136612624802851\n",
            "145: 0.6090363264083862 / 239: 0.4162095316284374\n",
            "146: 0.4157881438732147 / 239: 0.4179492309751872\n",
            "147: 0.6251079440116882 / 239: 0.42056474538527794\n",
            "148: 0.8419721126556396 / 239: 0.42408764125413\n",
            "149: 0.5948438048362732 / 239: 0.42657652746683405\n",
            "150: 0.6295679211616516 / 239: 0.42921070286918406\n",
            "151: 0.8870056867599487 / 239: 0.43292202373428845\n",
            "152: 0.5036672353744507 / 239: 0.43502941802455813\n",
            "153: 0.5786307454109192 / 239: 0.43745046716853686\n",
            "154: 0.46555307507514954 / 239: 0.4393983879847509\n",
            "155: 0.5442764163017273 / 239: 0.4416756951659297\n",
            "156: 0.6086054444313049 / 239: 0.4442221614606213\n",
            "157: 0.5414759516716003 / 239: 0.4464877512165694\n",
            "158: 0.6942685842514038 / 239: 0.44939264069042467\n",
            "159: 0.5812176465988159 / 239: 0.4518245136887461\n",
            "160: 0.7067455053329468 / 239: 0.4547816078533191\n",
            "161: 0.8093373775482178 / 239: 0.4581679567133535\n",
            "162: 0.673837423324585 / 239: 0.4609873601582262\n",
            "163: 0.46705615520477295 / 239: 0.4629415700126395\n",
            "164: 0.6607323884963989 / 239: 0.46570614067580435\n",
            "165: 0.8213138580322266 / 239: 0.46914260033284294\n",
            "166: 0.5102903246879578 / 239: 0.47127770629388044\n",
            "167: 0.7866517305374146 / 239: 0.474569136128765\n",
            "168: 0.7918678522109985 / 239: 0.4778823907405265\n",
            "169: 0.7408482432365417 / 239: 0.48098217418503086\n",
            "170: 0.6547209024429321 / 239: 0.48372159218688415\n",
            "171: 0.7658307552337646 / 239: 0.48692590497028904\n",
            "172: 0.638512909412384 / 239: 0.4895975071017216\n",
            "173: 0.7318028211593628 / 239: 0.49265944359192815\n",
            "174: 0.6187793016433716 / 239: 0.49524847832683766\n",
            "175: 0.7993055582046509 / 239: 0.49859285304735923\n",
            "176: 0.7429206371307373 / 239: 0.5017013075960234\n",
            "177: 0.5102806687355042 / 239: 0.5038363731555862\n",
            "178: 0.6097292900085449 / 239: 0.5063875417330278\n",
            "179: 0.540998101234436 / 239: 0.5086511321147619\n",
            "180: 0.67975914478302 / 239: 0.5114953126368665\n",
            "181: 0.8265893459320068 / 239: 0.514953845465034\n",
            "182: 0.544580340385437 / 239: 0.5172324242950985\n",
            "183: 0.9339182376861572 / 239: 0.5211400319841619\n",
            "184: 0.5080350637435913 / 239: 0.5232657017069384\n",
            "185: 0.7139600515365601 / 239: 0.5262529822573006\n",
            "186: 0.6694227457046509 / 239: 0.5290539142476965\n",
            "187: 0.6561391949653625 / 239: 0.5317992665278863\n",
            "188: 0.6144659519195557 / 239: 0.5343702537744117\n",
            "189: 0.57740318775177 / 239: 0.5367861666938752\n",
            "190: 0.6242753267288208 / 239: 0.5393981973496443\n",
            "191: 0.5224940776824951 / 239: 0.5415843650386925\n",
            "192: 0.5556082129478455 / 239: 0.5439090855949595\n",
            "193: 0.705435037612915 / 239: 0.5468606966309968\n",
            "194: 0.687915027141571 / 239: 0.5497390021838904\n",
            "195: 0.819757342338562 / 239: 0.5531689492229639\n",
            "196: 0.5517504215240479 / 239: 0.5554775283925206\n",
            "197: 0.5839623212814331 / 239: 0.55792088538533\n",
            "198: 0.6354880928993225 / 239: 0.5605798313807246\n",
            "199: 0.7238322496414185 / 239: 0.563608418199308\n",
            "200: 0.6577637195587158 / 239: 0.5663605676535285\n",
            "201: 0.5963380336761475 / 239: 0.5688557058697467\n",
            "202: 0.7053263187408447 / 239: 0.5718068620151059\n",
            "203: 0.5022138953208923 / 239: 0.5739081753846493\n",
            "204: 0.6616714596748352 / 239: 0.5766766752159248\n",
            "205: 0.6601353883743286 / 239: 0.5794387479706291\n",
            "206: 0.6158655881881714 / 239: 0.5820155914358516\n",
            "207: 0.6236788630485535 / 239: 0.5846251264276865\n",
            "208: 0.444821834564209 / 239: 0.5864863056518045\n",
            "209: 0.5062369108200073 / 239: 0.5886044517221811\n",
            "210: 0.4559933841228485 / 239: 0.5905123738314817\n",
            "211: 0.7945234179496765 / 239: 0.5938367395969615\n",
            "212: 0.5647151470184326 / 239: 0.5961995644798838\n",
            "213: 0.7951139211654663 / 239: 0.5995264009701158\n",
            "214: 0.5884124040603638 / 239: 0.6019883775561424\n",
            "215: 0.3882850408554077 / 239: 0.603613001158048\n",
            "216: 0.5060293078422546 / 239: 0.6057302785967185\n",
            "217: 0.6037795543670654 / 239: 0.6082565528827731\n",
            "218: 0.8754758834838867 / 239: 0.611919631893166\n",
            "219: 0.7222158312797546 / 239: 0.614941455455006\n",
            "220: 0.48825377225875854 / 239: 0.6169843582678042\n",
            "221: 0.7534816265106201 / 239: 0.6201370010565516\n",
            "222: 0.4322793185710907 / 239: 0.6219457011342548\n",
            "223: 0.3104078769683838 / 239: 0.6232444788621561\n",
            "224: 0.6135479211807251 / 239: 0.6258116249758829\n",
            "225: 0.4011577367782593 / 239: 0.6274901092301853\n",
            "226: 0.7945564985275269 / 239: 0.6308146134081247\n",
            "227: 0.3089849352836609 / 239: 0.6321074374051275\n",
            "228: 0.3488057851791382 / 239: 0.6335668758368395\n",
            "229: 0.5082686543464661 / 239: 0.6356935229261552\n",
            "230: 0.3862966299057007 / 239: 0.637309826816974\n",
            "231: 0.4408615231513977 / 239: 0.6391544357004527\n",
            "232: 0.7676686644554138 / 239: 0.6423664384806009\n",
            "233: 0.8793731927871704 / 239: 0.6460458242244803\n",
            "234: 0.5570002198219299 / 239: 0.6483763690772917\n",
            "235: 0.5096599459648132 / 239: 0.6505088374704499\n",
            "236: 0.41861650347709656 / 239: 0.6522603709578018\n",
            "237: 0.8182421922683716 / 239: 0.6556839784568326\n",
            "238: 0.4565732777118683 / 239: 0.6575943268991417\n",
            "\n",
            "Epoch : 1, train loss : 0.6575943268991417\n",
            "Epoch : 1, val loss : 0.4870193608753059\n",
            "\n",
            "Epoch : 2, train loss : 0.5085102091212153\n",
            "Epoch : 2, val loss : 0.4040740325075129\n",
            "\n",
            "Epoch : 3, train loss : 0.4106924930065246\n",
            "Epoch : 3, val loss : 0.3639450244524556\n",
            "\n",
            "Epoch : 4, train loss : 0.33840540436458905\n",
            "Epoch : 4, val loss : 0.32422543205725757\n",
            "\n",
            "Epoch : 5, train loss : 0.3155262526436455\n",
            "Epoch : 5, val loss : 0.3033988834156291\n",
            "\n",
            "Epoch : 6, train loss : 0.2727165694531662\n",
            "Epoch : 6, val loss : 0.33422605790521787\n",
            "\n",
            "Epoch : 7, train loss : 0.24846136052488793\n",
            "Epoch : 7, val loss : 0.35660301408042083\n",
            "\n",
            "Epoch : 8, train loss : 0.22090360690025157\n",
            "Epoch : 8, val loss : 0.3244769999190517\n",
            "\n",
            "Epoch : 9, train loss : 0.19954020338557718\n",
            "Epoch : 9, val loss : 0.3506767457027151\n",
            "\n",
            "Epoch : 10, train loss : 0.16213305239959233\n",
            "Epoch : 10, val loss : 0.511422338207131\n",
            "\n",
            "Epoch : 11, train loss : 0.15738584799107513\n",
            "Epoch : 11, val loss : 0.43717747195826284\n",
            "\n",
            "Epoch : 12, train loss : 0.1707591832017285\n",
            "Epoch : 12, val loss : 0.5116309491694784\n",
            "\n",
            "Epoch : 13, train loss : 0.11842560955171294\n",
            "Epoch : 13, val loss : 0.41290478913477907\n",
            "\n",
            "Epoch : 14, train loss : 0.14069090052395794\n",
            "Epoch : 14, val loss : 0.5020643784740496\n",
            "\n",
            "Epoch : 15, train loss : 0.087932896463027\n",
            "Epoch : 15, val loss : 0.540561014181509\n",
            "\n",
            "Epoch : 16, train loss : 0.08286194958520474\n",
            "Epoch : 16, val loss : 0.45861042043923034\n",
            "\n",
            "Epoch : 17, train loss : 0.10094791423031575\n",
            "Epoch : 17, val loss : 0.5402785045874264\n",
            "\n",
            "Epoch : 18, train loss : 0.08362784400430345\n",
            "Epoch : 18, val loss : 0.5629575154339166\n",
            "\n",
            "Epoch : 19, train loss : 0.06272147937691072\n",
            "Epoch : 19, val loss : 0.6065780738338263\n",
            "\n",
            "Epoch : 20, train loss : 0.09190702687527114\n",
            "Epoch : 20, val loss : 0.5339404100390234\n",
            "0: 0.7062665224075317 / 239: 0.0029550900519143587\n",
            "1: 0.8385371565818787 / 239: 0.0064636137196209635\n",
            "2: 0.6909102201461792 / 239: 0.009354451460818366\n",
            "3: 0.696487307548523 / 239: 0.012268624295749424\n",
            "4: 0.6800966262817383 / 239: 0.015114216874334103\n",
            "5: 0.6685985922813416 / 239: 0.01791170052404683\n",
            "6: 0.7641621828079224 / 239: 0.02110903183286659\n",
            "7: 0.6927878260612488 / 239: 0.024007725665758844\n",
            "8: 0.7391383647918701 / 239: 0.027100354807147422\n",
            "9: 0.6716165542602539 / 239: 0.02991046591283886\n",
            "10: 0.6951369047164917 / 239: 0.032818988526715394\n",
            "11: 0.682679295539856 / 239: 0.03567538725282358\n",
            "12: 0.6603268980979919 / 239: 0.03843826130344279\n",
            "13: 0.6422320008277893 / 239: 0.041125424486822666\n",
            "14: 0.6431868076324463 / 239: 0.0438165826777534\n",
            "15: 0.6852210164070129 / 239: 0.04668361621920534\n",
            "16: 0.6411457061767578 / 239: 0.049366234236681315\n",
            "17: 0.6336150765419006 / 239: 0.052017343343551195\n",
            "18: 0.6779066324234009 / 239: 0.05485377276791689\n",
            "19: 0.7344562411308289 / 239: 0.05792681143373626\n",
            "20: 0.8228391408920288 / 239: 0.06136965302742675\n",
            "21: 0.8598887324333191 / 239: 0.06496751383258709\n",
            "22: 0.7226627469062805 / 239: 0.06799120733428701\n",
            "23: 0.655520498752594 / 239: 0.0707339709273941\n",
            "24: 0.6190060973167419 / 239: 0.07332395459817545\n",
            "25: 0.6714988946914673 / 239: 0.0761335734044159\n",
            "26: 0.6635702848434448 / 239: 0.07891001811087382\n",
            "27: 0.6253966093063354 / 239: 0.08152674032554469\n",
            "28: 0.6704533100128174 / 239: 0.08433198430049371\n",
            "29: 0.5813309550285339 / 239: 0.0867643313926633\n",
            "30: 0.5532522797584534 / 239: 0.08907919448788695\n",
            "31: 0.7106587290763855 / 239: 0.09205266197356221\n",
            "32: 0.7804826498031616 / 239: 0.0953182797551654\n",
            "33: 0.5174529552459717 / 239: 0.09748335488171758\n",
            "34: 0.5457180142402649 / 239: 0.09976669385343417\n",
            "35: 0.6035722494125366 / 239: 0.10229210075474186\n",
            "36: 0.7916815876960754 / 239: 0.10560457601706853\n",
            "37: 0.8347647786140442 / 239: 0.10909731567654152\n",
            "38: 0.6293849349021912 / 239: 0.11173072544600675\n",
            "39: 0.6121312975883484 / 239: 0.11429194426436803\n",
            "40: 0.5666815042495728 / 239: 0.1166629965834039\n",
            "41: 0.6280246376991272 / 239: 0.11929071473277263\n",
            "42: 0.6835650205612183 / 239: 0.12215081942131328\n",
            "43: 0.6573199033737183 / 239: 0.12490111190404851\n",
            "44: 0.588142454624176 / 239: 0.12736195899452624\n",
            "45: 0.565859854221344 / 239: 0.1297295734473352\n",
            "46: 0.6179198026657104 / 239: 0.13231501195221265\n",
            "47: 0.6428599953651428 / 239: 0.13500480272779902\n",
            "48: 0.5889630317687988 / 239: 0.1374690831954509\n",
            "49: 0.47205790877342224 / 239: 0.13944422088906355\n",
            "50: 0.5156859755516052 / 239: 0.1416019027951372\n",
            "51: 0.6459819674491882 / 239: 0.1443047562154267\n",
            "52: 0.4530818462371826 / 239: 0.1462004961578417\n",
            "53: 0.6214728951454163 / 239: 0.14880080115845012\n",
            "54: 0.5216042399406433 / 239: 0.15098324567703023\n",
            "55: 0.533370852470398 / 239: 0.15321492288401933\n",
            "56: 0.4568916857242584 / 239: 0.15512660357742628\n",
            "57: 0.6780029535293579 / 239: 0.1579634360189717\n",
            "58: 0.44017913937568665 / 239: 0.15980518974020888\n",
            "59: 0.6049549579620361 / 239: 0.16233638203293707\n",
            "60: 0.847926676273346 / 239: 0.16588419239391342\n",
            "61: 0.7439601421356201 / 239: 0.16899699633590345\n",
            "62: 0.6308427453041077 / 239: 0.1716365057304813\n",
            "63: 0.818109393119812 / 239: 0.1750595575845391\n",
            "64: 0.40342193841934204 / 239: 0.17674751548587525\n",
            "65: 0.4878033995628357 / 239: 0.17878853389408794\n",
            "66: 0.6504141092300415 / 239: 0.18150993184065714\n",
            "67: 0.5855326652526855 / 239: 0.18395985931033365\n",
            "68: 0.6588431596755981 / 239: 0.18671652525039892\n",
            "69: 0.5211046934127808 / 239: 0.18889687961614277\n",
            "70: 0.4310927987098694 / 239: 0.19070061517559828\n",
            "71: 0.8486589193344116 / 239: 0.1942514893150728\n",
            "72: 0.6328356862068176 / 239: 0.19689933737451557\n",
            "73: 0.7257595658302307 / 239: 0.19993598827757092\n",
            "74: 0.568178117275238 / 239: 0.20231330257579366\n",
            "75: 0.5289490818977356 / 239: 0.20452647865067958\n",
            "76: 0.6858320832252502 / 239: 0.20739606895706136\n",
            "77: 0.5071413516998291 / 239: 0.20951799929890166\n",
            "78: 0.4933585226535797 / 239: 0.21158226089996265\n",
            "79: 0.5138898491859436 / 239: 0.21373242763295824\n",
            "80: 0.6287023425102234 / 239: 0.21636298136731064\n",
            "81: 0.45196351408958435 / 239: 0.21825404209571894\n",
            "82: 0.6098577380180359 / 239: 0.22080574811253081\n",
            "83: 0.5715678930282593 / 239: 0.2231972455729001\n",
            "84: 0.5530636310577393 / 239: 0.22551131934301616\n",
            "85: 0.5456938743591309 / 239: 0.227794557311046\n",
            "86: 0.4698878526687622 / 239: 0.22976061527200317\n",
            "87: 0.5708139538764954 / 239: 0.2321489581752521\n",
            "88: 0.48636457324028015 / 239: 0.2341839563896466\n",
            "89: 0.5685426592826843 / 239: 0.23656279596823523\n",
            "90: 0.42391493916511536 / 239: 0.23833649864256626\n",
            "91: 0.3274604082107544 / 239: 0.2397066258735736\n",
            "92: 0.3058616518974304 / 239: 0.24098638173925324\n",
            "93: 0.41772782802581787 / 239: 0.24273419691927758\n",
            "94: 0.4327004551887512 / 239: 0.24454465907487904\n",
            "95: 0.3905826807022095 / 239: 0.24617889623262887\n",
            "96: 0.540838360786438 / 239: 0.2484418182442876\n",
            "97: 0.5741506218910217 / 239: 0.2508441221015722\n",
            "98: 0.5685869455337524 / 239: 0.25322314697828247\n",
            "99: 0.4046432673931122 / 239: 0.2549162150426888\n",
            "100: 0.3611477315425873 / 239: 0.256427293417344\n",
            "101: 0.5816807746887207 / 239: 0.25886110419010017\n",
            "102: 0.27786099910736084 / 239: 0.2600237025127251\n",
            "103: 0.5645115375518799 / 239: 0.26238567547319325\n",
            "104: 0.8531972169876099 / 239: 0.2659555383057774\n",
            "105: 0.5523951053619385 / 239: 0.26826681489724996\n",
            "106: 0.6332886815071106 / 239: 0.2709165583345182\n",
            "107: 0.661159873008728 / 239: 0.27368291763581\n",
            "108: 0.41550910472869873 / 239: 0.2754214494547585\n",
            "109: 0.6109124422073364 / 239: 0.2779775684598101\n",
            "110: 1.0789552927017212 / 239: 0.2824920257514491\n",
            "111: 0.6225327849388123 / 239: 0.28509676543738555\n",
            "112: 0.6529867649078369 / 239: 0.28782892763365264\n",
            "113: 0.4415023624897003 / 239: 0.2896762178532748\n",
            "114: 0.8735458850860596 / 239: 0.29333122155656377\n",
            "115: 0.5047956109046936 / 239: 0.2954433370833617\n",
            "116: 0.4604707956314087 / 239: 0.29736999313202866\n",
            "117: 0.5775228142738342 / 239: 0.29978640658087313\n",
            "118: 0.7308956384658813 / 239: 0.3028445473275923\n",
            "119: 0.34186652302742004 / 239: 0.3042749511896317\n",
            "120: 0.45482751727104187 / 239: 0.30617799519494987\n",
            "121: 0.36706194281578064 / 239: 0.3077138192234678\n",
            "122: 0.32019567489624023 / 239: 0.30905355008077423\n",
            "123: 0.7022866010665894 / 239: 0.3119919877421407\n",
            "124: 0.5468483567237854 / 239: 0.31428005618031557\n",
            "125: 1.3948578834533691 / 239: 0.32011628163409533\n",
            "126: 0.5469819903373718 / 239: 0.32240490920872866\n",
            "127: 0.3781876862049103 / 239: 0.32398728446481617\n",
            "128: 0.4616146981716156 / 239: 0.32591872671658023\n",
            "129: 0.584729790687561 / 239: 0.3283652948784529\n",
            "130: 0.507055401802063 / 239: 0.33048686559728996\n",
            "131: 0.3933767080307007 / 239: 0.33213279324595396\n",
            "132: 0.5444610118865967 / 239: 0.3344108727935966\n",
            "133: 0.38649189472198486 / 239: 0.33602799369201497\n",
            "134: 0.4246572256088257 / 239: 0.3378048021673657\n",
            "135: 0.26107460260391235 / 239: 0.33889716452135693\n",
            "136: 0.6947381496429443 / 239: 0.3418040187039634\n",
            "137: 0.40512752532958984 / 239: 0.3434991129522043\n",
            "138: 0.2959398627281189 / 239: 0.34473735505566927\n",
            "139: 0.33677437901496887 / 239: 0.3461464528758156\n",
            "140: 0.28192439675331116 / 239: 0.3473260528622311\n",
            "141: 0.3294755220413208 / 239: 0.34870461153186005\n",
            "142: 0.8482805490493774 / 239: 0.3522539025320667\n",
            "143: 0.672988772392273 / 239: 0.35506975513621847\n",
            "144: 0.3808992803096771 / 239: 0.3566634759743343\n",
            "145: 0.283686101436615 / 239: 0.3578504471100523\n",
            "146: 0.6486209034919739 / 239: 0.3605643421037426\n",
            "147: 0.31063637137413025 / 239: 0.3618640758751825\n",
            "148: 0.656552255153656 / 239: 0.364611156440679\n",
            "149: 0.2975364327430725 / 239: 0.36585607875341153\n",
            "150: 0.3703940510749817 / 239: 0.36740584465749093\n",
            "151: 0.3160833418369293 / 239: 0.36872836910032325\n",
            "152: 0.46522045135498047 / 239: 0.37067489818549054\n",
            "153: 0.32920652627944946 / 239: 0.3720523313498397\n",
            "154: 0.2234111726284027 / 239: 0.37298710613071173\n",
            "155: 0.41544926166534424 / 239: 0.37472538756027385\n",
            "156: 0.43036800622940063 / 239: 0.3765260905152086\n",
            "157: 0.3074718713760376 / 239: 0.3778125837008824\n",
            "158: 0.536857545375824 / 239: 0.38005884958111597\n",
            "159: 0.37285009026527405 / 239: 0.38161889179979913\n",
            "160: 0.4841648042201996 / 239: 0.3836446859597163\n",
            "161: 0.42269694805145264 / 239: 0.385413292436919\n",
            "162: 0.5831618309020996 / 239: 0.3878533000975973\n",
            "163: 0.3082449436187744 / 239: 0.38914302789516536\n",
            "164: 0.861763596534729 / 239: 0.3927487333199969\n",
            "165: 0.911442756652832 / 239: 0.3965623013394648\n",
            "166: 1.1118662357330322 / 239: 0.40121446132161137\n",
            "167: 0.35719960927963257 / 239: 0.40270902035625417\n",
            "168: 0.47919511795043945 / 239: 0.40471402084977065\n",
            "169: 0.5169183611869812 / 239: 0.40687685918109695\n",
            "170: 0.426703542470932 / 239: 0.40866222965168664\n",
            "171: 0.4701407253742218 / 239: 0.41062934565743653\n",
            "172: 0.4793441593647003 / 239: 0.41263496975519676\n",
            "173: 0.2623331546783447 / 239: 0.41373259801744927\n",
            "174: 0.538114070892334 / 239: 0.4159841213266222\n",
            "175: 0.4357566833496094 / 239: 0.4178073710477503\n",
            "176: 0.23807822167873383 / 239: 0.4188035142346906\n",
            "177: 0.29785609245300293 / 239: 0.42004977403574917\n",
            "178: 0.5263484716415405 / 239: 0.422252068896174\n",
            "179: 0.8165946006774902 / 239: 0.42566878270654007\n",
            "180: 0.4083733856678009 / 239: 0.42737745796038024\n",
            "181: 0.5144442319869995 / 239: 0.42952994428668567\n",
            "182: 0.4139951169490814 / 239: 0.43126214142873204\n",
            "183: 0.5588157773017883 / 239: 0.4336002827563546\n",
            "184: 0.1879427582025528 / 239: 0.43438665412958705\n",
            "185: 0.3465609550476074 / 239: 0.43583669996660634\n",
            "186: 0.35165685415267944 / 239: 0.4373080675572033\n",
            "187: 0.37811946868896484 / 239: 0.4388901573843538\n",
            "188: 0.6753167510032654 / 239: 0.44171575048478584\n",
            "189: 0.2808297872543335 / 239: 0.4428907705151387\n",
            "190: 0.3363288938999176 / 239: 0.4442980043808287\n",
            "191: 0.44404762983322144 / 239: 0.4461559442546079\n",
            "192: 0.4442164897918701 / 239: 0.4480145906554107\n",
            "193: 0.3305615484714508 / 239: 0.4493976933686804\n",
            "194: 0.2274240255355835 / 239: 0.45034925832908035\n",
            "195: 0.9190702438354492 / 239: 0.45419474052086045\n",
            "196: 0.3013561964035034 / 239: 0.4554556451083228\n",
            "197: 0.7928200960159302 / 239: 0.45877288400378696\n",
            "198: 0.608110785484314 / 239: 0.46131728059577154\n",
            "199: 0.41378241777420044 / 239: 0.4630485877831113\n",
            "200: 0.2909722328186035 / 239: 0.4642660448241933\n",
            "201: 0.30517667531967163 / 239: 0.46554293467908736\n",
            "202: 0.3417457044124603 / 239: 0.46697283302390935\n",
            "203: 0.3356059193611145 / 239: 0.46837704189152907\n",
            "204: 0.8355529308319092 / 239: 0.4718730792590266\n",
            "205: 0.4345688223838806 / 239: 0.47369135885059094\n",
            "206: 0.34711089730262756 / 239: 0.47514370570122955\n",
            "207: 0.5561296939849854 / 239: 0.47747060818652237\n",
            "208: 0.7416253089904785 / 239: 0.4805736429521729\n",
            "209: 0.921087920665741 / 239: 0.4844275673064229\n",
            "210: 0.4212985038757324 / 239: 0.48619032255276484\n",
            "211: 0.6353263258934021 / 239: 0.4888485916987623\n",
            "212: 0.27210843563079834 / 239: 0.48998712071813805\n",
            "213: 0.3639310896396637 / 239: 0.49150984494257177\n",
            "214: 0.3618139624595642 / 239: 0.4930237108942854\n",
            "215: 1.07224702835083 / 239: 0.49751010013424707\n",
            "216: 0.7053492665290833 / 239: 0.5004613522954566\n",
            "217: 0.3176245391368866 / 239: 0.5017903252625566\n",
            "218: 0.48622947931289673 / 239: 0.503824758230393\n",
            "219: 0.33037129044532776 / 239: 0.5052070648849759\n",
            "220: 0.4014962613582611 / 239: 0.506886965560115\n",
            "221: 0.74212646484375 / 239: 0.509992097212181\n",
            "222: 0.6456730961799622 / 239: 0.5126936582840637\n",
            "223: 0.3480803072452545 / 239: 0.5141500612432488\n",
            "224: 0.775143563747406 / 239: 0.5173933397526522\n",
            "225: 0.45979392528533936 / 239: 0.519317163707821\n",
            "226: 0.38997384905815125 / 239: 0.5209488534528341\n",
            "227: 0.5952105522155762 / 239: 0.5234392741734014\n",
            "228: 0.39288103580474854 / 239: 0.5250831278796975\n",
            "229: 0.7653344869613647 / 239: 0.5282853642268162\n",
            "230: 0.4566778540611267 / 239: 0.5301961502270719\n",
            "231: 0.26083841919898987 / 239: 0.5312875243659798\n",
            "232: 0.4946405291557312 / 239: 0.5333571500109828\n",
            "233: 0.5006338953971863 / 239: 0.5354518525021844\n",
            "234: 0.5912331938743591 / 239: 0.537925631556052\n",
            "235: 0.41838693618774414 / 239: 0.5396762045108124\n",
            "236: 0.330081045627594 / 239: 0.541057296751932\n",
            "237: 0.6141356229782104 / 239: 0.5436269018689956\n",
            "238: 0.5827275514602661 / 239: 0.5460650924608796\n",
            "\n",
            "Epoch : 1, train loss : 0.5460650924608796\n",
            "Epoch : 1, val loss : 0.47750232093360107\n",
            "\n",
            "Epoch : 2, train loss : 0.3832197789741862\n",
            "Epoch : 2, val loss : 0.35786555286334903\n",
            "\n",
            "Epoch : 3, train loss : 0.2986719706738721\n",
            "Epoch : 3, val loss : 0.527108379761162\n",
            "\n",
            "Epoch : 4, train loss : 0.238772223380133\n",
            "Epoch : 4, val loss : 0.444530209728881\n",
            "\n",
            "Epoch : 5, train loss : 0.19886154947786827\n",
            "Epoch : 5, val loss : 0.3914318300641912\n",
            "\n",
            "Epoch : 6, train loss : 0.16203491149711818\n",
            "Epoch : 6, val loss : 0.4860023671720663\n",
            "\n",
            "Epoch : 7, train loss : 0.15051009557034548\n",
            "Epoch : 7, val loss : 0.45012229773134943\n",
            "\n",
            "Epoch : 8, train loss : 0.12659716280956582\n",
            "Epoch : 8, val loss : 0.4640267122911452\n",
            "\n",
            "Epoch : 9, train loss : 0.1065104994922876\n",
            "Epoch : 9, val loss : 0.4909298184692212\n",
            "\n",
            "Epoch : 10, train loss : 0.08754014574131572\n",
            "Epoch : 10, val loss : 0.5879742503824196\n",
            "\n",
            "Epoch : 11, train loss : 0.08291638474210031\n",
            "Epoch : 11, val loss : 0.5901190209412126\n",
            "\n",
            "Epoch : 12, train loss : 0.05479898355671332\n",
            "Epoch : 12, val loss : 0.663499364937509\n",
            "\n",
            "Epoch : 13, train loss : 0.062320649006236385\n",
            "Epoch : 13, val loss : 0.6965104154837521\n",
            "\n",
            "Epoch : 14, train loss : 0.07445573876027378\n",
            "Epoch : 14, val loss : 0.7381888286219456\n",
            "\n",
            "Epoch : 15, train loss : 0.04426921327563321\n",
            "Epoch : 15, val loss : 0.6442954387024177\n",
            "\n",
            "Epoch : 16, train loss : 0.0363120996036711\n",
            "Epoch : 16, val loss : 0.8556072577687844\n",
            "\n",
            "Epoch : 17, train loss : 0.07875020131390834\n",
            "Epoch : 17, val loss : 0.8284333658040218\n",
            "\n",
            "Epoch : 18, train loss : 0.03211967651069818\n",
            "Epoch : 18, val loss : 0.8054720451087312\n",
            "\n",
            "Epoch : 19, train loss : 0.022943171552698028\n",
            "Epoch : 19, val loss : 0.8351403667531246\n",
            "\n",
            "Epoch : 20, train loss : 0.03054628885501145\n",
            "Epoch : 20, val loss : 0.7255204984046388\n",
            "0: 0.7253875136375427 / 239: 0.0030350941993202625\n",
            "1: 0.6579135656356812 / 239: 0.0057878706245741585\n",
            "2: 1.042812705039978 / 239: 0.010151103700055238\n",
            "3: 0.7365404367446899 / 239: 0.01323286284961461\n",
            "4: 0.6788128018379211 / 239: 0.016073083777806748\n",
            "5: 0.6431978940963745 / 239: 0.018764288355615846\n",
            "6: 0.7434338331222534 / 239: 0.021874890167842848\n",
            "7: 0.817107081413269 / 239: 0.025293748249069915\n",
            "8: 0.6467700004577637 / 239: 0.027999898878600306\n",
            "9: 0.6875888109207153 / 239: 0.030876839510067734\n",
            "10: 0.6759856939315796 / 239: 0.03370523153488606\n",
            "11: 0.6376372575759888 / 239: 0.03637316985110359\n",
            "12: 0.6091121435165405 / 239: 0.03892175622564978\n",
            "13: 0.6029114127159119 / 239: 0.041444398119858616\n",
            "14: 0.728832483291626 / 239: 0.04449390641814994\n",
            "15: 0.7854809165000916 / 239: 0.047780437449531074\n",
            "16: 0.9543838500976562 / 239: 0.05177367531604846\n",
            "17: 0.6855791807174683 / 239: 0.054642207452941634\n",
            "18: 0.484333872795105 / 239: 0.05666870901275379\n",
            "19: 0.6017889976501465 / 239: 0.05918665460961633\n",
            "20: 0.6891473531723022 / 239: 0.06207011633837073\n",
            "21: 0.7176869511604309 / 239: 0.06507299061100852\n",
            "22: 0.6193462014198303 / 239: 0.0676643973115099\n",
            "23: 0.6890896558761597 / 239: 0.07054761762898337\n",
            "24: 0.6413485407829285 / 239: 0.07323108432681989\n",
            "25: 0.6477781534194946 / 239: 0.07594145316957929\n",
            "26: 0.6482143402099609 / 239: 0.07865364706167118\n",
            "27: 0.5653719305992126 / 239: 0.08101921999304863\n",
            "28: 0.5442571640014648 / 239: 0.08329644662066983\n",
            "29: 0.48362940549850464 / 239: 0.08532000061857152\n",
            "30: 0.6080816984176636 / 239: 0.08786427550734835\n",
            "31: 0.6311020851135254 / 239: 0.0905048700057313\n",
            "32: 0.6716274619102478 / 239: 0.09331502675012565\n",
            "33: 0.653406023979187 / 239: 0.09604894316844861\n",
            "34: 0.6450368165969849 / 239: 0.09874784198266194\n",
            "35: 0.6608883738517761 / 239: 0.10151306530421748\n",
            "36: 0.8850859999656677 / 239: 0.1052163540070027\n",
            "37: 0.7590466141700745 / 239: 0.10839228126294444\n",
            "38: 0.6535658836364746 / 239: 0.11112686655012634\n",
            "39: 0.8188969492912292 / 239: 0.11455321361829048\n",
            "40: 0.6269777417182922 / 239: 0.11717655144974777\n",
            "41: 0.6505364179611206 / 239: 0.11989846114832986\n",
            "42: 0.6369658708572388 / 239: 0.12256359031509655\n",
            "43: 0.38277071714401245 / 239: 0.12416514143285393\n",
            "44: 0.45151588320732117 / 239: 0.12605432922870047\n",
            "45: 0.8232294917106628 / 239: 0.12949880408941453\n",
            "46: 1.0703086853027344 / 239: 0.1339770831074176\n",
            "47: 0.6898578405380249 / 239: 0.13686351758665619\n",
            "48: 0.6616812348365784 / 239: 0.13963205831818998\n",
            "49: 0.5942548513412476 / 239: 0.14211848029032909\n",
            "50: 0.5239015221595764 / 239: 0.14431053686840262\n",
            "51: 0.5279282331466675 / 239: 0.14651944160960206\n",
            "52: 0.7773265838623047 / 239: 0.14977185409438157\n",
            "53: 0.6847454309463501 / 239: 0.15263689773850855\n",
            "54: 0.6695199012756348 / 239: 0.15543823623756978\n",
            "55: 0.4761611819267273 / 239: 0.1574305424381\n",
            "56: 0.5412594079971313 / 239: 0.15969522615356918\n",
            "57: 0.6890010833740234 / 239: 0.16257807587479942\n",
            "58: 0.6329262256622314 / 239: 0.16522630276041544\n",
            "59: 0.4370653033256531 / 239: 0.1670550278789328\n",
            "60: 0.6739019155502319 / 239: 0.16987470116575387\n",
            "61: 0.35674184560775757 / 239: 0.17136734487122565\n",
            "62: 0.3115130066871643 / 239: 0.1726707465728456\n",
            "63: 0.48162204027175903 / 239: 0.17468590155306216\n",
            "64: 0.4175345301628113 / 239: 0.17643290795541702\n",
            "65: 0.257094144821167 / 239: 0.17750861567433404\n",
            "66: 0.8839311599731445 / 239: 0.18120707241062334\n",
            "67: 0.37238121032714844 / 239: 0.1827651527885612\n",
            "68: 0.8703876733779907 / 239: 0.186406942216921\n",
            "69: 0.37277501821517944 / 239: 0.18796667032660794\n",
            "70: 0.663831353187561 / 239: 0.19074420736923373\n",
            "71: 0.5070231556892395 / 239: 0.19286564316709665\n",
            "72: 1.003906011581421 / 239: 0.19706608673019882\n",
            "73: 0.7756832242012024 / 239: 0.20031162323313273\n",
            "74: 0.6836448907852173 / 239: 0.20317206210671104\n",
            "75: 0.24367709457874298 / 239: 0.2041916315400949\n",
            "76: 0.402508020401001 / 239: 0.2058757655166681\n",
            "77: 0.35941049456596375 / 239: 0.20737957511736252\n",
            "78: 0.575478196144104 / 239: 0.20978743367863492\n",
            "79: 0.377424955368042 / 239: 0.21136661759230874\n",
            "80: 0.3574671149253845 / 239: 0.21286229589743585\n",
            "81: 0.491411954164505 / 239: 0.21491841286046726\n",
            "82: 0.38603928685188293 / 239: 0.21653364000210693\n",
            "83: 0.24805401265621185 / 239: 0.21757152290025006\n",
            "84: 0.3761069178581238 / 239: 0.21914519201262714\n",
            "85: 1.0656437873840332 / 239: 0.22360395262929672\n",
            "86: 0.4989427328109741 / 239: 0.22569157912641377\n",
            "87: 0.44996634125709534 / 239: 0.22757428348313802\n",
            "88: 0.2679787874221802 / 239: 0.22869553363971618\n",
            "89: 0.6919310688972473 / 239: 0.23159064271460006\n",
            "90: 0.45020341873168945 / 239: 0.23347433902728496\n",
            "91: 0.4035080075263977 / 239: 0.23516265705040795\n",
            "92: 0.6667977571487427 / 239: 0.2379526058250889\n",
            "93: 0.45203307271003723 / 239: 0.23984395759375013\n",
            "94: 0.21668508648872375 / 239: 0.2407505897547908\n",
            "95: 0.9229773879051208 / 239: 0.24461241982970763\n",
            "96: 0.4227317273616791 / 239: 0.24638117182703684\n",
            "97: 0.511110782623291 / 239: 0.24851971066646483\n",
            "98: 0.37032458186149597 / 239: 0.25006918590437904\n",
            "99: 0.27077001333236694 / 239: 0.2512021148304559\n",
            "100: 0.5854117274284363 / 239: 0.2536515362841314\n",
            "101: 0.4590713381767273 / 239: 0.2555723368622767\n",
            "102: 0.719635546207428 / 239: 0.25858336425226597\n",
            "103: 0.20264621078968048 / 239: 0.25943125634762026\n",
            "104: 0.42986994981765747 / 239: 0.26122987538451425\n",
            "105: 0.5590375661849976 / 239: 0.2635689446990958\n",
            "106: 0.40435972809791565 / 239: 0.26526082640661847\n",
            "107: 0.5988790392875671 / 239: 0.26776659644547857\n",
            "108: 0.2368844598531723 / 239: 0.2687577448130651\n",
            "109: 0.6510650515556335 / 239: 0.27148186636769117\n",
            "110: 0.7521646022796631 / 239: 0.2746289985948027\n",
            "111: 0.5480833053588867 / 239: 0.2769222341820784\n",
            "112: 0.6256330609321594 / 239: 0.2795399457340958\n",
            "113: 0.42848730087280273 / 239: 0.2813327796289611\n",
            "114: 0.4847134053707123 / 239: 0.28336086919118164\n",
            "115: 0.8948764801025391 / 239: 0.287105122246004\n",
            "116: 0.6691977977752686 / 239: 0.28990511303167454\n",
            "117: 0.5365901589393616 / 239: 0.29215026014020745\n",
            "118: 0.6111347675323486 / 239: 0.2947073093767445\n",
            "119: 0.752733588218689 / 239: 0.2978568222981616\n",
            "120: 0.5622288584709167 / 239: 0.300209244300132\n",
            "121: 0.4938654899597168 / 239: 0.30227562710331074\n",
            "122: 0.6015774011611938 / 239: 0.30479268735921533\n",
            "123: 0.6959233283996582 / 239: 0.30770450044875364\n",
            "124: 0.4532991647720337 / 239: 0.30960114967374125\n",
            "125: 0.9482679963111877 / 239: 0.31356879819387173\n",
            "126: 0.4700946807861328 / 239: 0.3155357215444413\n",
            "127: 0.4976128339767456 / 239: 0.3176177836112896\n",
            "128: 0.539077877998352 / 239: 0.31987333958617814\n",
            "129: 0.481067419052124 / 239: 0.32188617397551755\n",
            "130: 0.4478411078453064 / 239: 0.32375998614223434\n",
            "131: 0.5797574520111084 / 239: 0.32618574953977036\n",
            "132: 0.30355104804039 / 239: 0.32745583760688496\n",
            "133: 0.5812494158744812 / 239: 0.32988784353104594\n",
            "134: 0.3324882388114929 / 239: 0.33127900771017355\n",
            "135: 0.6580614447593689 / 239: 0.33403240287653074\n",
            "136: 0.5840487480163574 / 239: 0.33647612148747785\n",
            "137: 0.4741012454032898 / 239: 0.3384598087067385\n",
            "138: 0.5125852227210999 / 239: 0.34060451675159664\n",
            "139: 0.8075655102729797 / 239: 0.3439834519410233\n",
            "140: 0.255025714635849 / 239: 0.34505050514033647\n",
            "141: 0.3910941183567047 / 239: 0.3466868822045905\n",
            "142: 0.4230476915836334 / 239: 0.348456956227953\n",
            "143: 1.014037013053894 / 239: 0.352699788918555\n",
            "144: 0.7060655355453491 / 239: 0.3556540380212552\n",
            "145: 0.4189087748527527 / 239: 0.3574067944013923\n",
            "146: 0.7122610211372375 / 239: 0.3603869660379498\n",
            "147: 0.33062800765037537 / 239: 0.3617703468230978\n",
            "148: 0.324607789516449 / 239: 0.3631285384110327\n",
            "149: 0.5503485202789307 / 239: 0.36543125188500314\n",
            "150: 0.44786152243614197 / 239: 0.36730514946841797\n",
            "151: 0.4971869885921478 / 239: 0.3693854297554144\n",
            "152: 0.6329867839813232 / 239: 0.37203391002311864\n",
            "153: 0.47815078496932983 / 239: 0.3740345409225719\n",
            "154: 0.37741899490356445 / 239: 0.3756136998970638\n",
            "155: 0.37049612402915955 / 239: 0.3771638928846335\n",
            "156: 0.17805060744285583 / 239: 0.3779088745057333\n",
            "157: 0.597108006477356 / 239: 0.3804072343654712\n",
            "158: 0.7462242841720581 / 239: 0.38352951170510324\n",
            "159: 0.4223242998123169 / 239: 0.3852965589846527\n",
            "160: 0.3693704605102539 / 239: 0.386842042083022\n",
            "161: 0.6211165189743042 / 239: 0.38944085596994377\n",
            "162: 0.5288222432136536 / 239: 0.39165350133903853\n",
            "163: 0.5876072645187378 / 239: 0.39411210914037215\n",
            "164: 0.5644661784172058 / 239: 0.3964738923136659\n",
            "165: 0.4879072606563568 / 239: 0.39851534528712346\n",
            "166: 0.4018731117248535 / 239: 0.40019682274203916\n",
            "167: 0.44151705503463745 / 239: 0.4020441744367448\n",
            "168: 0.6749576330184937 / 239: 0.4048682649514665\n",
            "169: 0.6092680096626282 / 239: 0.4074175034856197\n",
            "170: 0.5185321569442749 / 239: 0.4095870941004493\n",
            "171: 0.38057371973991394 / 239: 0.4111794527604489\n",
            "172: 0.7176404595375061 / 239: 0.41418213250746777\n",
            "173: 0.3620968461036682 / 239: 0.4156971820727551\n",
            "174: 0.7839334011077881 / 239: 0.4189772381443358\n",
            "175: 0.49765756726264954 / 239: 0.42105948737974436\n",
            "176: 0.46850770711898804 / 239: 0.42301977067312924\n",
            "177: 0.308677613735199 / 239: 0.4243113088059125\n",
            "178: 0.3212089240550995 / 239: 0.42565527919944846\n",
            "179: 0.26565390825271606 / 239: 0.42676680182812093\n",
            "180: 0.2630918622016907 / 239: 0.4278676045988393\n",
            "181: 0.5060009956359863 / 239: 0.42998476357639576\n",
            "182: 0.4584583342075348 / 239: 0.4319029992843771\n",
            "183: 0.49367356300354004 / 239: 0.4339685790458982\n",
            "184: 0.25461316108703613 / 239: 0.4350339060797352\n",
            "185: 0.21844708919525146 / 239: 0.4359479106370375\n",
            "186: 0.8284047842025757 / 239: 0.43941403944123236\n",
            "187: 0.30083683133125305 / 239: 0.4406727709530786\n",
            "188: 0.5776989459991455 / 239: 0.4430899213547487\n",
            "189: 0.37629520893096924 / 239: 0.44466437829588246\n",
            "190: 0.3719886839389801 / 239: 0.44622081630399535\n",
            "191: 0.6244779229164124 / 239: 0.44883369464255773\n",
            "192: 0.5696775913238525 / 239: 0.4512172828907747\n",
            "193: 0.4038476347923279 / 239: 0.45290702194848315\n",
            "194: 0.5143222808837891 / 239: 0.45505899801912664\n",
            "195: 0.3082401156425476 / 239: 0.45634870561595736\n",
            "196: 0.31018269062042236 / 239: 0.45764654114156583\n",
            "197: 0.31313440203666687 / 239: 0.458956726924146\n",
            "198: 0.5070797801017761 / 239: 0.46107839964423714\n",
            "199: 0.5721555352210999 / 239: 0.46347235585855134\n",
            "200: 0.547502338886261 / 239: 0.4657631606237658\n",
            "201: 0.4533098340034485 / 239: 0.46765985448988906\n",
            "202: 0.1915474534034729 / 239: 0.4684613082698199\n",
            "203: 0.6792404651641846 / 239: 0.4713033185843144\n",
            "204: 0.3758479654788971 / 239: 0.4728759042139332\n",
            "205: 0.43199434876441956 / 239: 0.4746834119493492\n",
            "206: 0.19323132932186127 / 239: 0.4754919112352147\n",
            "207: 0.40611210465431213 / 239: 0.4771911250622202\n",
            "208: 0.820129930973053 / 239: 0.48062263104955516\n",
            "209: 0.26628339290618896 / 239: 0.4817367875052296\n",
            "210: 0.5337873697280884 / 239: 0.4839702074622509\n",
            "211: 1.0943807363510132 / 239: 0.488549206359117\n",
            "212: 0.608326256275177 / 239: 0.4910945045025278\n",
            "213: 0.4582764506340027 / 239: 0.49301197919137296\n",
            "214: 0.21395306289196014 / 239: 0.49390718029133934\n",
            "215: 0.35469353199005127 / 239: 0.49539125364694625\n",
            "216: 0.5094786882400513 / 239: 0.4975229636395824\n",
            "217: 0.5945229530334473 / 239: 0.5000105073761241\n",
            "218: 0.8944766521453857 / 239: 0.5037530875106236\n",
            "219: 0.49753689765930176 / 239: 0.5058348318522944\n",
            "220: 0.7885061502456665 / 239: 0.5091340207654562\n",
            "221: 0.5697078108787537 / 239: 0.5115177354553255\n",
            "222: 0.44793397188186646 / 239: 0.5133919361744965\n",
            "223: 0.4236198663711548 / 239: 0.5151644042346268\n",
            "224: 0.5051299929618835 / 239: 0.5172779188495301\n",
            "225: 0.2824842035770416 / 239: 0.5184598611239111\n",
            "226: 0.6117939352989197 / 239: 0.521019668384576\n",
            "227: 0.47074002027511597 / 239: 0.5229892919003715\n",
            "228: 0.19070330262184143 / 239: 0.5237872136686637\n",
            "229: 0.6168681979179382 / 239: 0.526368252153676\n",
            "230: 0.7509226202964783 / 239: 0.529510187803452\n",
            "231: 0.9145349860191345 / 239: 0.5333366940211053\n",
            "232: 0.39989173412323 / 239: 0.5350098811931691\n",
            "233: 0.3235670328140259 / 239: 0.53636371815055\n",
            "234: 0.5100175738334656 / 239: 0.5384976828946231\n",
            "235: 0.36108219623565674 / 239: 0.5400084870629731\n",
            "236: 0.6097787618637085 / 239: 0.5425598626356246\n",
            "237: 0.5719411969184875 / 239: 0.5449529220369572\n",
            "238: 0.37503761053085327 / 239: 0.5465221170600989\n",
            "\n",
            "Epoch : 1, train loss : 0.5465221170600989\n",
            "Epoch : 1, val loss : 0.47020106001392664\n",
            "\n",
            "Epoch : 2, train loss : 0.4422924096489551\n",
            "Epoch : 2, val loss : 0.4752536604747824\n",
            "\n",
            "Epoch : 3, train loss : 0.3693301471955608\n",
            "Epoch : 3, val loss : 0.37908095898835553\n",
            "\n",
            "Epoch : 4, train loss : 0.30165317140239056\n",
            "Epoch : 4, val loss : 0.36587742080345087\n",
            "\n",
            "Epoch : 5, train loss : 0.25353489604985113\n",
            "Epoch : 5, val loss : 0.34609155460139335\n",
            "\n",
            "Epoch : 6, train loss : 0.22182812547081285\n",
            "Epoch : 6, val loss : 0.42683896815161343\n",
            "\n",
            "Epoch : 7, train loss : 0.19127111859077986\n",
            "Epoch : 7, val loss : 0.4424972507151088\n",
            "\n",
            "Epoch : 8, train loss : 0.19461731400699317\n",
            "Epoch : 8, val loss : 0.4100840614561964\n",
            "\n",
            "Epoch : 9, train loss : 0.17072502379675852\n",
            "Epoch : 9, val loss : 0.41319846512709313\n",
            "\n",
            "Epoch : 10, train loss : 0.15370762174380131\n",
            "Epoch : 10, val loss : 0.38228279841100815\n",
            "\n",
            "Epoch : 11, train loss : 0.13182919852486247\n",
            "Epoch : 11, val loss : 0.4378958706840186\n",
            "\n",
            "Epoch : 12, train loss : 0.1225964020363886\n",
            "Epoch : 12, val loss : 0.4726975387539311\n",
            "\n",
            "Epoch : 13, train loss : 0.14818397368497815\n",
            "Epoch : 13, val loss : 0.41727120495077374\n",
            "\n",
            "Epoch : 14, train loss : 0.10981475786796951\n",
            "Epoch : 14, val loss : 0.45754016041958123\n",
            "\n",
            "Epoch : 15, train loss : 0.07365966555720903\n",
            "Epoch : 15, val loss : 0.5490344724081375\n",
            "\n",
            "Epoch : 16, train loss : 0.08262390085392742\n",
            "Epoch : 16, val loss : 0.5876298168415972\n",
            "\n",
            "Epoch : 17, train loss : 0.07343151915899318\n",
            "Epoch : 17, val loss : 0.4630745312872181\n",
            "\n",
            "Epoch : 18, train loss : 0.05770662453823932\n",
            "Epoch : 18, val loss : 0.6927602580468846\n",
            "\n",
            "Epoch : 19, train loss : 0.05342462639252279\n",
            "Epoch : 19, val loss : 0.5440593142361055\n",
            "\n",
            "Epoch : 20, train loss : 0.08040797460186644\n",
            "Epoch : 20, val loss : 0.5468150151478451\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seedn = 7\n",
        "seeds = [38333, 79984, 66390, 18079, 15244, 36378, 65024, 16278, 96089, 80817,\n",
        "         15820, 84086, 81869, 42235, 78231, 13501, 62904, 81499, 70093, 45182,\n",
        "         36201, 31709, 37794, 82643, 49473, 23698, 25148, 82890, 75009, 40721,\n",
        "         39663, 45414, 51407, 17793, 66530, 19383, 86574, 59920, 13187, 82918,\n",
        "         64587, 84040, 14681, 31941, 68987, 82403, 23492, 54427, 29316, 38279,\n",
        "         75782, 12617, 36385, 71490, 12924, 51894, 29646, 15556, 52387, 47171,\n",
        "         36631, 64924, 32661, 60206, 61079, 38763, 47185, 51165, 37078, 91493,\n",
        "         15507, 94325, 38695, 18167, 55667, 23816, 55812, 55002, 84690, 91869,\n",
        "         70293, 83810, 78368, 18832, 89594, 86696, 76356, 86349, 35784, 54066,\n",
        "         23793, 93586, 78475, 52226, 92427, 23244, 35298, 61167, 93357, 11480,\n",
        "         32909, 91040, 25266, 43908, 75798, 44285, 50841, 25611, 48329, 40202,\n",
        "         39898, 21002, 73790, 94005, 71650, 47142, 22625, 12788, 99651, 62684,\n",
        "         21942, 52884, 91096, 64224, 22993, 59695, 28217, 42843, 53415, 20168,\n",
        "         71887, 75207, 81045, 55057, 96637, 17166, 61124, 93537, 19632, 47096,\n",
        "         77510, 26684, 64662, 81874, 76832, 66655, 75951, 26273, 26645, 91141]\n",
        "\n",
        "for trial in range(1,5):\n",
        "  torch.manual_seed(seeds[seedn])\n",
        "  tname = \"NEWCLStrial\"+str(trial)\n",
        "\n",
        "  # FIT / EVALUATE MODELS: IMAGE CLASSIFICATION\n",
        "  # A\n",
        "  train_dataA = datasets.ImageFolder(traindirA,transform=train_transforms)\n",
        "  val_dataA = datasets.ImageFolder(valdirA,transform=val_transforms)\n",
        "\n",
        "  trainloaderA = torch.utils.data.DataLoader(train_dataA, shuffle = True, batch_size=8)\n",
        "  valloaderA = torch.utils.data.DataLoader(val_dataA, shuffle = True, batch_size=8)\n",
        "\n",
        "  modelAschema = vision_transformer_clspred.VitAttentionSchema().to(device)\n",
        "  modelAcontrol = vision_transformer_clspred.VitControl().to(device)\n",
        "\n",
        "  fitschema(modelAschema, trainloaderA, valloaderA, tname+\"A\")\n",
        "  fitcontrol(modelAcontrol, trainloaderA, valloaderA, tname+\"A\")\n",
        "\n",
        "  evaluate(modelAschema, valloaderA, tname+\"Aschema\", save_attn=False)\n",
        "  evaluate(modelAcontrol, valloaderA, tname+\"Acontrol\", save_attn=False)\n",
        "\n",
        "  del(train_dataA)\n",
        "  del(val_dataA)\n",
        "  del(trainloaderA)\n",
        "  del(valloaderA)\n",
        "\n",
        "  # B\n",
        "  train_dataB = datasets.ImageFolder(traindirB,transform=train_transforms)\n",
        "  val_dataB = datasets.ImageFolder(valdirB,transform=val_transforms)\n",
        "\n",
        "  trainloaderB = torch.utils.data.DataLoader(train_dataB, shuffle = True, batch_size=8)\n",
        "  valloaderB = torch.utils.data.DataLoader(val_dataB, shuffle = True, batch_size=8)\n",
        "\n",
        "  modelBschema = vision_transformer_clspred.VitAttentionSchema().to(device)\n",
        "  modelBcontrol = vision_transformer_clspred.VitControl().to(device)\n",
        "\n",
        "  fitschema(modelBschema, trainloaderB, valloaderB, tname+\"B\")\n",
        "  fitcontrol(modelBcontrol, trainloaderB, valloaderB, tname+\"B\")\n",
        "\n",
        "  evaluate(modelBschema, valloaderB, tname+\"Bschema\", save_attn=False)\n",
        "  evaluate(modelBcontrol, valloaderB, tname+\"Bcontrol\", save_attn=False)\n",
        "\n",
        "  del(train_dataB)\n",
        "  del(val_dataB)\n",
        "  del(trainloaderB)\n",
        "  del(valloaderB)\n",
        "\n",
        "  # C\n",
        "  train_dataC = datasets.ImageFolder(traindirC,transform=train_transforms)\n",
        "  val_dataC = datasets.ImageFolder(valdirC,transform=val_transforms)\n",
        "\n",
        "  trainloaderC = torch.utils.data.DataLoader(train_dataC, shuffle = True, batch_size=8)\n",
        "  valloaderC = torch.utils.data.DataLoader(val_dataC, shuffle = True, batch_size=8)\n",
        "\n",
        "  modelCschema = vision_transformer_clspred.VitAttentionSchema().to(device)\n",
        "  modelCcontrol = vision_transformer_clspred.VitControl().to(device)\n",
        "  fitschema(modelCschema, trainloaderC, valloaderC, tname+\"C\")\n",
        "  fitcontrol(modelCcontrol, trainloaderC, valloaderC, tname+\"C\")\n",
        "\n",
        "  evaluate(modelCschema, valloaderC, tname+\"Cschema\", save_attn=False)\n",
        "  evaluate(modelCcontrol, valloaderC, tname+\"Ccontrol\", save_attn=False)\n",
        "\n",
        "  del(train_dataC)\n",
        "  del(val_dataC)\n",
        "  del(trainloaderC)\n",
        "  del(valloaderC)\n",
        "\n",
        "  # FREEZE THE MODELS\n",
        "  freeze_models([modelAschema, modelAcontrol, modelBschema, modelBcontrol, modelCschema, modelCcontrol])\n",
        "\n",
        "  modelAschema_wts = deepcopy(modelAschema.state_dict())\n",
        "  modelAcontrol_wts = deepcopy(modelAcontrol.state_dict())\n",
        "  modelBschema_wts = deepcopy(modelBschema.state_dict())\n",
        "  modelBcontrol_wts = deepcopy(modelBcontrol.state_dict())\n",
        "  modelCschema_wts = deepcopy(modelCschema.state_dict())\n",
        "  modelCcontrol_wts = deepcopy(modelCcontrol.state_dict())\n",
        "\n",
        "  # FIT / EVALUATE MODELS: CLS ATTENTION CLASSIFICATION\n",
        "  # SCHEMA\n",
        "  batch_size = 8\n",
        "\n",
        "  schemaAattn = torch.load(\"/content/drive/My Drive/networkattention/data/attentions/CLStrial0Aschemaattn.pt\")\n",
        "  schemaAattn = torch.nn.functional.interpolate(schemaAattn[:,:,0,:], size=(65536), mode=\"nearest\")\n",
        "  schemaAattn = schemaAattn.reshape(730,3,256,256)\n",
        "\n",
        "  false_schemaAattn = torch.clone(schemaAattn)\n",
        "  indices = torch.randperm(false_schemaAattn.shape[-1])\n",
        "  false_schemaAattn = false_schemaAattn[:,:,indices] # false attention values are shuffled along last dimension\n",
        "\n",
        "  dataset_schemaAattn = TensorDataset(torch.cat((schemaAattn, false_schemaAattn),0),\n",
        "                                      torch.cat((torch.ones(730,), torch.zeros(730,)),0))\n",
        "  dataset_schemaAattn = torch.utils.data.random_split(dataset_schemaAattn, [0.9, 0.1])\n",
        "  schemaAattntrain = DataLoader(dataset_schemaAattn[0], batch_size, shuffle=True)\n",
        "  schemaAattnval = DataLoader(dataset_schemaAattn[1], batch_size, shuffle=True)\n",
        "\n",
        "  fitcontrol(modelCcontrol, schemaAattntrain, schemaAattnval, tname+\"CAcontrol_schemaattn\", n_epochs=800)\n",
        "  evaluate(modelCcontrol, schemaAattnval, tname+\"CAcontrol_schemaattn\", save_attn=False)\n",
        "  fitschema(modelCschema, schemaAattntrain, schemaAattnval, tname+\"CAschema_schemaattn\", n_epochs=800, policy_only=True)\n",
        "  evaluate(modelCschema, schemaAattnval, tname+\"CAschema_schemaattn\", save_attn=False)\n",
        "\n",
        "  seedn += 1\n",
        "  torch.manual_seed(seeds[seedn])\n",
        "\n",
        "  del(schemaAattn)\n",
        "  del(false_schemaAattn)\n",
        "  del(indices)\n",
        "  del(dataset_schemaAattn)\n",
        "  del(schemaAattntrain)\n",
        "  del(schemaAattnval)\n",
        "\n",
        "  schemaBattn = torch.load(\"/content/drive/My Drive/networkattention/data/attentions/CLStrial0Bschemaattn.pt\")\n",
        "  schemaBattn = torch.nn.functional.interpolate(schemaBattn[:,:,0,:], size=(65536), mode=\"nearest\")\n",
        "  schemaBattn = schemaBattn.reshape(730,3,256,256)\n",
        "\n",
        "  false_schemaBattn = torch.clone(schemaBattn)\n",
        "  indices = torch.randperm(false_schemaBattn.shape[-1])\n",
        "  false_schemaBattn = false_schemaBattn[:,:,indices]\n",
        "\n",
        "  dataset_schemaBattn = TensorDataset(torch.cat((schemaBattn, false_schemaBattn),0),\n",
        "                                      torch.cat((torch.ones(730,), torch.zeros(730,)),0))\n",
        "  dataset_schemaBattn = torch.utils.data.random_split(dataset_schemaBattn, [0.9, 0.1])\n",
        "  schemaBattntrain = DataLoader(dataset_schemaBattn[0], batch_size, shuffle=True)\n",
        "  schemaBattnval = DataLoader(dataset_schemaBattn[1], batch_size, shuffle=True)\n",
        "\n",
        "  fitschema(modelAschema, schemaBattntrain, schemaBattnval, tname+\"ABschema_schemaattn\", n_epochs=800, policy_only=True)\n",
        "  fitcontrol(modelAcontrol, schemaBattntrain, schemaBattnval, tname+\"ABcontrol_schemaattn\", n_epochs=800)\n",
        "\n",
        "  evaluate(modelAschema, schemaBattnval, tname+\"ABschema_schemaattn\", save_attn=False)\n",
        "  evaluate(modelAcontrol, schemaBattnval, tname+\"ABcontrol_schemaattn\", save_attn=False)\n",
        "\n",
        "  seedn += 1\n",
        "  torch.manual_seed(seeds[seedn])\n",
        "\n",
        "  del(schemaBattn)\n",
        "  del(false_schemaBattn)\n",
        "  del(indices)\n",
        "  del(dataset_schemaBattn)\n",
        "  del(schemaBattntrain)\n",
        "  del(schemaBattnval)\n",
        "\n",
        "  schemaCattn = torch.load(\"/content/drive/My Drive/networkattention/data/attentions/CLStrial0Cschemaattn.pt\")\n",
        "  schemaCattn = torch.nn.functional.interpolate(schemaCattn[:,:,0,:], size=(65536), mode=\"nearest\")\n",
        "  schemaCattn = schemaCattn.reshape(730,3,256,256)\n",
        "\n",
        "  false_schemaCattn = torch.clone(schemaCattn)\n",
        "  indices = torch.randperm(false_schemaCattn.shape[-1])\n",
        "  false_schemaCattn = false_schemaCattn[:,:,indices]\n",
        "\n",
        "  dataset_schemaCattn = TensorDataset(torch.cat((schemaCattn, false_schemaCattn),0),\n",
        "                                      torch.cat((torch.ones(730,), torch.zeros(730,)),0))\n",
        "  dataset_schemaCattn = torch.utils.data.random_split(dataset_schemaCattn, [0.9, 0.1])\n",
        "  schemaCattntrain = DataLoader(dataset_schemaCattn[0], batch_size, shuffle=True)\n",
        "  schemaCattnval = DataLoader(dataset_schemaCattn[1], batch_size, shuffle=True)\n",
        "\n",
        "  fitschema(modelBschema, schemaCattntrain, schemaCattnval, \"BCschema_schemaattn\", n_epochs=800, policy_only=True)\n",
        "  fitcontrol(modelBcontrol, schemaCattntrain, schemaCattnval, \"BCcontrol_schemaattn\", n_epochs=800)\n",
        "\n",
        "  evaluate(modelBschema, schemaCattnval, tname+\"BCschema_schemaattn\", save_attn=False)\n",
        "  evaluate(modelBcontrol, schemaCattnval, tname+\"BCcontrol_schemaattn\", save_attn=False)\n",
        "\n",
        "  seedn += 1\n",
        "  torch.manual_seed(seeds[seedn])\n",
        "\n",
        "  del(schemaCattn)\n",
        "  del(false_schemaCattn)\n",
        "  del(indices)\n",
        "  del(dataset_schemaCattn)\n",
        "  del(schemaCattntrain)\n",
        "  del(schemaCattnval)\n",
        "\n",
        "  # CONTROL\n",
        "  del(modelAschema)\n",
        "  del(modelAcontrol)\n",
        "  del(modelBschema)\n",
        "  del(modelBcontrol)\n",
        "  del(modelCschema)\n",
        "  del(modelCcontrol)\n",
        "\n",
        "  modelCschema = vision_transformer_clspred.VitAttentionSchema().to(device)\n",
        "  modelCcontrol = vision_transformer_clspred.VitControl().to(device)\n",
        "  modelCschema.load_state_dict(modelCschema_wts)\n",
        "  modelCcontrol.load_state_dict(modelCcontrol_wts)\n",
        "  freeze_models([modelCschema, modelCcontrol])\n",
        "\n",
        "  controlAattn = torch.load(\"/content/drive/My Drive/networkattention/data/attentions/CLStrial0Acontrolattn.pt\")\n",
        "  controlAattn = torch.nn.functional.interpolate(controlAattn[:,:,0,:], size=(65536), mode=\"nearest\")\n",
        "  controlAattn = controlAattn.reshape(730,3,256,256)\n",
        "\n",
        "  false_controlAattn = torch.clone(controlAattn)\n",
        "  indices = torch.randperm(false_controlAattn.shape[-1])\n",
        "  false_controlAattn = false_controlAattn[:,:,indices] # false attention values are shuffled along last dimension\n",
        "\n",
        "  dataset_controlAattn = TensorDataset(torch.cat((controlAattn, false_controlAattn),0),\n",
        "                                      torch.cat((torch.ones(730,), torch.zeros(730,)),0))\n",
        "  dataset_controlAattn = torch.utils.data.random_split(dataset_controlAattn, [0.9, 0.1])\n",
        "  controlAattntrain = DataLoader(dataset_controlAattn[0], batch_size, shuffle=True)\n",
        "  controlAattnval = DataLoader(dataset_controlAattn[1], batch_size, shuffle=True)\n",
        "\n",
        "  fitschema(modelCschema, controlAattntrain, controlAattnval, tname+\"CAs_controlattn\", n_epochs=800, policy_only=True)\n",
        "  fitcontrol(modelCcontrol, controlAattntrain, controlAattnval, tname+\"CAc_controlattn\", n_epochs=800)\n",
        "\n",
        "  evaluate(modelCschema, controlAattnval, tname+\"CAschema_controlattn\", save_attn=False)\n",
        "  evaluate(modelCcontrol, controlAattnval, tname+\"CAcontrol_controlattn\", save_attn=False)\n",
        "\n",
        "  seedn += 1\n",
        "  torch.manual_seed(seeds[seedn])\n",
        "\n",
        "  del(controlAattn)\n",
        "  del(false_controlAattn)\n",
        "  del(indices)\n",
        "  del(dataset_controlAattn)\n",
        "  del(controlAattntrain)\n",
        "  del(controlAattnval)\n",
        "  del(modelCschema)\n",
        "  del(modelCcontrol)\n",
        "  del(modelCschema_wts)\n",
        "  del(modelCcontrol_wts)\n",
        "\n",
        "  modelAschema = vision_transformer_clspred.VitAttentionSchema().to(device)\n",
        "  modelAcontrol = vision_transformer_clspred.VitControl().to(device)\n",
        "  modelAschema.load_state_dict(modelAschema_wts)\n",
        "  modelAcontrol.load_state_dict(modelAcontrol_wts)\n",
        "  freeze_models([modelAschema, modelAcontrol])\n",
        "\n",
        "  controlBattn = torch.load(\"/content/drive/My Drive/networkattention/data/attentions/CLStrial0Bcontrolattn.pt\")\n",
        "  controlBattn = torch.nn.functional.interpolate(controlBattn[:,:,0,:], size=(65536), mode=\"nearest\")\n",
        "  controlBattn = controlBattn.reshape(730,3,256,256)\n",
        "\n",
        "  false_controlBattn = torch.clone(controlBattn)\n",
        "  indices = torch.randperm(false_controlBattn.shape[-1])\n",
        "  false_controlBattn = false_controlBattn[:,:,indices]\n",
        "\n",
        "  dataset_controlBattn = TensorDataset(torch.cat((controlBattn, false_controlBattn),0),\n",
        "                                      torch.cat((torch.ones(730,), torch.zeros(730,)),0))\n",
        "  dataset_controlBattn = torch.utils.data.random_split(dataset_controlBattn, [0.9, 0.1])\n",
        "  controlBattntrain = DataLoader(dataset_controlBattn[0], batch_size, shuffle=True)\n",
        "  controlBattnval = DataLoader(dataset_controlBattn[1], batch_size, shuffle=True)\n",
        "\n",
        "  fitschema(modelAschema, controlBattntrain, controlBattnval, tname+\"ABs_controlattn\", n_epochs=800, policy_only=True)\n",
        "  fitcontrol(modelAcontrol, controlBattntrain, controlBattnval, tname+\"ABc_controlattn\", n_epochs=800)\n",
        "\n",
        "  evaluate(modelAschema, controlBattnval, tname+\"ABschema_controlattn\", save_attn=False)\n",
        "  evaluate(modelAcontrol, controlBattnval, tname+\"ABcontrol_controlattn\", save_attn=False)\n",
        "\n",
        "  seedn += 1\n",
        "  torch.manual_seed(seeds[seedn])\n",
        "\n",
        "  del(controlBattn)\n",
        "  del(false_controlBattn)\n",
        "  del(indices)\n",
        "  del(dataset_controlBattn)\n",
        "  del(controlBattntrain)\n",
        "  del(controlBattnval)\n",
        "  del(modelAschema)\n",
        "  del(modelAcontrol)\n",
        "  del(modelAschema_wts)\n",
        "  del(modelAcontrol_wts)\n",
        "\n",
        "  modelBschema = vision_transformer_clspred.VitAttentionSchema().to(device)\n",
        "  modelBcontrol = vision_transformer_clspred.VitControl().to(device)\n",
        "  modelBschema.load_state_dict(modelBschema_wts)\n",
        "  modelBcontrol.load_state_dict(modelBcontrol_wts)\n",
        "  freeze_models([modelBschema, modelBcontrol])\n",
        "\n",
        "  controlCattn = torch.load(\"/content/drive/My Drive/networkattention/data/attentions/CLStrial0Ccontrolattn.pt\")\n",
        "  controlCattn = torch.nn.functional.interpolate(controlCattn[:,:,0,:], size=(65536), mode=\"nearest\")\n",
        "  controlCattn = controlCattn.reshape(730,3,256,256)\n",
        "\n",
        "  false_controlCattn = torch.clone(controlCattn)\n",
        "  indices = torch.randperm(false_controlCattn.shape[-1])\n",
        "  false_controlCattn = false_controlCattn[:,:,indices]\n",
        "\n",
        "  dataset_controlCattn = TensorDataset(torch.cat((controlCattn, false_controlCattn),0),\n",
        "                                      torch.cat((torch.ones(730,), torch.zeros(730,)),0))\n",
        "  dataset_controlCattn = torch.utils.data.random_split(dataset_controlCattn, [0.9, 0.1])\n",
        "  controlCattntrain = DataLoader(dataset_controlCattn[0], batch_size, shuffle=True)\n",
        "  controlCattnval = DataLoader(dataset_controlCattn[1], batch_size, shuffle=True)\n",
        "\n",
        "  fitschema(modelBschema, controlCattntrain, controlCattnval, tname+\"BCs_controlattn\", n_epochs=800, policy_only=True)\n",
        "  fitcontrol(modelBcontrol, controlCattntrain, controlCattnval, tname+\"BCc_controlattn\", n_epochs=800)\n",
        "\n",
        "  evaluate(modelBschema, controlCattnval, tname+\"BCschema_controlattn\", save_attn=False)\n",
        "  evaluate(modelBcontrol, controlCattnval, tname+\"BCcontrol_controlattn\", save_attn=False)\n",
        "\n",
        "  seedn += 1\n",
        "  torch.manual_seed(seeds[seedn])\n",
        "\n",
        "  del(controlCattn)\n",
        "  del(false_controlCattn)\n",
        "  del(indices)\n",
        "  del(dataset_controlCattn)\n",
        "  del(controlCattntrain)\n",
        "  del(controlCattnval)\n",
        "  del(modelBschema)\n",
        "  del(modelBcontrol)\n",
        "  del(modelBschema_wts)\n",
        "  del(modelBcontrol_wts)\n"
      ],
      "metadata": {
        "id": "wvGJaXMs5v25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a48d0950-6b18-4782-c409-5d845e24839e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "130: 0.9570909738540649 / 165: 2.6774544858571248\n",
            "131: 1.9722931385040283 / 165: 2.689407777605634\n",
            "132: 0.7331352829933167 / 165: 2.6938510217449876\n",
            "133: 3.8234598636627197 / 165: 2.717023505767186\n",
            "134: 3.7971339225769043 / 165: 2.7400364386312885\n",
            "135: 4.894780158996582 / 165: 2.7697017729282374\n",
            "136: 3.2752065658569336 / 165: 2.7895515096910066\n",
            "137: 2.9079184532165527 / 165: 2.807175257892319\n",
            "138: 0.884784460067749 / 165: 2.8125375879533356\n",
            "139: 0.880483865737915 / 165: 2.817873853806293\n",
            "140: 4.430624961853027 / 165: 2.8447261263023718\n",
            "141: 1.7367322444915771 / 165: 2.8552517762689873\n",
            "142: 3.6735332012176514 / 165: 2.8775156138521245\n",
            "143: 2.766575813293457 / 165: 2.894282739993297\n",
            "144: 2.3524439334869385 / 165: 2.9085399759538237\n",
            "145: 2.461306571960449 / 165: 2.9234569854808568\n",
            "146: 1.906010627746582 / 165: 2.935008565042957\n",
            "147: 2.284855604171753 / 165: 2.94885617476521\n",
            "148: 3.498631715774536 / 165: 2.9700600033456617\n",
            "149: 1.8709912300109863 / 165: 2.981399344133607\n",
            "150: 4.011641502380371 / 165: 3.005712322935912\n",
            "151: 0.9413421750068665 / 165: 3.011417427026863\n",
            "152: 2.317129611968994 / 165: 3.025460636796372\n",
            "153: 0.08469747006893158 / 165: 3.0259739547967897\n",
            "154: 4.436066150665283 / 165: 3.052859204194761\n",
            "155: 2.465641975402832 / 165: 3.067802488894172\n",
            "156: 3.8284590244293213 / 165: 3.0910052708604105\n",
            "157: 2.627607822418213 / 165: 3.1069301667538545\n",
            "158: 2.807528257369995 / 165: 3.1239454895257937\n",
            "159: 1.3424668312072754 / 165: 3.132081652139171\n",
            "160: 2.248941659927368 / 165: 3.1457116015932765\n",
            "161: 4.949295520782471 / 165: 3.1757073320222613\n",
            "162: 3.463055372238159 / 165: 3.196695546399462\n",
            "163: 3.6285295486450195 / 165: 3.2186866345730683\n",
            "164: 12.415180206298828 / 165: 3.2939301509748793\n",
            "\n",
            "Epoch : 1, train loss : 3.2939301509748793\n",
            "Epoch : 1, val loss : 2.6550569228925984\n",
            "\n",
            "Epoch : 2, train loss : 2.3239236376502292\n",
            "Epoch : 2, val loss : 1.8616731653088017\n",
            "\n",
            "Epoch : 3, train loss : 1.497374131824031\n",
            "Epoch : 3, val loss : 1.0193639876026857\n",
            "\n",
            "Epoch : 4, train loss : 0.9207908184239361\n",
            "Epoch : 4, val loss : 0.7312820694948496\n",
            "\n",
            "Epoch : 5, train loss : 0.7232300617478109\n",
            "Epoch : 5, val loss : 0.699383512923592\n",
            "\n",
            "Epoch : 6, train loss : 0.6988106810685364\n",
            "Epoch : 6, val loss : 0.7009783073475486\n",
            "\n",
            "Epoch : 7, train loss : 0.693839817697352\n",
            "Epoch : 7, val loss : 0.6944216866242258\n",
            "\n",
            "Epoch : 8, train loss : 0.6898820053447374\n",
            "Epoch : 8, val loss : 0.6980635398312619\n",
            "\n",
            "Epoch : 9, train loss : 0.6872023051435298\n",
            "Epoch : 9, val loss : 0.6908378538332488\n",
            "\n",
            "Epoch : 10, train loss : 0.6848938624064124\n",
            "Epoch : 10, val loss : 0.6938163826340124\n",
            "\n",
            "Epoch : 11, train loss : 0.6833690441015996\n",
            "Epoch : 11, val loss : 0.6892613203901993\n",
            "\n",
            "Epoch : 12, train loss : 0.6814612276626357\n",
            "Epoch : 12, val loss : 0.6858224586436622\n",
            "\n",
            "Epoch : 13, train loss : 0.6804806911584103\n",
            "Epoch : 13, val loss : 0.686989693265212\n",
            "\n",
            "Epoch : 14, train loss : 0.6798680442752257\n",
            "Epoch : 14, val loss : 0.6851943794049715\n",
            "\n",
            "Epoch : 15, train loss : 0.6787514115824842\n",
            "Epoch : 15, val loss : 0.6960855283235248\n",
            "\n",
            "Epoch : 16, train loss : 0.6773958596316247\n",
            "Epoch : 16, val loss : 0.690287957065984\n",
            "\n",
            "Epoch : 17, train loss : 0.6759455160661174\n",
            "Epoch : 17, val loss : 0.6838402403028389\n",
            "\n",
            "Epoch : 18, train loss : 0.6755252137328643\n",
            "Epoch : 18, val loss : 0.6859112099597328\n",
            "\n",
            "Epoch : 19, train loss : 0.6742034243814877\n",
            "Epoch : 19, val loss : 0.6856251609952825\n",
            "\n",
            "Epoch : 20, train loss : 0.6741981975960007\n",
            "Epoch : 20, val loss : 0.6841695057718379\n",
            "\n",
            "Epoch : 21, train loss : 0.6728441881411004\n",
            "Epoch : 21, val loss : 0.6829883017038044\n",
            "\n",
            "Epoch : 22, train loss : 0.6717029430649498\n",
            "Epoch : 22, val loss : 0.6828506243856329\n",
            "\n",
            "Epoch : 23, train loss : 0.6708253640117062\n",
            "Epoch : 23, val loss : 0.6835913313062567\n",
            "\n",
            "Epoch : 24, train loss : 0.6711008454814105\n",
            "Epoch : 24, val loss : 0.6881061290439807\n",
            "\n",
            "Epoch : 25, train loss : 0.669418476567124\n",
            "Epoch : 25, val loss : 0.6852165146877891\n",
            "\n",
            "Epoch : 26, train loss : 0.667692257418777\n",
            "Epoch : 26, val loss : 0.686571146312513\n",
            "\n",
            "Epoch : 27, train loss : 0.6677297664411139\n",
            "Epoch : 27, val loss : 0.6852673417643497\n",
            "\n",
            "Epoch : 28, train loss : 0.6667246251395257\n",
            "Epoch : 28, val loss : 0.6746685661767658\n",
            "\n",
            "Epoch : 29, train loss : 0.6668765949480465\n",
            "Epoch : 29, val loss : 0.6784260963138778\n",
            "\n",
            "Epoch : 30, train loss : 0.6655943805521185\n",
            "Epoch : 30, val loss : 0.6769368930866843\n",
            "\n",
            "Epoch : 31, train loss : 0.6646508690082668\n",
            "Epoch : 31, val loss : 0.6858317318715547\n",
            "\n",
            "Epoch : 32, train loss : 0.6635933019898155\n",
            "Epoch : 32, val loss : 0.6775220601182236\n",
            "\n",
            "Epoch : 33, train loss : 0.6638417728019483\n",
            "Epoch : 33, val loss : 0.6769150213191385\n",
            "\n",
            "Epoch : 34, train loss : 0.6629972746877953\n",
            "Epoch : 34, val loss : 0.6717049071663304\n",
            "\n",
            "Epoch : 35, train loss : 0.6615362893451346\n",
            "Epoch : 35, val loss : 0.6761453716378463\n",
            "\n",
            "Epoch : 36, train loss : 0.6610478563742201\n",
            "Epoch : 36, val loss : 0.6717733364356191\n",
            "\n",
            "Epoch : 37, train loss : 0.6612643895727217\n",
            "Epoch : 37, val loss : 0.6711323010294061\n",
            "\n",
            "Epoch : 38, train loss : 0.6611785368485883\n",
            "Epoch : 38, val loss : 0.6729954512495744\n",
            "\n",
            "Epoch : 39, train loss : 0.6598444570194592\n",
            "Epoch : 39, val loss : 0.6744755224177711\n",
            "\n",
            "Epoch : 40, train loss : 0.6591971946485117\n",
            "Epoch : 40, val loss : 0.6694133250336899\n",
            "\n",
            "Epoch : 41, train loss : 0.6592747148239251\n",
            "Epoch : 41, val loss : 0.6745431329074657\n",
            "\n",
            "Epoch : 42, train loss : 0.6595717282006237\n",
            "Epoch : 42, val loss : 0.6705365682903088\n",
            "\n",
            "Epoch : 43, train loss : 0.6579338658939708\n",
            "Epoch : 43, val loss : 0.6672387358389403\n",
            "\n",
            "Epoch : 44, train loss : 0.6559613607146523\n",
            "Epoch : 44, val loss : 0.6733662486076355\n",
            "\n",
            "Epoch : 45, train loss : 0.654726888194229\n",
            "Epoch : 45, val loss : 0.6771566020814995\n",
            "\n",
            "Epoch : 46, train loss : 0.6558591062372381\n",
            "Epoch : 46, val loss : 0.6722207100767837\n",
            "\n",
            "Epoch : 47, train loss : 0.6542819687814424\n",
            "Epoch : 47, val loss : 0.6674392725292004\n",
            "\n",
            "Epoch : 48, train loss : 0.6540428241093954\n",
            "Epoch : 48, val loss : 0.6746532415088853\n",
            "\n",
            "Epoch : 49, train loss : 0.6544434489625874\n",
            "Epoch : 49, val loss : 0.6720355498163324\n",
            "\n",
            "Epoch : 50, train loss : 0.6529818162773594\n",
            "Epoch : 50, val loss : 0.67563333636836\n",
            "\n",
            "Epoch : 51, train loss : 0.6521308754429674\n",
            "Epoch : 51, val loss : 0.6753454804420472\n",
            "\n",
            "Epoch : 52, train loss : 0.652679548480294\n",
            "Epoch : 52, val loss : 0.6614476693303961\n",
            "\n",
            "Epoch : 53, train loss : 0.6511465376073663\n",
            "Epoch : 53, val loss : 0.6689602324837133\n",
            "\n",
            "Epoch : 54, train loss : 0.650335641702016\n",
            "Epoch : 54, val loss : 0.6692082630960564\n",
            "\n",
            "Epoch : 55, train loss : 0.6500879191991057\n",
            "Epoch : 55, val loss : 0.668909041505111\n",
            "\n",
            "Epoch : 56, train loss : 0.650068587245363\n",
            "Epoch : 56, val loss : 0.6746279534540678\n",
            "\n",
            "Epoch : 57, train loss : 0.6496848783709783\n",
            "Epoch : 57, val loss : 0.6705346891754552\n",
            "\n",
            "Epoch : 58, train loss : 0.6489242976362055\n",
            "Epoch : 58, val loss : 0.6648843382534227\n",
            "\n",
            "Epoch : 59, train loss : 0.6479703769539341\n",
            "Epoch : 59, val loss : 0.6649148872024135\n",
            "\n",
            "Epoch : 60, train loss : 0.6473400401346606\n",
            "Epoch : 60, val loss : 0.664798689515967\n",
            "\n",
            "Epoch : 61, train loss : 0.6470074758385168\n",
            "Epoch : 61, val loss : 0.6607167312973424\n",
            "\n",
            "Epoch : 62, train loss : 0.6453696724140287\n",
            "Epoch : 62, val loss : 0.6761413404816076\n",
            "\n",
            "Epoch : 63, train loss : 0.6452032883961997\n",
            "Epoch : 63, val loss : 0.6626899744334972\n",
            "\n",
            "Epoch : 64, train loss : 0.6452240167242106\n",
            "Epoch : 64, val loss : 0.6698172155179475\n",
            "\n",
            "Epoch : 65, train loss : 0.6452036178473268\n",
            "Epoch : 65, val loss : 0.667939117080287\n",
            "\n",
            "Epoch : 66, train loss : 0.6437469915910204\n",
            "Epoch : 66, val loss : 0.6605927065799112\n",
            "\n",
            "Epoch : 67, train loss : 0.6446981368642868\n",
            "Epoch : 67, val loss : 0.6696837983633342\n",
            "\n",
            "Epoch : 68, train loss : 0.6436737537384033\n",
            "Epoch : 68, val loss : 0.6821678218088654\n",
            "\n",
            "Epoch : 69, train loss : 0.6434199685400185\n",
            "Epoch : 69, val loss : 0.65819104407963\n",
            "\n",
            "Epoch : 70, train loss : 0.6425760072289092\n",
            "Epoch : 70, val loss : 0.6723057502194453\n",
            "\n",
            "Epoch : 71, train loss : 0.642689428546212\n",
            "Epoch : 71, val loss : 0.6659576234064606\n",
            "\n",
            "Epoch : 72, train loss : 0.6418366251569804\n",
            "Epoch : 72, val loss : 0.6664063993253205\n",
            "\n",
            "Epoch : 73, train loss : 0.6410031219323477\n",
            "Epoch : 73, val loss : 0.6655331975535341\n",
            "\n",
            "Epoch : 74, train loss : 0.6410454240712252\n",
            "Epoch : 74, val loss : 0.6754554792454368\n",
            "\n",
            "Epoch : 75, train loss : 0.6402215087052543\n",
            "Epoch : 75, val loss : 0.665957648503153\n",
            "\n",
            "Epoch : 76, train loss : 0.639609674251441\n",
            "Epoch : 76, val loss : 0.6656710536856401\n",
            "\n",
            "Epoch : 77, train loss : 0.6400478942827742\n",
            "Epoch : 77, val loss : 0.6576060182169863\n",
            "\n",
            "Epoch : 78, train loss : 0.6406124367858425\n",
            "Epoch : 78, val loss : 0.6613596301329764\n",
            "\n",
            "Epoch : 79, train loss : 0.6377583279754175\n",
            "Epoch : 79, val loss : 0.6713439916309557\n",
            "\n",
            "Epoch : 80, train loss : 0.637244515707999\n",
            "Epoch : 80, val loss : 0.6822949585161712\n",
            "\n",
            "Epoch : 81, train loss : 0.6376642711234817\n",
            "Epoch : 81, val loss : 0.6549308660783265\n",
            "\n",
            "Epoch : 82, train loss : 0.6386688991026443\n",
            "Epoch : 82, val loss : 0.6648160501530296\n",
            "\n",
            "Epoch : 83, train loss : 0.6362731557903865\n",
            "Epoch : 83, val loss : 0.6684504182715165\n",
            "\n",
            "Epoch : 84, train loss : 0.6361131964307843\n",
            "Epoch : 84, val loss : 0.6720438756440814\n",
            "\n",
            "Epoch : 85, train loss : 0.6358822128989475\n",
            "Epoch : 85, val loss : 0.6800202758688676\n",
            "\n",
            "Epoch : 86, train loss : 0.6352800734115367\n",
            "Epoch : 86, val loss : 0.6584800670021458\n",
            "\n",
            "Epoch : 87, train loss : 0.635739767551422\n",
            "Epoch : 87, val loss : 0.6631048541319999\n",
            "\n",
            "Epoch : 88, train loss : 0.6357684720646254\n",
            "Epoch : 88, val loss : 0.6618080264643619\n",
            "\n",
            "Epoch : 89, train loss : 0.6340903894467784\n",
            "Epoch : 89, val loss : 0.6758638996826974\n",
            "\n",
            "Epoch : 90, train loss : 0.6343650657119175\n",
            "Epoch : 90, val loss : 0.6696681348901046\n",
            "\n",
            "Epoch : 91, train loss : 0.6333747047366514\n",
            "Epoch : 91, val loss : 0.6687167192760265\n",
            "\n",
            "Epoch : 92, train loss : 0.6339838669155584\n",
            "Epoch : 92, val loss : 0.6604875169302289\n",
            "\n",
            "Epoch : 93, train loss : 0.6320988535881047\n",
            "Epoch : 93, val loss : 0.66147423104236\n",
            "\n",
            "Epoch : 94, train loss : 0.6333351426052325\n",
            "Epoch : 94, val loss : 0.6703209563305504\n",
            "\n",
            "Epoch : 95, train loss : 0.6306644542650742\n",
            "Epoch : 95, val loss : 0.660218496071665\n",
            "\n",
            "Epoch : 96, train loss : 0.6317609236095888\n",
            "Epoch : 96, val loss : 0.6629233360290527\n",
            "\n",
            "Epoch : 97, train loss : 0.6312268190311663\n",
            "Epoch : 97, val loss : 0.6674781755397193\n",
            "\n",
            "Epoch : 98, train loss : 0.631871677348108\n",
            "Epoch : 98, val loss : 0.6651231426941722\n",
            "\n",
            "Epoch : 99, train loss : 0.6306804866501778\n",
            "Epoch : 99, val loss : 0.6566219486688312\n",
            "\n",
            "Epoch : 100, train loss : 0.6310052649541337\n",
            "Epoch : 100, val loss : 0.6539780936743084\n",
            "\n",
            "Epoch : 101, train loss : 0.6302794790629185\n",
            "Epoch : 101, val loss : 0.6670169328388413\n",
            "\n",
            "Epoch : 102, train loss : 0.630507291627653\n",
            "Epoch : 102, val loss : 0.6676944431505705\n",
            "\n",
            "Epoch : 103, train loss : 0.6290578137744557\n",
            "Epoch : 103, val loss : 0.6617361118918972\n",
            "\n",
            "Epoch : 104, train loss : 0.6292460427139744\n",
            "Epoch : 104, val loss : 0.6565168939138714\n",
            "\n",
            "Epoch : 105, train loss : 0.6289821911941875\n",
            "Epoch : 105, val loss : 0.6742926334079943\n",
            "\n",
            "Epoch : 106, train loss : 0.6284315546353656\n",
            "Epoch : 106, val loss : 0.6654264330863953\n",
            "\n",
            "Epoch : 107, train loss : 0.628100606889436\n",
            "Epoch : 107, val loss : 0.6782211880934865\n",
            "\n",
            "Epoch : 108, train loss : 0.6269905027114984\n",
            "Epoch : 108, val loss : 0.6499606465038499\n",
            "\n",
            "Epoch : 109, train loss : 0.6276311139265699\n",
            "Epoch : 109, val loss : 0.6710573497571444\n",
            "\n",
            "Epoch : 110, train loss : 0.6257967650890351\n",
            "Epoch : 110, val loss : 0.6563585714290017\n",
            "\n",
            "Epoch : 111, train loss : 0.6270279541160119\n",
            "Epoch : 111, val loss : 0.658644829925738\n",
            "\n",
            "Epoch : 112, train loss : 0.6251068758242057\n",
            "Epoch : 112, val loss : 0.6694291610466806\n",
            "\n",
            "Epoch : 113, train loss : 0.625870828195052\n",
            "Epoch : 113, val loss : 0.6653775729631122\n",
            "\n",
            "Epoch : 114, train loss : 0.623498652198098\n",
            "Epoch : 114, val loss : 0.6586800405853674\n",
            "\n",
            "Epoch : 115, train loss : 0.6244632151993834\n",
            "Epoch : 115, val loss : 0.65179076006538\n",
            "\n",
            "Epoch : 116, train loss : 0.6238397242444931\n",
            "Epoch : 116, val loss : 0.6595592404666701\n",
            "\n",
            "Epoch : 117, train loss : 0.6234112658283927\n",
            "Epoch : 117, val loss : 0.6623414880351015\n",
            "\n",
            "Epoch : 118, train loss : 0.6255930362325721\n",
            "Epoch : 118, val loss : 0.6530779443289104\n",
            "\n",
            "Epoch : 119, train loss : 0.6249787417325106\n",
            "Epoch : 119, val loss : 0.6548850410862973\n",
            "\n",
            "Epoch : 120, train loss : 0.6235966211015525\n",
            "Epoch : 120, val loss : 0.6636496280369005\n",
            "\n",
            "Epoch : 121, train loss : 0.6241782264275981\n",
            "Epoch : 121, val loss : 0.6623803502634954\n",
            "\n",
            "Epoch : 122, train loss : 0.6232230861981708\n",
            "Epoch : 122, val loss : 0.6517188282389389\n",
            "\n",
            "Epoch : 123, train loss : 0.6231285761703145\n",
            "Epoch : 123, val loss : 0.657846273560273\n",
            "\n",
            "Epoch : 124, train loss : 0.6230176105643768\n",
            "Epoch : 124, val loss : 0.6632421706852161\n",
            "\n",
            "Epoch : 125, train loss : 0.6217677587812599\n",
            "Epoch : 125, val loss : 0.6729037259754382\n",
            "\n",
            "Epoch : 126, train loss : 0.6223681325262245\n",
            "Epoch : 126, val loss : 0.6626550931679576\n",
            "\n",
            "Epoch : 127, train loss : 0.6216075774395108\n",
            "Epoch : 127, val loss : 0.6472803746399126\n",
            "\n",
            "Epoch : 128, train loss : 0.6217866697094658\n",
            "Epoch : 128, val loss : 0.6597822716361598\n",
            "\n",
            "Epoch : 129, train loss : 0.6197919990077164\n",
            "Epoch : 129, val loss : 0.6515410683656994\n",
            "\n",
            "Epoch : 130, train loss : 0.6185717458074742\n",
            "Epoch : 130, val loss : 0.6653757879608555\n",
            "\n",
            "Epoch : 131, train loss : 0.6204552012862584\n",
            "Epoch : 131, val loss : 0.6575680955460196\n",
            "\n",
            "Epoch : 132, train loss : 0.6192805530446949\n",
            "Epoch : 132, val loss : 0.6654200585264908\n",
            "\n",
            "Epoch : 133, train loss : 0.619769023584597\n",
            "Epoch : 133, val loss : 0.6606317021344837\n",
            "\n",
            "Epoch : 134, train loss : 0.6190078292832231\n",
            "Epoch : 134, val loss : 0.653740224085356\n",
            "\n",
            "Epoch : 135, train loss : 0.6185501445423472\n",
            "Epoch : 135, val loss : 0.6561869696566931\n",
            "\n",
            "Epoch : 136, train loss : 0.6192738538438625\n",
            "Epoch : 136, val loss : 0.6468756481220846\n",
            "\n",
            "Epoch : 137, train loss : 0.6184885913675482\n",
            "Epoch : 137, val loss : 0.6510359456664638\n",
            "\n",
            "Epoch : 138, train loss : 0.6189035345207563\n",
            "Epoch : 138, val loss : 0.6687475286032024\n",
            "\n",
            "Epoch : 139, train loss : 0.6169180857412745\n",
            "Epoch : 139, val loss : 0.6523173576907108\n",
            "\n",
            "Epoch : 140, train loss : 0.6162744563637353\n",
            "Epoch : 140, val loss : 0.6598888416039317\n",
            "\n",
            "Epoch : 141, train loss : 0.616868048906326\n",
            "Epoch : 141, val loss : 0.6603762162359136\n",
            "\n",
            "Epoch : 142, train loss : 0.6159052733219033\n",
            "Epoch : 142, val loss : 0.6619397558664021\n",
            "\n",
            "Epoch : 143, train loss : 0.6161424346042401\n",
            "Epoch : 143, val loss : 0.6599855203377573\n",
            "\n",
            "Epoch : 144, train loss : 0.6161418092973305\n",
            "Epoch : 144, val loss : 0.6475810314479625\n",
            "\n",
            "Epoch : 145, train loss : 0.6151277413873959\n",
            "Epoch : 145, val loss : 0.6532384910081562\n",
            "\n",
            "Epoch : 146, train loss : 0.6157265888922142\n",
            "Epoch : 146, val loss : 0.6491225346138603\n",
            "\n",
            "Epoch : 147, train loss : 0.6159188615553307\n",
            "Epoch : 147, val loss : 0.6478498154564907\n",
            "\n",
            "Epoch : 148, train loss : 0.6160733656449751\n",
            "Epoch : 148, val loss : 0.666859623632933\n",
            "\n",
            "Epoch : 149, train loss : 0.6157080751476866\n",
            "Epoch : 149, val loss : 0.6547264613603291\n",
            "\n",
            "Epoch : 150, train loss : 0.6151394824186959\n",
            "Epoch : 150, val loss : 0.6595660479445206\n",
            "\n",
            "Epoch : 151, train loss : 0.6137559574661832\n",
            "Epoch : 151, val loss : 0.6614579087809511\n",
            "\n",
            "Epoch : 152, train loss : 0.614838486368006\n",
            "Epoch : 152, val loss : 0.6577606640363994\n",
            "\n",
            "Epoch : 153, train loss : 0.6133950265971096\n",
            "Epoch : 153, val loss : 0.6555774698131962\n",
            "\n",
            "Epoch : 154, train loss : 0.6132558829856647\n",
            "Epoch : 154, val loss : 0.647893908776735\n",
            "\n",
            "Epoch : 155, train loss : 0.6149526751402653\n",
            "Epoch : 155, val loss : 0.6528716840242084\n",
            "\n",
            "Epoch : 156, train loss : 0.6124426988038149\n",
            "Epoch : 156, val loss : 0.6472370608856804\n",
            "\n",
            "Epoch : 157, train loss : 0.6143327649795648\n",
            "Epoch : 157, val loss : 0.6446628570556641\n",
            "\n",
            "Epoch : 158, train loss : 0.6130209718689773\n",
            "Epoch : 158, val loss : 0.6676557440506784\n",
            "\n",
            "Epoch : 159, train loss : 0.6112323704994087\n",
            "Epoch : 159, val loss : 0.6582141267625908\n",
            "\n",
            "Epoch : 160, train loss : 0.612984198512453\n",
            "Epoch : 160, val loss : 0.6502337361636914\n",
            "\n",
            "Epoch : 161, train loss : 0.6114650905132293\n",
            "Epoch : 161, val loss : 0.6518826610163638\n",
            "\n",
            "Epoch : 162, train loss : 0.6117483111945066\n",
            "Epoch : 162, val loss : 0.6600363913335298\n",
            "\n",
            "Epoch : 163, train loss : 0.6096505080208634\n",
            "Epoch : 163, val loss : 0.6496357117828568\n",
            "\n",
            "Epoch : 164, train loss : 0.6112277388572691\n",
            "Epoch : 164, val loss : 0.6585913488739414\n",
            "\n",
            "Epoch : 165, train loss : 0.6104837630734296\n",
            "Epoch : 165, val loss : 0.6687639339974052\n",
            "\n",
            "Epoch : 166, train loss : 0.611343909032417\n",
            "Epoch : 166, val loss : 0.6490127977571989\n",
            "\n",
            "Epoch : 167, train loss : 0.6099235420877284\n",
            "Epoch : 167, val loss : 0.6656702850994313\n",
            "\n",
            "Epoch : 168, train loss : 0.6105378559141452\n",
            "Epoch : 168, val loss : 0.6487289886725577\n",
            "\n",
            "Epoch : 169, train loss : 0.6104067896351667\n",
            "Epoch : 169, val loss : 0.6506758912613517\n",
            "\n",
            "Epoch : 170, train loss : 0.6098039451873664\n",
            "Epoch : 170, val loss : 0.658695034290615\n",
            "\n",
            "Epoch : 171, train loss : 0.609792308373885\n",
            "Epoch : 171, val loss : 0.6630678020025554\n",
            "\n",
            "Epoch : 172, train loss : 0.6091677806594155\n",
            "Epoch : 172, val loss : 0.6453439797225752\n",
            "\n",
            "Epoch : 173, train loss : 0.6095242946436911\n",
            "Epoch : 173, val loss : 0.6472791935268201\n",
            "\n",
            "Epoch : 174, train loss : 0.6102166085532214\n",
            "Epoch : 174, val loss : 0.6622518551977056\n",
            "\n",
            "Epoch : 175, train loss : 0.6071207292152175\n",
            "Epoch : 175, val loss : 0.657955063016791\n",
            "\n",
            "Epoch : 176, train loss : 0.6066237655552948\n",
            "Epoch : 176, val loss : 0.6606149516607585\n",
            "\n",
            "Epoch : 177, train loss : 0.6103798682039431\n",
            "Epoch : 177, val loss : 0.6465882753071033\n",
            "\n",
            "Epoch : 178, train loss : 0.607242258209171\n",
            "Epoch : 178, val loss : 0.6507638379147178\n",
            "\n",
            "Epoch : 179, train loss : 0.6069056111754794\n",
            "Epoch : 179, val loss : 0.6528269491697611\n",
            "\n",
            "Epoch : 180, train loss : 0.6065227349599205\n",
            "Epoch : 180, val loss : 0.642207754285712\n",
            "\n",
            "Epoch : 181, train loss : 0.6068007480014453\n",
            "Epoch : 181, val loss : 0.6586247870796604\n",
            "\n",
            "Epoch : 182, train loss : 0.6054976259217117\n",
            "Epoch : 182, val loss : 0.6614461682344738\n",
            "\n",
            "Epoch : 183, train loss : 0.607215687903491\n",
            "Epoch : 183, val loss : 0.6582445659135517\n",
            "\n",
            "Epoch : 184, train loss : 0.6086095195828066\n",
            "Epoch : 184, val loss : 0.6490005427285245\n",
            "\n",
            "Epoch : 185, train loss : 0.6064479425097958\n",
            "Epoch : 185, val loss : 0.650786280632019\n",
            "\n",
            "Epoch : 186, train loss : 0.6056888237144007\n",
            "Epoch : 186, val loss : 0.6556332707405089\n",
            "\n",
            "Epoch : 187, train loss : 0.6056255562738938\n",
            "Epoch : 187, val loss : 0.656677710382562\n",
            "\n",
            "Epoch : 188, train loss : 0.6053148065552569\n",
            "Epoch : 188, val loss : 0.6428066994014539\n",
            "\n",
            "Epoch : 189, train loss : 0.6076184373913389\n",
            "Epoch : 189, val loss : 0.6469615729231584\n",
            "\n",
            "Epoch : 190, train loss : 0.6048718027996294\n",
            "Epoch : 190, val loss : 0.6556256727168435\n",
            "\n",
            "Epoch : 191, train loss : 0.6042879744009538\n",
            "Epoch : 191, val loss : 0.6574660570997942\n",
            "\n",
            "Epoch : 192, train loss : 0.6039050895156283\n",
            "Epoch : 192, val loss : 0.669966526721653\n",
            "\n",
            "Epoch : 193, train loss : 0.6044867716052316\n",
            "Epoch : 193, val loss : 0.6576207515440489\n",
            "\n",
            "Epoch : 194, train loss : 0.6049095666769778\n",
            "Epoch : 194, val loss : 0.663693296281915\n",
            "\n",
            "Epoch : 195, train loss : 0.603385861534061\n",
            "Epoch : 195, val loss : 0.6425755463148419\n",
            "\n",
            "Epoch : 196, train loss : 0.6053798845320036\n",
            "Epoch : 196, val loss : 0.6559550479838724\n",
            "\n",
            "Epoch : 197, train loss : 0.6032196758371413\n",
            "Epoch : 197, val loss : 0.6465114198232952\n",
            "\n",
            "Epoch : 198, train loss : 0.604042349981539\n",
            "Epoch : 198, val loss : 0.647435702775654\n",
            "\n",
            "Epoch : 199, train loss : 0.6043408133766869\n",
            "Epoch : 199, val loss : 0.6476588970736453\n",
            "\n",
            "Epoch : 200, train loss : 0.6021503948804107\n",
            "Epoch : 200, val loss : 0.6320881043609821\n",
            "\n",
            "Epoch : 201, train loss : 0.6034517985401732\n",
            "Epoch : 201, val loss : 0.6357396841049193\n",
            "\n",
            "Epoch : 202, train loss : 0.6037465771039329\n",
            "Epoch : 202, val loss : 0.6453851461410524\n",
            "\n",
            "Epoch : 203, train loss : 0.6042560208927499\n",
            "Epoch : 203, val loss : 0.6590066834499961\n",
            "\n",
            "Epoch : 204, train loss : 0.6030093933596758\n",
            "Epoch : 204, val loss : 0.6517774274474696\n",
            "\n",
            "Epoch : 205, train loss : 0.6021960579987731\n",
            "Epoch : 205, val loss : 0.63997178956082\n",
            "\n",
            "Epoch : 206, train loss : 0.600365143472498\n",
            "Epoch : 206, val loss : 0.6457485776198539\n",
            "\n",
            "Epoch : 207, train loss : 0.6036426417755358\n",
            "Epoch : 207, val loss : 0.6446293310115212\n",
            "\n",
            "Epoch : 208, train loss : 0.6032506530935114\n",
            "Epoch : 208, val loss : 0.6469541913584658\n",
            "\n",
            "Epoch : 209, train loss : 0.6017242597811151\n",
            "Epoch : 209, val loss : 0.6401586877672295\n",
            "\n",
            "Epoch : 210, train loss : 0.5993928713328909\n",
            "Epoch : 210, val loss : 0.6526062519926773\n",
            "\n",
            "Epoch : 211, train loss : 0.6013460282123452\n",
            "Epoch : 211, val loss : 0.6524395801519093\n",
            "\n",
            "Epoch : 212, train loss : 0.5998697591550423\n",
            "Epoch : 212, val loss : 0.648586691994416\n",
            "\n",
            "Epoch : 213, train loss : 0.5986786997679509\n",
            "Epoch : 213, val loss : 0.6478766861714816\n",
            "\n",
            "Epoch : 214, train loss : 0.5990626335144044\n",
            "Epoch : 214, val loss : 0.6395202953564493\n",
            "\n",
            "Epoch : 215, train loss : 0.6002116503137527\n",
            "Epoch : 215, val loss : 0.6333752729390798\n",
            "\n",
            "Epoch : 216, train loss : 0.5991508727723902\n",
            "Epoch : 216, val loss : 0.6534784969530607\n",
            "\n",
            "Epoch : 217, train loss : 0.6005642596519352\n",
            "Epoch : 217, val loss : 0.6562248531140779\n",
            "\n",
            "Epoch : 218, train loss : 0.5994238443446883\n",
            "Epoch : 218, val loss : 0.6498099941956369\n",
            "\n",
            "Epoch : 219, train loss : 0.5987098905173213\n",
            "Epoch : 219, val loss : 0.6408182006133231\n",
            "\n",
            "Epoch : 220, train loss : 0.6001366774241129\n",
            "Epoch : 220, val loss : 0.6354623609467556\n",
            "\n",
            "Epoch : 221, train loss : 0.5986996981230648\n",
            "Epoch : 221, val loss : 0.6429090092056677\n",
            "\n",
            "Epoch : 222, train loss : 0.5988940157673572\n",
            "Epoch : 222, val loss : 0.6468147102155183\n",
            "\n",
            "Epoch : 223, train loss : 0.5981632277821051\n",
            "Epoch : 223, val loss : 0.6476527013276753\n",
            "\n",
            "Epoch : 224, train loss : 0.5975633258169349\n",
            "Epoch : 224, val loss : 0.6480407965810675\n",
            "\n",
            "Epoch : 225, train loss : 0.5989593469735349\n",
            "Epoch : 225, val loss : 0.6531423032283784\n",
            "\n",
            "Epoch : 226, train loss : 0.5976824883258703\n",
            "Epoch : 226, val loss : 0.6439617056595651\n",
            "\n",
            "Epoch : 227, train loss : 0.5991358390360169\n",
            "Epoch : 227, val loss : 0.6378902573334544\n",
            "\n",
            "Epoch : 228, train loss : 0.598521773742907\n",
            "Epoch : 228, val loss : 0.6427016571948403\n",
            "\n",
            "Epoch : 229, train loss : 0.596387276143739\n",
            "Epoch : 229, val loss : 0.6591188656656365\n",
            "\n",
            "Epoch : 230, train loss : 0.5976649990587523\n",
            "Epoch : 230, val loss : 0.6555136787263971\n",
            "\n",
            "Epoch : 231, train loss : 0.5970947516686987\n",
            "Epoch : 231, val loss : 0.64685615112907\n",
            "\n",
            "Epoch : 232, train loss : 0.5968914080749864\n",
            "Epoch : 232, val loss : 0.6396750556795221\n",
            "\n",
            "Epoch : 233, train loss : 0.5989973433089981\n",
            "Epoch : 233, val loss : 0.6567025466969137\n",
            "\n",
            "Epoch : 234, train loss : 0.5963698000618911\n",
            "Epoch : 234, val loss : 0.6509433388710022\n",
            "\n",
            "Epoch : 235, train loss : 0.5954279265620491\n",
            "Epoch : 235, val loss : 0.6371056939426222\n",
            "\n",
            "Epoch : 236, train loss : 0.5965775773380741\n",
            "Epoch : 236, val loss : 0.6354827378925526\n",
            "\n",
            "Epoch : 237, train loss : 0.5968496432810118\n",
            "Epoch : 237, val loss : 0.6360916401210583\n",
            "\n",
            "Epoch : 238, train loss : 0.5958089022925406\n",
            "Epoch : 238, val loss : 0.6408999107385936\n",
            "\n",
            "Epoch : 239, train loss : 0.5955744573564242\n",
            "Epoch : 239, val loss : 0.6511643623050888\n",
            "\n",
            "Epoch : 240, train loss : 0.5934837742285296\n",
            "Epoch : 240, val loss : 0.6408777205567611\n",
            "\n",
            "Epoch : 241, train loss : 0.5946228894320402\n",
            "Epoch : 241, val loss : 0.6433595544413516\n",
            "\n",
            "Epoch : 242, train loss : 0.5952181761915034\n",
            "Epoch : 242, val loss : 0.6384595428642476\n",
            "\n",
            "Epoch : 243, train loss : 0.5950279788537457\n",
            "Epoch : 243, val loss : 0.6509458140323037\n",
            "\n",
            "Epoch : 244, train loss : 0.5944978175741255\n",
            "Epoch : 244, val loss : 0.6425443887710571\n",
            "\n",
            "Epoch : 245, train loss : 0.5940824378620495\n",
            "Epoch : 245, val loss : 0.6489713725290801\n",
            "\n",
            "Epoch : 246, train loss : 0.5941917383309568\n",
            "Epoch : 246, val loss : 0.6480105111473484\n",
            "\n",
            "Epoch : 247, train loss : 0.5934930530461399\n",
            "Epoch : 247, val loss : 0.6393219279615503\n",
            "\n",
            "Epoch : 248, train loss : 0.5930221974849701\n",
            "Epoch : 248, val loss : 0.6537279706252248\n",
            "\n",
            "Epoch : 249, train loss : 0.593503613544233\n",
            "Epoch : 249, val loss : 0.6345790938327187\n",
            "\n",
            "Epoch : 250, train loss : 0.5931792477766674\n",
            "Epoch : 250, val loss : 0.6535218044331199\n",
            "\n",
            "Epoch : 251, train loss : 0.5929075441577215\n",
            "Epoch : 251, val loss : 0.6528871529980709\n",
            "\n",
            "Epoch : 252, train loss : 0.5934535772511452\n",
            "Epoch : 252, val loss : 0.6525054511271025\n",
            "\n",
            "Epoch : 253, train loss : 0.5918087552894246\n",
            "Epoch : 253, val loss : 0.6460032651298924\n",
            "\n",
            "Epoch : 254, train loss : 0.5928252355618909\n",
            "Epoch : 254, val loss : 0.6402656875158611\n",
            "\n",
            "Epoch : 255, train loss : 0.5925138242316973\n",
            "Epoch : 255, val loss : 0.6383395759682905\n",
            "\n",
            "Epoch : 256, train loss : 0.5929722177259844\n",
            "Epoch : 256, val loss : 0.6342169830673621\n",
            "\n",
            "Epoch : 257, train loss : 0.5915582759813828\n",
            "Epoch : 257, val loss : 0.6424006851095901\n",
            "\n",
            "Epoch : 258, train loss : 0.5926240043206646\n",
            "Epoch : 258, val loss : 0.6426386033233844\n",
            "\n",
            "Epoch : 259, train loss : 0.593275814164768\n",
            "Epoch : 259, val loss : 0.6294750320283989\n",
            "\n",
            "Epoch : 260, train loss : 0.592562595280734\n",
            "Epoch : 260, val loss : 0.6375642638457449\n",
            "\n",
            "Epoch : 261, train loss : 0.5915903183546936\n",
            "Epoch : 261, val loss : 0.6448279195710231\n",
            "\n",
            "Epoch : 262, train loss : 0.5923588882793077\n",
            "Epoch : 262, val loss : 0.6340014291437047\n",
            "\n",
            "Epoch : 263, train loss : 0.5912663089506554\n",
            "Epoch : 263, val loss : 0.6409030870387428\n",
            "\n",
            "Epoch : 264, train loss : 0.5890582263469694\n",
            "Epoch : 264, val loss : 0.6409973298248492\n",
            "\n",
            "Epoch : 265, train loss : 0.5912087680715501\n",
            "Epoch : 265, val loss : 0.6338727897719334\n",
            "\n",
            "Epoch : 266, train loss : 0.5913145327206817\n",
            "Epoch : 266, val loss : 0.6321844897772138\n",
            "\n",
            "Epoch : 267, train loss : 0.5894566861065953\n",
            "Epoch : 267, val loss : 0.6405913578836542\n",
            "\n",
            "Epoch : 268, train loss : 0.5919481132969714\n",
            "Epoch : 268, val loss : 0.6396634578704834\n",
            "\n",
            "Epoch : 269, train loss : 0.5902508157672306\n",
            "Epoch : 269, val loss : 0.6477017151682001\n",
            "\n",
            "Epoch : 270, train loss : 0.5912223377011039\n",
            "Epoch : 270, val loss : 0.6264516664178748\n",
            "\n",
            "Epoch : 271, train loss : 0.5897739348989547\n",
            "Epoch : 271, val loss : 0.6598155733786131\n",
            "\n",
            "Epoch : 272, train loss : 0.5906024838938858\n",
            "Epoch : 272, val loss : 0.6264529447806509\n",
            "\n",
            "Epoch : 273, train loss : 0.5887234458417603\n",
            "Epoch : 273, val loss : 0.6287684911175777\n",
            "\n",
            "Epoch : 274, train loss : 0.5922027092991453\n",
            "Epoch : 274, val loss : 0.6466866841441707\n",
            "\n",
            "Epoch : 275, train loss : 0.5906158730839237\n",
            "Epoch : 275, val loss : 0.6405285361566042\n",
            "\n",
            "Epoch : 276, train loss : 0.5891776238427018\n",
            "Epoch : 276, val loss : 0.6351835994344008\n",
            "\n",
            "Epoch : 277, train loss : 0.5886247098445889\n",
            "Epoch : 277, val loss : 0.6558705882022254\n",
            "\n",
            "Epoch : 278, train loss : 0.5898603959517045\n",
            "Epoch : 278, val loss : 0.633681461999291\n",
            "\n",
            "Epoch : 279, train loss : 0.591045578140201\n",
            "Epoch : 279, val loss : 0.6424427236381329\n",
            "\n",
            "Epoch : 280, train loss : 0.5882538347533255\n",
            "Epoch : 280, val loss : 0.6516759740678887\n",
            "\n",
            "Epoch : 281, train loss : 0.5882192503322254\n",
            "Epoch : 281, val loss : 0.6367072776744241\n",
            "\n",
            "Epoch : 282, train loss : 0.5882041288144663\n",
            "Epoch : 282, val loss : 0.6444430806134877\n",
            "\n",
            "Epoch : 283, train loss : 0.589116185542309\n",
            "Epoch : 283, val loss : 0.6356940536122573\n",
            "\n",
            "Epoch : 284, train loss : 0.5882072445118068\n",
            "Epoch : 284, val loss : 0.6312922113820126\n",
            "\n",
            "Epoch : 285, train loss : 0.5870234442479686\n",
            "Epoch : 285, val loss : 0.6289353041272415\n",
            "\n",
            "Epoch : 286, train loss : 0.5872897373907493\n",
            "Epoch : 286, val loss : 0.6399424750553935\n",
            "\n",
            "Epoch : 287, train loss : 0.5863216151793798\n",
            "Epoch : 287, val loss : 0.6530164354725888\n",
            "\n",
            "Epoch : 288, train loss : 0.5882969789432757\n",
            "Epoch : 288, val loss : 0.6329457038327266\n",
            "\n",
            "Epoch : 289, train loss : 0.5859266729065865\n",
            "Epoch : 289, val loss : 0.6465673697622198\n",
            "\n",
            "Epoch : 290, train loss : 0.5878289820569936\n",
            "Epoch : 290, val loss : 0.6409786923935539\n",
            "\n",
            "Epoch : 291, train loss : 0.5869487760644971\n",
            "Epoch : 291, val loss : 0.6529740032396818\n",
            "\n",
            "Epoch : 292, train loss : 0.5864721198876697\n",
            "Epoch : 292, val loss : 0.6405421511123056\n",
            "\n",
            "Epoch : 293, train loss : 0.5857615429343601\n",
            "Epoch : 293, val loss : 0.6415105932637265\n",
            "\n",
            "Epoch : 294, train loss : 0.5870538518284304\n",
            "Epoch : 294, val loss : 0.6316055498625102\n",
            "\n",
            "Epoch : 295, train loss : 0.5864057867816\n",
            "Epoch : 295, val loss : 0.6277300809559069\n",
            "\n",
            "Epoch : 296, train loss : 0.5864863059737464\n",
            "Epoch : 296, val loss : 0.6359251361144216\n",
            "\n",
            "Epoch : 297, train loss : 0.5854775741244808\n",
            "Epoch : 297, val loss : 0.6554460933333949\n",
            "\n",
            "Epoch : 298, train loss : 0.5855182949340705\n",
            "Epoch : 298, val loss : 0.6330435338773226\n",
            "\n",
            "Epoch : 299, train loss : 0.5862719524990435\n",
            "Epoch : 299, val loss : 0.637453229803788\n",
            "\n",
            "Epoch : 300, train loss : 0.5850603237296592\n",
            "Epoch : 300, val loss : 0.6400546447226875\n",
            "\n",
            "Epoch : 301, train loss : 0.5862447502035082\n",
            "Epoch : 301, val loss : 0.6463571582969867\n",
            "\n",
            "Epoch : 302, train loss : 0.5863430985898684\n",
            "Epoch : 302, val loss : 0.640891980183752\n",
            "\n",
            "Epoch : 303, train loss : 0.5867473154356988\n",
            "Epoch : 303, val loss : 0.6326325927910053\n",
            "\n",
            "Epoch : 304, train loss : 0.5846122591784507\n",
            "Epoch : 304, val loss : 0.6276196445289411\n",
            "\n",
            "Epoch : 305, train loss : 0.5838795544523182\n",
            "Epoch : 305, val loss : 0.6541867005197625\n",
            "\n",
            "Epoch : 306, train loss : 0.5876206688808674\n",
            "Epoch : 306, val loss : 0.6304680752126794\n",
            "\n",
            "Epoch : 307, train loss : 0.5848508401350543\n",
            "Epoch : 307, val loss : 0.6533850603982021\n",
            "\n",
            "Epoch : 308, train loss : 0.5833737463662118\n",
            "Epoch : 308, val loss : 0.630894744082501\n",
            "\n",
            "Epoch : 309, train loss : 0.5851812209143785\n",
            "Epoch : 309, val loss : 0.6268677774228548\n",
            "\n",
            "Epoch : 310, train loss : 0.5837942370862671\n",
            "Epoch : 310, val loss : 0.623329482580486\n",
            "\n",
            "Epoch : 311, train loss : 0.5856905315861559\n",
            "Epoch : 311, val loss : 0.6442645082348271\n",
            "\n",
            "Epoch : 312, train loss : 0.5836240091107108\n",
            "Epoch : 312, val loss : 0.6235431636634626\n",
            "\n",
            "Epoch : 313, train loss : 0.5846972566662412\n",
            "Epoch : 313, val loss : 0.6373313602648284\n",
            "\n",
            "Epoch : 314, train loss : 0.58472218802481\n",
            "Epoch : 314, val loss : 0.6344375296642906\n",
            "\n",
            "Epoch : 315, train loss : 0.585985679337473\n",
            "Epoch : 315, val loss : 0.6300952528652392\n",
            "\n",
            "Epoch : 316, train loss : 0.5845786641944538\n",
            "Epoch : 316, val loss : 0.6350586006515905\n",
            "\n",
            "Epoch : 317, train loss : 0.5839761824318856\n",
            "Epoch : 317, val loss : 0.6521792898052617\n",
            "\n",
            "Epoch : 318, train loss : 0.5833426453850482\n",
            "Epoch : 318, val loss : 0.6429003633950887\n",
            "\n",
            "Epoch : 319, train loss : 0.5811943924788274\n",
            "Epoch : 319, val loss : 0.6314849194727444\n",
            "\n",
            "Epoch : 320, train loss : 0.582516957954927\n",
            "Epoch : 320, val loss : 0.6331458483871661\n",
            "\n",
            "Epoch : 321, train loss : 0.5846653176076485\n",
            "Epoch : 321, val loss : 0.6320735576905703\n",
            "\n",
            "Epoch : 322, train loss : 0.5837039358688121\n",
            "Epoch : 322, val loss : 0.6428083808798538\n",
            "\n",
            "Epoch : 323, train loss : 0.582909431963256\n",
            "Epoch : 323, val loss : 0.6436216925319872\n",
            "\n",
            "Epoch : 324, train loss : 0.5815786130500562\n",
            "Epoch : 324, val loss : 0.6409080357928025\n",
            "\n",
            "Epoch : 325, train loss : 0.5810199854951916\n",
            "Epoch : 325, val loss : 0.6317604858624307\n",
            "\n",
            "Epoch : 326, train loss : 0.5805595957871642\n",
            "Epoch : 326, val loss : 0.6448026330847489\n",
            "\n",
            "Epoch : 327, train loss : 0.5834100271716262\n",
            "Epoch : 327, val loss : 0.6286037297625291\n",
            "\n",
            "Epoch : 328, train loss : 0.5820047109416036\n",
            "Epoch : 328, val loss : 0.65580268596348\n",
            "\n",
            "Epoch : 329, train loss : 0.5788096236460136\n",
            "Epoch : 329, val loss : 0.6319216803500526\n",
            "\n",
            "Epoch : 330, train loss : 0.5804506235050434\n",
            "Epoch : 330, val loss : 0.6338986691675688\n",
            "\n",
            "Epoch : 331, train loss : 0.5794710182782374\n",
            "Epoch : 331, val loss : 0.6343597895220706\n",
            "\n",
            "Epoch : 332, train loss : 0.5827219903469083\n",
            "Epoch : 332, val loss : 0.6337108831656608\n",
            "\n",
            "Epoch : 333, train loss : 0.5803393152627078\n",
            "Epoch : 333, val loss : 0.6393149557866549\n",
            "\n",
            "Epoch : 334, train loss : 0.58307792706923\n",
            "Epoch : 334, val loss : 0.6388643729059319\n",
            "\n",
            "Epoch : 335, train loss : 0.5840839971195568\n",
            "Epoch : 335, val loss : 0.6419385734357332\n",
            "\n",
            "Epoch : 336, train loss : 0.5813951472441357\n",
            "Epoch : 336, val loss : 0.6276125092255442\n",
            "\n",
            "Epoch : 337, train loss : 0.5829841917211357\n",
            "Epoch : 337, val loss : 0.6343470639304111\n",
            "\n",
            "Epoch : 338, train loss : 0.5790605902671813\n",
            "Epoch : 338, val loss : 0.6335176856894241\n",
            "\n",
            "Epoch : 339, train loss : 0.5808169422727646\n",
            "Epoch : 339, val loss : 0.6452203738062005\n",
            "\n",
            "Epoch : 340, train loss : 0.5799275246533478\n",
            "Epoch : 340, val loss : 0.6625658869743347\n",
            "\n",
            "Epoch : 341, train loss : 0.5793599903583527\n",
            "Epoch : 341, val loss : 0.6337458616808841\n",
            "\n",
            "Epoch : 342, train loss : 0.5805210758339273\n",
            "Epoch : 342, val loss : 0.6190067655161807\n",
            "\n",
            "Epoch : 343, train loss : 0.5800929371154672\n",
            "Epoch : 343, val loss : 0.6244473269111231\n",
            "\n",
            "Epoch : 344, train loss : 0.5804101582729454\n",
            "Epoch : 344, val loss : 0.6321627717269095\n",
            "\n",
            "Epoch : 345, train loss : 0.5807100501927464\n",
            "Epoch : 345, val loss : 0.6345421326787849\n",
            "\n",
            "Epoch : 346, train loss : 0.5779357801784165\n",
            "Epoch : 346, val loss : 0.634078634412665\n",
            "\n",
            "Epoch : 347, train loss : 0.5802769543546616\n",
            "Epoch : 347, val loss : 0.6229908764362336\n",
            "\n",
            "Epoch : 348, train loss : 0.5801131712667869\n",
            "Epoch : 348, val loss : 0.6588657121909292\n",
            "\n",
            "Epoch : 349, train loss : 0.5789935695402546\n",
            "Epoch : 349, val loss : 0.6157547015892832\n",
            "\n",
            "Epoch : 350, train loss : 0.5797697957718007\n",
            "Epoch : 350, val loss : 0.6468028583024678\n",
            "\n",
            "Epoch : 351, train loss : 0.5784390427849512\n",
            "Epoch : 351, val loss : 0.6482411168123546\n",
            "\n",
            "Epoch : 352, train loss : 0.5808918759678348\n",
            "Epoch : 352, val loss : 0.6322649764387231\n",
            "\n",
            "Epoch : 353, train loss : 0.5780279733917929\n",
            "Epoch : 353, val loss : 0.64272645900124\n",
            "\n",
            "Epoch : 354, train loss : 0.5776753933140729\n",
            "Epoch : 354, val loss : 0.6349370667808933\n",
            "\n",
            "Epoch : 355, train loss : 0.5777657555811335\n",
            "Epoch : 355, val loss : 0.6389321198588922\n",
            "\n",
            "Epoch : 356, train loss : 0.5777379417058192\n",
            "Epoch : 356, val loss : 0.643530801722878\n",
            "\n",
            "Epoch : 357, train loss : 0.5773716110171692\n",
            "Epoch : 357, val loss : 0.6561995336883946\n",
            "\n",
            "Epoch : 358, train loss : 0.5789401908715566\n",
            "Epoch : 358, val loss : 0.6444393035612608\n",
            "\n",
            "Epoch : 359, train loss : 0.5776868576353243\n",
            "Epoch : 359, val loss : 0.6402065252002918\n",
            "\n",
            "Epoch : 360, train loss : 0.5776000830260191\n",
            "Epoch : 360, val loss : 0.6426487659153185\n",
            "\n",
            "Epoch : 361, train loss : 0.5779561190894156\n",
            "Epoch : 361, val loss : 0.6124689500582846\n",
            "\n",
            "Epoch : 362, train loss : 0.5797759180719206\n",
            "Epoch : 362, val loss : 0.6170133336594231\n",
            "\n",
            "Epoch : 363, train loss : 0.5759975978822419\n",
            "Epoch : 363, val loss : 0.618297666311264\n",
            "\n",
            "Epoch : 364, train loss : 0.5765593284910373\n",
            "Epoch : 364, val loss : 0.6321551674290707\n",
            "\n",
            "Epoch : 365, train loss : 0.5778376335447485\n",
            "Epoch : 365, val loss : 0.6388520661153292\n",
            "\n",
            "Epoch : 366, train loss : 0.5764860100818402\n",
            "Epoch : 366, val loss : 0.641065239906311\n",
            "\n",
            "Epoch : 367, train loss : 0.5757553263144057\n",
            "Epoch : 367, val loss : 0.6498169773503354\n",
            "\n",
            "Epoch : 368, train loss : 0.5783261508652657\n",
            "Epoch : 368, val loss : 0.6399657459635484\n",
            "\n",
            "Epoch : 369, train loss : 0.5750211899930783\n",
            "Epoch : 369, val loss : 0.6386032135863055\n",
            "\n",
            "Epoch : 370, train loss : 0.5765088601545855\n",
            "Epoch : 370, val loss : 0.6405371992211593\n",
            "\n",
            "Epoch : 371, train loss : 0.575483772790793\n",
            "Epoch : 371, val loss : 0.6227098985722191\n",
            "\n",
            "Epoch : 372, train loss : 0.5745620546918927\n",
            "Epoch : 372, val loss : 0.6255944543763211\n",
            "\n",
            "Epoch : 373, train loss : 0.5766378807299065\n",
            "Epoch : 373, val loss : 0.6322813677160363\n",
            "\n",
            "Epoch : 374, train loss : 0.5787637401710858\n",
            "Epoch : 374, val loss : 0.6412961388889111\n",
            "\n",
            "Epoch : 375, train loss : 0.5742562046556762\n",
            "Epoch : 375, val loss : 0.6204860492756491\n",
            "\n",
            "Epoch : 376, train loss : 0.5768530269463858\n",
            "Epoch : 376, val loss : 0.6370457206901752\n",
            "\n",
            "Epoch : 377, train loss : 0.5777109196691801\n",
            "Epoch : 377, val loss : 0.6329649059396042\n",
            "\n",
            "Epoch : 378, train loss : 0.5746208346251287\n",
            "Epoch : 378, val loss : 0.6245590448379517\n",
            "\n",
            "Epoch : 379, train loss : 0.573942613782305\n",
            "Epoch : 379, val loss : 0.6344536637005053\n",
            "\n",
            "Epoch : 380, train loss : 0.5751237311146477\n",
            "Epoch : 380, val loss : 0.6321161922655606\n",
            "\n",
            "Epoch : 381, train loss : 0.5742658196073592\n",
            "Epoch : 381, val loss : 0.6368990571875323\n",
            "\n",
            "Epoch : 382, train loss : 0.573897631601854\n",
            "Epoch : 382, val loss : 0.634213808335756\n",
            "\n",
            "Epoch : 383, train loss : 0.5736855416586907\n",
            "Epoch : 383, val loss : 0.6309579328486793\n",
            "\n",
            "Epoch : 384, train loss : 0.5742076622717306\n",
            "Epoch : 384, val loss : 0.6464569349037973\n",
            "\n",
            "Epoch : 385, train loss : 0.5755469154227861\n",
            "Epoch : 385, val loss : 0.6164365991165764\n",
            "\n",
            "Epoch : 386, train loss : 0.5754184632590322\n",
            "Epoch : 386, val loss : 0.6174553475881878\n",
            "\n",
            "Epoch : 387, train loss : 0.5754544747598246\n",
            "Epoch : 387, val loss : 0.6237872478209044\n",
            "\n",
            "Epoch : 388, train loss : 0.5765083302151077\n",
            "Epoch : 388, val loss : 0.6566214922227356\n",
            "\n",
            "Epoch : 389, train loss : 0.5738149003549053\n",
            "Epoch : 389, val loss : 0.649316425386228\n",
            "\n",
            "Epoch : 390, train loss : 0.5741596272497468\n",
            "Epoch : 390, val loss : 0.6244835555553436\n",
            "\n",
            "Epoch : 391, train loss : 0.575090077790347\n",
            "Epoch : 391, val loss : 0.6302162816649989\n",
            "\n",
            "Epoch : 392, train loss : 0.5752089106675345\n",
            "Epoch : 392, val loss : 0.6359902071325403\n",
            "\n",
            "Epoch : 393, train loss : 0.5726366904648871\n",
            "Epoch : 393, val loss : 0.6272899367307363\n",
            "\n",
            "Epoch : 394, train loss : 0.5731845268697449\n",
            "Epoch : 394, val loss : 0.631221575172324\n",
            "\n",
            "Epoch : 395, train loss : 0.5735903998215995\n",
            "Epoch : 395, val loss : 0.6273881730280424\n",
            "\n",
            "Epoch : 396, train loss : 0.5718416094779968\n",
            "Epoch : 396, val loss : 0.6320217769396932\n",
            "\n",
            "Epoch : 397, train loss : 0.574233054572886\n",
            "Epoch : 397, val loss : 0.6170730151628192\n",
            "\n",
            "Epoch : 398, train loss : 0.5754722288160613\n",
            "Epoch : 398, val loss : 0.6394595033244082\n",
            "\n",
            "Epoch : 399, train loss : 0.5738105773925783\n",
            "Epoch : 399, val loss : 0.6441044744692351\n",
            "\n",
            "Epoch : 400, train loss : 0.5730892457745291\n",
            "Epoch : 400, val loss : 0.62479955899088\n",
            "\n",
            "Epoch : 401, train loss : 0.5728883822758996\n",
            "Epoch : 401, val loss : 0.635086505036605\n",
            "\n",
            "Epoch : 402, train loss : 0.5725280122323471\n",
            "Epoch : 402, val loss : 0.6305049438225595\n",
            "\n",
            "Epoch : 403, train loss : 0.5751865600094652\n",
            "Epoch : 403, val loss : 0.6310678689103378\n",
            "\n",
            "Epoch : 404, train loss : 0.5719770729541783\n",
            "Epoch : 404, val loss : 0.6309728967516047\n",
            "\n",
            "Epoch : 405, train loss : 0.5730882749413001\n",
            "Epoch : 405, val loss : 0.6151068132174642\n",
            "\n",
            "Epoch : 406, train loss : 0.5733789779923176\n",
            "Epoch : 406, val loss : 0.6525568554275915\n",
            "\n",
            "Epoch : 407, train loss : 0.572816983858744\n",
            "Epoch : 407, val loss : 0.6228699495917872\n",
            "\n",
            "Epoch : 408, train loss : 0.5726445698376856\n",
            "Epoch : 408, val loss : 0.6394353540320146\n",
            "\n",
            "Epoch : 409, train loss : 0.572834681742119\n",
            "Epoch : 409, val loss : 0.619334804384332\n",
            "\n",
            "Epoch : 410, train loss : 0.5729957183202107\n",
            "Epoch : 410, val loss : 0.6310548562752574\n",
            "\n",
            "Epoch : 411, train loss : 0.5719781891866167\n",
            "Epoch : 411, val loss : 0.623513461727845\n",
            "\n",
            "Epoch : 412, train loss : 0.5718967358271279\n",
            "Epoch : 412, val loss : 0.6062323090277218\n",
            "\n",
            "Epoch : 413, train loss : 0.5718091211535716\n",
            "Epoch : 413, val loss : 0.621545114015278\n",
            "\n",
            "Epoch : 414, train loss : 0.570609404823997\n",
            "Epoch : 414, val loss : 0.6102188998147061\n",
            "\n",
            "Epoch : 415, train loss : 0.5707512653235234\n",
            "Epoch : 415, val loss : 0.6303646925248596\n",
            "\n",
            "Epoch : 416, train loss : 0.5721702297528584\n",
            "Epoch : 416, val loss : 0.621946212492491\n",
            "\n",
            "Epoch : 417, train loss : 0.5697662965817882\n",
            "Epoch : 417, val loss : 0.6157621804036593\n",
            "\n",
            "Epoch : 418, train loss : 0.5688408138173999\n",
            "Epoch : 418, val loss : 0.6336378078711661\n",
            "\n",
            "Epoch : 419, train loss : 0.5717855596181122\n",
            "Epoch : 419, val loss : 0.6300593269498724\n",
            "\n",
            "Epoch : 420, train loss : 0.5704805814858633\n",
            "Epoch : 420, val loss : 0.6219564644913924\n",
            "\n",
            "Epoch : 421, train loss : 0.5701532712488466\n",
            "Epoch : 421, val loss : 0.6353901985444521\n",
            "\n",
            "Epoch : 422, train loss : 0.5705837302135698\n",
            "Epoch : 422, val loss : 0.6167197164736294\n",
            "\n",
            "Epoch : 423, train loss : 0.5690777056144942\n",
            "Epoch : 423, val loss : 0.6177104429194802\n",
            "\n",
            "Epoch : 424, train loss : 0.5710110527096374\n",
            "Epoch : 424, val loss : 0.6346441601452075\n",
            "\n",
            "Epoch : 425, train loss : 0.5680245405796804\n",
            "Epoch : 425, val loss : 0.6286436319351197\n",
            "\n",
            "Epoch : 426, train loss : 0.568797884204171\n",
            "Epoch : 426, val loss : 0.6202019939297124\n",
            "\n",
            "Epoch : 427, train loss : 0.5704983245242725\n",
            "Epoch : 427, val loss : 0.6237956207049521\n",
            "\n",
            "Epoch : 428, train loss : 0.5705304913448564\n",
            "Epoch : 428, val loss : 0.6209745438475358\n",
            "\n",
            "Epoch : 429, train loss : 0.5707926986795484\n",
            "Epoch : 429, val loss : 0.6085891723632812\n",
            "\n",
            "Epoch : 430, train loss : 0.5682589408123129\n",
            "Epoch : 430, val loss : 0.6203023148210425\n",
            "\n",
            "Epoch : 431, train loss : 0.5720662234407483\n",
            "Epoch : 431, val loss : 0.6221608205845481\n",
            "\n",
            "Epoch : 432, train loss : 0.5683842299562514\n",
            "Epoch : 432, val loss : 0.6172229443725787\n",
            "\n",
            "Epoch : 433, train loss : 0.5707688694650478\n",
            "Epoch : 433, val loss : 0.6296235558233763\n",
            "\n",
            "Epoch : 434, train loss : 0.5681954727028357\n",
            "Epoch : 434, val loss : 0.6475796856378253\n",
            "\n",
            "Epoch : 435, train loss : 0.5711822005835445\n",
            "Epoch : 435, val loss : 0.6346408175794702\n",
            "\n",
            "Epoch : 436, train loss : 0.5708400413845526\n",
            "Epoch : 436, val loss : 0.6121547975038227\n",
            "\n",
            "Epoch : 437, train loss : 0.5683493711731653\n",
            "Epoch : 437, val loss : 0.6381628183942092\n",
            "\n",
            "Epoch : 438, train loss : 0.5715309670477202\n",
            "Epoch : 438, val loss : 0.6319754782475924\n",
            "\n",
            "Epoch : 439, train loss : 0.5695954732822649\n",
            "Epoch : 439, val loss : 0.640299677848816\n",
            "\n",
            "Epoch : 440, train loss : 0.5711634213274172\n",
            "Epoch : 440, val loss : 0.6130323911968031\n",
            "\n",
            "Epoch : 441, train loss : 0.5683863330971114\n",
            "Epoch : 441, val loss : 0.6159207334643916\n",
            "\n",
            "Epoch : 442, train loss : 0.5669692743908277\n",
            "Epoch : 442, val loss : 0.6147989285619636\n",
            "\n",
            "Epoch : 443, train loss : 0.5690541489557788\n",
            "Epoch : 443, val loss : 0.617615803291923\n",
            "\n",
            "Epoch : 444, train loss : 0.5665132130637316\n",
            "Epoch : 444, val loss : 0.6169340971269105\n",
            "\n",
            "Epoch : 445, train loss : 0.5695394154750939\n",
            "Epoch : 445, val loss : 0.6301611159977158\n",
            "\n",
            "Epoch : 446, train loss : 0.5695127749081816\n",
            "Epoch : 446, val loss : 0.611403837800026\n",
            "\n",
            "Epoch : 447, train loss : 0.5684852107004685\n",
            "Epoch : 447, val loss : 0.6255076151145131\n",
            "\n",
            "Epoch : 448, train loss : 0.5697324317513092\n",
            "Epoch : 448, val loss : 0.6235146867601495\n",
            "\n",
            "Epoch : 449, train loss : 0.5712012386683265\n",
            "Epoch : 449, val loss : 0.6287891409899059\n",
            "\n",
            "Epoch : 450, train loss : 0.5710954848564033\n",
            "Epoch : 450, val loss : 0.6268808324086038\n",
            "\n",
            "Epoch : 451, train loss : 0.5662164433435959\n",
            "Epoch : 451, val loss : 0.6274365265118449\n",
            "\n",
            "Epoch : 452, train loss : 0.5685836795604592\n",
            "Epoch : 452, val loss : 0.6181952247494145\n",
            "\n",
            "Epoch : 453, train loss : 0.5670796143286155\n",
            "Epoch : 453, val loss : 0.640285694285443\n",
            "\n",
            "Epoch : 454, train loss : 0.5676973196593198\n",
            "Epoch : 454, val loss : 0.6176672562172538\n",
            "\n",
            "Epoch : 455, train loss : 0.5679805528033864\n",
            "Epoch : 455, val loss : 0.6212986284180693\n",
            "\n",
            "Epoch : 456, train loss : 0.5678493431120208\n",
            "Epoch : 456, val loss : 0.6392423824260111\n",
            "\n",
            "Epoch : 457, train loss : 0.565798839113929\n",
            "Epoch : 457, val loss : 0.621689321179139\n",
            "\n",
            "Epoch : 458, train loss : 0.5674529131614804\n",
            "Epoch : 458, val loss : 0.6276283687666844\n",
            "\n",
            "Epoch : 459, train loss : 0.5661889267690255\n",
            "Epoch : 459, val loss : 0.6137977038559163\n",
            "\n",
            "Epoch : 460, train loss : 0.5674754688234038\n",
            "Epoch : 460, val loss : 0.6253588732920194\n",
            "\n",
            "Epoch : 461, train loss : 0.5668785378788456\n",
            "Epoch : 461, val loss : 0.6204050183296204\n",
            "\n",
            "Epoch : 462, train loss : 0.5661277696941838\n",
            "Epoch : 462, val loss : 0.6225754697071878\n",
            "\n",
            "Epoch : 463, train loss : 0.5654499523567433\n",
            "Epoch : 463, val loss : 0.6072822150431183\n",
            "\n",
            "Epoch : 464, train loss : 0.5683447727651306\n",
            "Epoch : 464, val loss : 0.6068553579481024\n",
            "\n",
            "Epoch : 465, train loss : 0.5674438196601289\n",
            "Epoch : 465, val loss : 0.6222077419883326\n",
            "\n",
            "Epoch : 466, train loss : 0.5659768653638435\n",
            "Epoch : 466, val loss : 0.6269558589709433\n",
            "\n",
            "Epoch : 467, train loss : 0.5667698690385531\n",
            "Epoch : 467, val loss : 0.6249306092136784\n",
            "\n",
            "Epoch : 468, train loss : 0.5652558454961489\n",
            "Epoch : 468, val loss : 0.6152181233230389\n",
            "\n",
            "Epoch : 469, train loss : 0.5664089780865292\n",
            "Epoch : 469, val loss : 0.6231561507049359\n",
            "\n",
            "Epoch : 470, train loss : 0.5657557449557565\n",
            "Epoch : 470, val loss : 0.6139430921328696\n",
            "\n",
            "Epoch : 471, train loss : 0.5667209070740322\n",
            "Epoch : 471, val loss : 0.6379483985273464\n",
            "\n",
            "Epoch : 472, train loss : 0.5663128153844313\n",
            "Epoch : 472, val loss : 0.6245236820296237\n",
            "\n",
            "Epoch : 473, train loss : 0.5657794508067044\n",
            "Epoch : 473, val loss : 0.6231926240419084\n",
            "\n",
            "Epoch : 474, train loss : 0.5669814935236264\n",
            "Epoch : 474, val loss : 0.6215369246507945\n",
            "\n",
            "Epoch : 475, train loss : 0.565805530909336\n",
            "Epoch : 475, val loss : 0.618345900585777\n",
            "\n",
            "Epoch : 476, train loss : 0.5675113139730511\n",
            "Epoch : 476, val loss : 0.6300451081050069\n",
            "\n",
            "Epoch : 477, train loss : 0.5647726380463804\n",
            "Epoch : 477, val loss : 0.6026317783092198\n",
            "\n",
            "Epoch : 478, train loss : 0.566806275916822\n",
            "Epoch : 478, val loss : 0.6332483966099588\n",
            "\n",
            "Epoch : 479, train loss : 0.566534319429686\n",
            "Epoch : 479, val loss : 0.6111551507523187\n",
            "\n",
            "Epoch : 480, train loss : 0.5665137567303399\n",
            "Epoch : 480, val loss : 0.6111797737447839\n",
            "\n",
            "Epoch : 481, train loss : 0.5670725966944835\n",
            "Epoch : 481, val loss : 0.649575401293604\n",
            "\n",
            "Epoch : 482, train loss : 0.564554831837163\n",
            "Epoch : 482, val loss : 0.62485576460236\n",
            "\n",
            "Epoch : 483, train loss : 0.5650356240344772\n",
            "Epoch : 483, val loss : 0.6082814022114401\n",
            "\n",
            "Epoch : 484, train loss : 0.5639453794016982\n",
            "Epoch : 484, val loss : 0.6214297602051182\n",
            "\n",
            "Epoch : 485, train loss : 0.5643227584434277\n",
            "Epoch : 485, val loss : 0.6172145903110505\n",
            "\n",
            "Epoch : 486, train loss : 0.5680334396434554\n",
            "Epoch : 486, val loss : 0.6149944866958417\n",
            "\n",
            "Epoch : 487, train loss : 0.5639441370060951\n",
            "Epoch : 487, val loss : 0.6247118962438483\n",
            "\n",
            "Epoch : 488, train loss : 0.5613320751623673\n",
            "Epoch : 488, val loss : 0.6194054107916982\n",
            "\n",
            "Epoch : 489, train loss : 0.5635189206311194\n",
            "Epoch : 489, val loss : 0.623348118443238\n",
            "\n",
            "Epoch : 490, train loss : 0.5647299817114164\n",
            "Epoch : 490, val loss : 0.6252296629704927\n",
            "\n",
            "Epoch : 491, train loss : 0.562688741720084\n",
            "Epoch : 491, val loss : 0.6144871680360091\n",
            "\n",
            "Epoch : 492, train loss : 0.5634145848678821\n",
            "Epoch : 492, val loss : 0.6236935901014429\n",
            "\n",
            "Epoch : 493, train loss : 0.5651641424858209\n",
            "Epoch : 493, val loss : 0.6165908073124133\n",
            "\n",
            "Epoch : 494, train loss : 0.5629261939814595\n",
            "Epoch : 494, val loss : 0.6224809837968727\n",
            "\n",
            "Epoch : 495, train loss : 0.5640664261398897\n",
            "Epoch : 495, val loss : 0.62221150492367\n",
            "\n",
            "Epoch : 496, train loss : 0.5619868480797969\n",
            "Epoch : 496, val loss : 0.6056296511700279\n",
            "\n",
            "Epoch : 497, train loss : 0.5639225497390286\n",
            "Epoch : 497, val loss : 0.6125341402856928\n",
            "\n",
            "Epoch : 498, train loss : 0.5636914952234787\n",
            "Epoch : 498, val loss : 0.6090725613267799\n",
            "\n",
            "Epoch : 499, train loss : 0.5680099761847295\n",
            "Epoch : 499, val loss : 0.6240364736632298\n",
            "\n",
            "Epoch : 500, train loss : 0.5665109139500242\n",
            "Epoch : 500, val loss : 0.6082806508792074\n",
            "\n",
            "Epoch : 501, train loss : 0.5627252203045465\n",
            "Epoch : 501, val loss : 0.6230327047799763\n",
            "\n",
            "Epoch : 502, train loss : 0.5622861555128388\n",
            "Epoch : 502, val loss : 0.6081509401923733\n",
            "\n",
            "Epoch : 503, train loss : 0.5623974381071151\n",
            "Epoch : 503, val loss : 0.614519240040528\n",
            "\n",
            "Epoch : 504, train loss : 0.563458930542975\n",
            "Epoch : 504, val loss : 0.6195858434626932\n",
            "\n",
            "Epoch : 505, train loss : 0.5615356896862841\n",
            "Epoch : 505, val loss : 0.6113918172685724\n",
            "\n",
            "Epoch : 506, train loss : 0.5636628456187972\n",
            "Epoch : 506, val loss : 0.6256844683697349\n",
            "\n",
            "Epoch : 507, train loss : 0.5635248558087782\n",
            "Epoch : 507, val loss : 0.6117356701901083\n",
            "\n",
            "Epoch : 508, train loss : 0.5623961224700464\n",
            "Epoch : 508, val loss : 0.6134748474547738\n",
            "\n",
            "Epoch : 509, train loss : 0.5614302030115416\n",
            "Epoch : 509, val loss : 0.6252737923672325\n",
            "\n",
            "Epoch : 510, train loss : 0.5621902613928825\n",
            "Epoch : 510, val loss : 0.6345071353410421\n",
            "\n",
            "Epoch : 511, train loss : 0.5639825163465559\n",
            "Epoch : 511, val loss : 0.6454743946853438\n",
            "\n",
            "Epoch : 512, train loss : 0.5623665208166295\n",
            "Epoch : 512, val loss : 0.6061121052817293\n",
            "\n",
            "Epoch : 513, train loss : 0.5620985647042591\n",
            "Epoch : 513, val loss : 0.6262431097658057\n",
            "\n",
            "Epoch : 514, train loss : 0.5608731777379007\n",
            "Epoch : 514, val loss : 0.6199628789173928\n",
            "\n",
            "Epoch : 515, train loss : 0.5618195195992787\n",
            "Epoch : 515, val loss : 0.6070584419526552\n",
            "\n",
            "Epoch : 516, train loss : 0.5641937082464044\n",
            "Epoch : 516, val loss : 0.6136908233165742\n",
            "\n",
            "Epoch : 517, train loss : 0.5631448476603537\n",
            "Epoch : 517, val loss : 0.6332657901864304\n",
            "\n",
            "Epoch : 518, train loss : 0.5625747944369461\n",
            "Epoch : 518, val loss : 0.6169490312275134\n",
            "\n",
            "Epoch : 519, train loss : 0.565523192918662\n",
            "Epoch : 519, val loss : 0.6161109067891772\n",
            "\n",
            "Epoch : 520, train loss : 0.5623594121499498\n",
            "Epoch : 520, val loss : 0.6101617718997755\n",
            "\n",
            "Epoch : 521, train loss : 0.5619367478471814\n",
            "Epoch : 521, val loss : 0.6118302392332178\n",
            "\n",
            "Epoch : 522, train loss : 0.5607843521869541\n",
            "Epoch : 522, val loss : 0.6270820169072403\n",
            "\n",
            "Epoch : 523, train loss : 0.5623199132355777\n",
            "Epoch : 523, val loss : 0.6153087553225065\n",
            "\n",
            "Epoch : 524, train loss : 0.5607327547940343\n",
            "Epoch : 524, val loss : 0.6182630736576884\n",
            "\n",
            "Epoch : 525, train loss : 0.5619500494364538\n",
            "Epoch : 525, val loss : 0.6191671945546803\n",
            "\n",
            "Epoch : 526, train loss : 0.5628827458078212\n",
            "Epoch : 526, val loss : 0.6052954479267724\n",
            "\n",
            "Epoch : 527, train loss : 0.5613193127242\n",
            "Epoch : 527, val loss : 0.6080915457323979\n",
            "\n",
            "Epoch : 528, train loss : 0.5615655601024627\n",
            "Epoch : 528, val loss : 0.6282843241566106\n",
            "\n",
            "Epoch : 529, train loss : 0.5597256041837461\n",
            "Epoch : 529, val loss : 0.6162809666834378\n",
            "\n",
            "Epoch : 530, train loss : 0.5606105676203061\n",
            "Epoch : 530, val loss : 0.6039592404114571\n",
            "\n",
            "Epoch : 531, train loss : 0.5598585898225957\n",
            "Epoch : 531, val loss : 0.6158735814847445\n",
            "\n",
            "Epoch : 532, train loss : 0.5623312953746681\n",
            "Epoch : 532, val loss : 0.6185427072801087\n",
            "\n",
            "Epoch : 533, train loss : 0.5615827459277529\n",
            "Epoch : 533, val loss : 0.5991254386148955\n",
            "\n",
            "Epoch : 534, train loss : 0.5621690641749989\n",
            "Epoch : 534, val loss : 0.6233271281970175\n",
            "\n",
            "Epoch : 535, train loss : 0.558405203620593\n",
            "Epoch : 535, val loss : 0.6156061636774163\n",
            "\n",
            "Epoch : 536, train loss : 0.5604600200147343\n",
            "Epoch : 536, val loss : 0.6145498376143608\n",
            "\n",
            "Epoch : 537, train loss : 0.5604665758031784\n",
            "Epoch : 537, val loss : 0.6247129550105648\n",
            "\n",
            "Epoch : 538, train loss : 0.5604669159108943\n",
            "Epoch : 538, val loss : 0.6268391326854104\n",
            "\n",
            "Epoch : 539, train loss : 0.5581301049752667\n",
            "Epoch : 539, val loss : 0.6120300622362839\n",
            "\n",
            "Epoch : 540, train loss : 0.5598756901242516\n",
            "Epoch : 540, val loss : 0.6117784788734035\n",
            "\n",
            "Epoch : 541, train loss : 0.5603170530362567\n",
            "Epoch : 541, val loss : 0.6412322646693178\n",
            "\n",
            "Epoch : 542, train loss : 0.5602965438004696\n",
            "Epoch : 542, val loss : 0.6207076875787032\n",
            "\n",
            "Epoch : 543, train loss : 0.5592715989459647\n",
            "Epoch : 543, val loss : 0.6195469137869383\n",
            "\n",
            "Epoch : 544, train loss : 0.5598115046819052\n",
            "Epoch : 544, val loss : 0.6197542893259149\n",
            "\n",
            "Epoch : 545, train loss : 0.5578974932432172\n",
            "Epoch : 545, val loss : 0.6067945549362584\n",
            "\n",
            "Epoch : 546, train loss : 0.5599554367137674\n",
            "Epoch : 546, val loss : 0.6235158553248957\n",
            "\n",
            "Epoch : 547, train loss : 0.5592903144431838\n",
            "Epoch : 547, val loss : 0.5998100716816751\n",
            "\n",
            "Epoch : 548, train loss : 0.5605409941890024\n",
            "Epoch : 548, val loss : 0.621085850816024\n",
            "\n",
            "Epoch : 549, train loss : 0.5583356859106007\n",
            "Epoch : 549, val loss : 0.6239351598840011\n",
            "\n",
            "Epoch : 550, train loss : 0.5596435044751025\n",
            "Epoch : 550, val loss : 0.6233914416087303\n",
            "\n",
            "Epoch : 551, train loss : 0.558588136145563\n",
            "Epoch : 551, val loss : 0.6145981126709988\n",
            "\n",
            "Epoch : 552, train loss : 0.5596115760730973\n",
            "Epoch : 552, val loss : 0.6292828070489984\n",
            "\n",
            "Epoch : 553, train loss : 0.5597403233701532\n",
            "Epoch : 553, val loss : 0.6051932981139736\n",
            "\n",
            "Epoch : 554, train loss : 0.5595952153205874\n",
            "Epoch : 554, val loss : 0.6124650004662965\n",
            "\n",
            "Epoch : 555, train loss : 0.5598364618691531\n",
            "Epoch : 555, val loss : 0.6041798403388575\n",
            "\n",
            "Epoch : 556, train loss : 0.5592536527099032\n",
            "Epoch : 556, val loss : 0.6066653493203616\n",
            "\n",
            "Epoch : 557, train loss : 0.5582598413481861\n",
            "Epoch : 557, val loss : 0.6105997782004507\n",
            "\n",
            "Epoch : 558, train loss : 0.5581319975130485\n",
            "Epoch : 558, val loss : 0.6140929805605034\n",
            "\n",
            "Epoch : 559, train loss : 0.5581897645285634\n",
            "Epoch : 559, val loss : 0.6074648013240415\n",
            "\n",
            "Epoch : 560, train loss : 0.5588933381167325\n",
            "Epoch : 560, val loss : 0.6154866893040507\n",
            "\n",
            "Epoch : 561, train loss : 0.5593680269790416\n",
            "Epoch : 561, val loss : 0.632387703970859\n",
            "\n",
            "Epoch : 562, train loss : 0.5575209848808521\n",
            "Epoch : 562, val loss : 0.6195408372502578\n",
            "\n",
            "Epoch : 563, train loss : 0.5582126472935534\n",
            "Epoch : 563, val loss : 0.6111465159215425\n",
            "\n",
            "Epoch : 564, train loss : 0.5597442991805798\n",
            "Epoch : 564, val loss : 0.6218411091126893\n",
            "\n",
            "Epoch : 565, train loss : 0.5584502539851449\n",
            "Epoch : 565, val loss : 0.615191715328317\n",
            "\n",
            "Epoch : 566, train loss : 0.5587660999009104\n",
            "Epoch : 566, val loss : 0.6365819996909091\n",
            "\n",
            "Epoch : 567, train loss : 0.5583712810819798\n",
            "Epoch : 567, val loss : 0.6074325489370447\n",
            "\n",
            "Epoch : 568, train loss : 0.5584731275385076\n",
            "Epoch : 568, val loss : 0.6053421230692614\n",
            "\n",
            "Epoch : 569, train loss : 0.5572469653505264\n",
            "Epoch : 569, val loss : 0.6018186744890714\n",
            "\n",
            "Epoch : 570, train loss : 0.5567869746323791\n",
            "Epoch : 570, val loss : 0.6178369820117953\n",
            "\n",
            "Epoch : 571, train loss : 0.5591999449513174\n",
            "Epoch : 571, val loss : 0.6148472067556884\n",
            "\n",
            "Epoch : 572, train loss : 0.5586764711322206\n",
            "Epoch : 572, val loss : 0.6176361002420124\n",
            "\n",
            "Epoch : 573, train loss : 0.5570227516419962\n",
            "Epoch : 573, val loss : 0.6029418001049442\n",
            "\n",
            "Epoch : 574, train loss : 0.5554838821743474\n",
            "Epoch : 574, val loss : 0.6134768347991141\n",
            "\n",
            "Epoch : 575, train loss : 0.5573209180976403\n",
            "Epoch : 575, val loss : 0.6150606525571722\n",
            "\n",
            "Epoch : 576, train loss : 0.5584618877280846\n",
            "Epoch : 576, val loss : 0.6176424167658152\n",
            "\n",
            "Epoch : 577, train loss : 0.55586770845182\n",
            "Epoch : 577, val loss : 0.6200377580366636\n",
            "\n",
            "Epoch : 578, train loss : 0.5565325084960826\n",
            "Epoch : 578, val loss : 0.6156793895520661\n",
            "\n",
            "Epoch : 579, train loss : 0.5557922599893628\n",
            "Epoch : 579, val loss : 0.6203443580552152\n",
            "\n",
            "Epoch : 580, train loss : 0.5609167033975775\n",
            "Epoch : 580, val loss : 0.6021217358739752\n",
            "\n",
            "Epoch : 581, train loss : 0.5558054925817429\n",
            "Epoch : 581, val loss : 0.6126138646351664\n",
            "\n",
            "Epoch : 582, train loss : 0.5584390757661876\n",
            "Epoch : 582, val loss : 0.6364730596542358\n",
            "\n",
            "Epoch : 583, train loss : 0.5571539197907305\n",
            "Epoch : 583, val loss : 0.6116495838290766\n",
            "\n",
            "Epoch : 584, train loss : 0.5577003199042695\n",
            "Epoch : 584, val loss : 0.6209520634851957\n",
            "\n",
            "Epoch : 585, train loss : 0.5557426445411913\n",
            "Epoch : 585, val loss : 0.6143783111321299\n",
            "\n",
            "Epoch : 586, train loss : 0.554549579277183\n",
            "Epoch : 586, val loss : 0.6080150463079151\n",
            "\n",
            "Epoch : 587, train loss : 0.5551318338423062\n",
            "Epoch : 587, val loss : 0.6409389674663544\n",
            "\n",
            "Epoch : 588, train loss : 0.556812293059898\n",
            "Epoch : 588, val loss : 0.6126528460728494\n",
            "\n",
            "Epoch : 589, train loss : 0.5557367575891092\n",
            "Epoch : 589, val loss : 0.594353724467127\n",
            "\n",
            "Epoch : 590, train loss : 0.5547690987586975\n",
            "Epoch : 590, val loss : 0.6247282639930123\n",
            "\n",
            "Epoch : 591, train loss : 0.5550405854528604\n",
            "Epoch : 591, val loss : 0.616276835140429\n",
            "\n",
            "Epoch : 592, train loss : 0.5561070794408972\n",
            "Epoch : 592, val loss : 0.6046800189896634\n",
            "\n",
            "Epoch : 593, train loss : 0.5558789222529439\n",
            "Epoch : 593, val loss : 0.6069555643357729\n",
            "\n",
            "Epoch : 594, train loss : 0.556937259796894\n",
            "Epoch : 594, val loss : 0.6086496459810357\n",
            "\n",
            "Epoch : 595, train loss : 0.5560642556710674\n",
            "Epoch : 595, val loss : 0.6134563778576098\n",
            "\n",
            "Epoch : 596, train loss : 0.5550665028167494\n",
            "Epoch : 596, val loss : 0.5968954633725316\n",
            "\n",
            "Epoch : 597, train loss : 0.5570571048693227\n",
            "Epoch : 597, val loss : 0.6043029051077994\n",
            "\n",
            "Epoch : 598, train loss : 0.5548937737941743\n",
            "Epoch : 598, val loss : 0.61347444433915\n",
            "\n",
            "Epoch : 599, train loss : 0.5548624197642011\n",
            "Epoch : 599, val loss : 0.6192801547677895\n",
            "\n",
            "Epoch : 600, train loss : 0.5580036846074191\n",
            "Epoch : 600, val loss : 0.6085110526335867\n",
            "\n",
            "Epoch : 601, train loss : 0.5548161143606358\n",
            "Epoch : 601, val loss : 0.6002723292300577\n",
            "\n",
            "Epoch : 602, train loss : 0.5553327322006226\n",
            "Epoch : 602, val loss : 0.6169543972140865\n",
            "\n",
            "Epoch : 603, train loss : 0.5533077679800265\n",
            "Epoch : 603, val loss : 0.6197525372630671\n",
            "\n",
            "Epoch : 604, train loss : 0.5538301287275376\n",
            "Epoch : 604, val loss : 0.6338958316727689\n",
            "\n",
            "Epoch : 605, train loss : 0.5545923299861673\n",
            "Epoch : 605, val loss : 0.6061374278444993\n",
            "\n",
            "Epoch : 606, train loss : 0.5551850047978488\n",
            "Epoch : 606, val loss : 0.5996545772803457\n",
            "\n",
            "Epoch : 607, train loss : 0.5590856951294522\n",
            "Epoch : 607, val loss : 0.6014132452638525\n",
            "\n",
            "Epoch : 608, train loss : 0.5558222460024282\n",
            "Epoch : 608, val loss : 0.6244833610559763\n",
            "\n",
            "Epoch : 609, train loss : 0.554683900240696\n",
            "Epoch : 609, val loss : 0.6049845077489552\n",
            "\n",
            "Epoch : 610, train loss : 0.5545660911184367\n",
            "Epoch : 610, val loss : 0.6193148415339621\n",
            "\n",
            "Epoch : 611, train loss : 0.5534549727584377\n",
            "Epoch : 611, val loss : 0.6184140600656207\n",
            "\n",
            "Epoch : 612, train loss : 0.5551742725300066\n",
            "Epoch : 612, val loss : 0.6234298097459894\n",
            "\n",
            "Epoch : 613, train loss : 0.5548820909225578\n",
            "Epoch : 613, val loss : 0.6199383437633512\n",
            "\n",
            "Epoch : 614, train loss : 0.5578724875594634\n",
            "Epoch : 614, val loss : 0.6007400076640279\n",
            "\n",
            "Epoch : 615, train loss : 0.5540191805723943\n",
            "Epoch : 615, val loss : 0.5972199549800472\n",
            "\n",
            "Epoch : 616, train loss : 0.5570996237523628\n",
            "Epoch : 616, val loss : 0.6059378495341853\n",
            "\n",
            "Epoch : 617, train loss : 0.5526871168252195\n",
            "Epoch : 617, val loss : 0.6072811421595121\n",
            "\n",
            "Epoch : 618, train loss : 0.5556998402783363\n",
            "Epoch : 618, val loss : 0.6069475820190029\n",
            "\n",
            "Epoch : 619, train loss : 0.5529670596122741\n",
            "Epoch : 619, val loss : 0.6154971295281461\n",
            "\n",
            "Epoch : 620, train loss : 0.5554899091070349\n",
            "Epoch : 620, val loss : 0.6223390761174653\n",
            "\n",
            "Epoch : 621, train loss : 0.5541728796380938\n",
            "Epoch : 621, val loss : 0.605108181112691\n",
            "\n",
            "Epoch : 622, train loss : 0.5526328993566112\n",
            "Epoch : 622, val loss : 0.6131826733288013\n",
            "\n",
            "Epoch : 623, train loss : 0.5539637816674783\n",
            "Epoch : 623, val loss : 0.6195367574691772\n",
            "\n",
            "Epoch : 624, train loss : 0.5531178301030939\n",
            "Epoch : 624, val loss : 0.6409877570051896\n",
            "\n",
            "Epoch : 625, train loss : 0.5531288197546296\n",
            "Epoch : 625, val loss : 0.6270611584186555\n",
            "\n",
            "Epoch : 626, train loss : 0.5542968717488379\n",
            "Epoch : 626, val loss : 0.5985107766954524\n",
            "\n",
            "Epoch : 627, train loss : 0.5529252735051241\n",
            "Epoch : 627, val loss : 0.6060018492372413\n",
            "\n",
            "Epoch : 628, train loss : 0.5539376253431496\n",
            "Epoch : 628, val loss : 0.6005178564473204\n",
            "\n",
            "Epoch : 629, train loss : 0.5531524661815529\n",
            "Epoch : 629, val loss : 0.6228110209891671\n",
            "\n",
            "Epoch : 630, train loss : 0.5514541724414537\n",
            "Epoch : 630, val loss : 0.5970916293169324\n",
            "\n",
            "Epoch : 631, train loss : 0.5538635551929474\n",
            "Epoch : 631, val loss : 0.6000094539240787\n",
            "\n",
            "Epoch : 632, train loss : 0.5514871508786174\n",
            "Epoch : 632, val loss : 0.6018689666923723\n",
            "\n",
            "Epoch : 633, train loss : 0.5567756163351465\n",
            "Epoch : 633, val loss : 0.5952094636465373\n",
            "\n",
            "Epoch : 634, train loss : 0.5523532242485971\n",
            "Epoch : 634, val loss : 0.6288991288134926\n",
            "\n",
            "Epoch : 635, train loss : 0.5540511441953258\n",
            "Epoch : 635, val loss : 0.6333888737778915\n",
            "\n",
            "Epoch : 636, train loss : 0.552978704553662\n",
            "Epoch : 636, val loss : 0.6163242725949538\n",
            "\n",
            "Epoch : 637, train loss : 0.5568677037051228\n",
            "Epoch : 637, val loss : 0.5971955813859637\n",
            "\n",
            "Epoch : 638, train loss : 0.5529228515697249\n",
            "Epoch : 638, val loss : 0.598446963649047\n",
            "\n",
            "Epoch : 639, train loss : 0.5542128170981552\n",
            "Epoch : 639, val loss : 0.6102276271895358\n",
            "\n",
            "Epoch : 640, train loss : 0.5541624139655722\n",
            "Epoch : 640, val loss : 0.6104042530059816\n",
            "\n",
            "Epoch : 641, train loss : 0.5522149774161252\n",
            "Epoch : 641, val loss : 0.5990743323376304\n",
            "\n",
            "Epoch : 642, train loss : 0.5513257142269248\n",
            "Epoch : 642, val loss : 0.5938106618429486\n",
            "\n",
            "Epoch : 643, train loss : 0.5512642974203282\n",
            "Epoch : 643, val loss : 0.6114131290661661\n",
            "\n",
            "Epoch : 644, train loss : 0.5512802411209454\n",
            "Epoch : 644, val loss : 0.624434304864783\n",
            "\n",
            "Epoch : 645, train loss : 0.5520408868789676\n",
            "Epoch : 645, val loss : 0.6006377856982382\n",
            "\n",
            "Epoch : 646, train loss : 0.552140057628805\n",
            "Epoch : 646, val loss : 0.6004243524451004\n",
            "\n",
            "Epoch : 647, train loss : 0.5513534206332583\n",
            "Epoch : 647, val loss : 0.6239281233988311\n",
            "\n",
            "Epoch : 648, train loss : 0.5510997922131508\n",
            "Epoch : 648, val loss : 0.6097692439430639\n",
            "\n",
            "Epoch : 649, train loss : 0.5517166703036338\n",
            "Epoch : 649, val loss : 0.6031161797674077\n",
            "\n",
            "Epoch : 650, train loss : 0.552733970410896\n",
            "Epoch : 650, val loss : 0.6074757277965546\n",
            "\n",
            "Epoch : 651, train loss : 0.5529002856124527\n",
            "Epoch : 651, val loss : 0.6153923884818427\n",
            "\n",
            "Epoch : 652, train loss : 0.5524291347373619\n",
            "Epoch : 652, val loss : 0.6147200182864542\n",
            "\n",
            "Epoch : 653, train loss : 0.5520426313082377\n",
            "Epoch : 653, val loss : 0.6084401246748472\n",
            "\n",
            "Epoch : 654, train loss : 0.5513285510467758\n",
            "Epoch : 654, val loss : 0.6163432754968341\n",
            "\n",
            "Epoch : 655, train loss : 0.5528727880029964\n",
            "Epoch : 655, val loss : 0.6014280005505211\n",
            "\n",
            "Epoch : 656, train loss : 0.550365581476327\n",
            "Epoch : 656, val loss : 0.6184516517739547\n",
            "\n",
            "Epoch : 657, train loss : 0.5500615483883653\n",
            "Epoch : 657, val loss : 0.6270824608049894\n",
            "\n",
            "Epoch : 658, train loss : 0.5525767210758092\n",
            "Epoch : 658, val loss : 0.6198601628604689\n",
            "\n",
            "Epoch : 659, train loss : 0.5510729986609835\n",
            "Epoch : 659, val loss : 0.606299307785536\n",
            "\n",
            "Epoch : 660, train loss : 0.5523495601885249\n",
            "Epoch : 660, val loss : 0.6099450792136946\n",
            "\n",
            "Epoch : 661, train loss : 0.5512552015709155\n",
            "Epoch : 661, val loss : 0.6136387366997569\n",
            "\n",
            "Epoch : 662, train loss : 0.5507141275839375\n",
            "Epoch : 662, val loss : 0.6132531213132959\n",
            "\n",
            "Epoch : 663, train loss : 0.5513929321910399\n",
            "Epoch : 663, val loss : 0.5962546248185004\n",
            "\n",
            "Epoch : 664, train loss : 0.5503371081568978\n",
            "Epoch : 664, val loss : 0.5881660188499249\n",
            "\n",
            "Epoch : 665, train loss : 0.5513918842330121\n",
            "Epoch : 665, val loss : 0.6047880179003666\n",
            "\n",
            "Epoch : 666, train loss : 0.5499022966081446\n",
            "Epoch : 666, val loss : 0.5951538383960725\n",
            "\n",
            "Epoch : 667, train loss : 0.5495410812623572\n",
            "Epoch : 667, val loss : 0.5978892022057583\n",
            "\n",
            "Epoch : 668, train loss : 0.5509300529956819\n",
            "Epoch : 668, val loss : 0.6210473775863646\n",
            "\n",
            "Epoch : 669, train loss : 0.5503952593514415\n",
            "Epoch : 669, val loss : 0.6132908871299344\n",
            "\n",
            "Epoch : 670, train loss : 0.5505205324201875\n",
            "Epoch : 670, val loss : 0.593784239731337\n",
            "\n",
            "Epoch : 671, train loss : 0.5506295424519163\n",
            "Epoch : 671, val loss : 0.6022024860507563\n",
            "\n",
            "Epoch : 672, train loss : 0.5500012274944421\n",
            "Epoch : 672, val loss : 0.6128421425819397\n",
            "\n",
            "Epoch : 673, train loss : 0.5517435603069538\n",
            "Epoch : 673, val loss : 0.5935036696885762\n",
            "\n",
            "Epoch : 674, train loss : 0.54932198434165\n",
            "Epoch : 674, val loss : 0.6015352873425736\n",
            "\n",
            "Epoch : 675, train loss : 0.5497016717087141\n",
            "Epoch : 675, val loss : 0.6276084529726129\n",
            "\n",
            "Epoch : 676, train loss : 0.5488696719660904\n",
            "Epoch : 676, val loss : 0.6150480477433453\n",
            "\n",
            "Epoch : 677, train loss : 0.5497659065506673\n",
            "Epoch : 677, val loss : 0.6342219318214215\n",
            "\n",
            "Epoch : 678, train loss : 0.5498752532583298\n",
            "Epoch : 678, val loss : 0.6045878309952586\n",
            "\n",
            "Epoch : 679, train loss : 0.5515924921541503\n",
            "Epoch : 679, val loss : 0.5947754932077307\n",
            "\n",
            "Epoch : 680, train loss : 0.5479804301803763\n",
            "Epoch : 680, val loss : 0.5854276418685912\n",
            "\n",
            "Epoch : 681, train loss : 0.5485187992905125\n",
            "Epoch : 681, val loss : 0.6144868370733763\n",
            "\n",
            "Epoch : 682, train loss : 0.5506472202864557\n",
            "Epoch : 682, val loss : 0.6070956666218608\n",
            "\n",
            "Epoch : 683, train loss : 0.548278977473577\n",
            "Epoch : 683, val loss : 0.6224489980622341\n",
            "\n",
            "Epoch : 684, train loss : 0.5489967199889095\n",
            "Epoch : 684, val loss : 0.6079128949265731\n",
            "\n",
            "Epoch : 685, train loss : 0.5516625881195069\n",
            "Epoch : 685, val loss : 0.6119287061063866\n",
            "\n",
            "Epoch : 686, train loss : 0.5498116018194141\n",
            "Epoch : 686, val loss : 0.6333493951119875\n",
            "\n",
            "Epoch : 687, train loss : 0.5502932293848559\n",
            "Epoch : 687, val loss : 0.5873825440281315\n",
            "\n",
            "Epoch : 688, train loss : 0.550795331687638\n",
            "Epoch : 688, val loss : 0.6014435228548551\n",
            "\n",
            "Epoch : 689, train loss : 0.5485372153195466\n",
            "Epoch : 689, val loss : 0.6039119205976788\n",
            "\n",
            "Epoch : 690, train loss : 0.5479009068373479\n",
            "Epoch : 690, val loss : 0.5936932046162455\n",
            "\n",
            "Epoch : 691, train loss : 0.5486583258166456\n",
            "Epoch : 691, val loss : 0.6147567880781075\n",
            "\n",
            "Epoch : 692, train loss : 0.5484098808331926\n",
            "Epoch : 692, val loss : 0.5998520945247852\n",
            "\n",
            "Epoch : 693, train loss : 0.552335335810979\n",
            "Epoch : 693, val loss : 0.6017629551260095\n",
            "\n",
            "Epoch : 694, train loss : 0.5478734977317582\n",
            "Epoch : 694, val loss : 0.6219978489373861\n",
            "\n",
            "Epoch : 695, train loss : 0.5511464124376123\n",
            "Epoch : 695, val loss : 0.5980979910022334\n",
            "\n",
            "Epoch : 696, train loss : 0.5477652945301749\n",
            "Epoch : 696, val loss : 0.6055075385068593\n",
            "\n",
            "Epoch : 697, train loss : 0.5470845439217304\n",
            "Epoch : 697, val loss : 0.6060794905612343\n",
            "\n",
            "Epoch : 698, train loss : 0.547904083403674\n",
            "Epoch : 698, val loss : 0.5938738019842852\n",
            "\n",
            "Epoch : 699, train loss : 0.5485387211496181\n",
            "Epoch : 699, val loss : 0.6402054836875514\n",
            "\n",
            "Epoch : 700, train loss : 0.5499241798213033\n",
            "Epoch : 700, val loss : 0.6038613052744614\n",
            "\n",
            "Epoch : 701, train loss : 0.5485907857165196\n",
            "Epoch : 701, val loss : 0.6021451981444108\n",
            "\n",
            "Epoch : 702, train loss : 0.5475723264795359\n",
            "Epoch : 702, val loss : 0.6078315581146039\n",
            "\n",
            "Epoch : 703, train loss : 0.5471494196039257\n",
            "Epoch : 703, val loss : 0.6165753367700075\n",
            "\n",
            "Epoch : 704, train loss : 0.549409272454002\n",
            "Epoch : 704, val loss : 0.5966545751220302\n",
            "\n",
            "Epoch : 705, train loss : 0.548735233147939\n",
            "Epoch : 705, val loss : 0.59994942734116\n",
            "\n",
            "Epoch : 706, train loss : 0.5474500480926398\n",
            "Epoch : 706, val loss : 0.5872576777872286\n",
            "\n",
            "Epoch : 707, train loss : 0.5521554784341289\n",
            "Epoch : 707, val loss : 0.6079988118849302\n",
            "\n",
            "Epoch : 708, train loss : 0.5477864568883721\n",
            "Epoch : 708, val loss : 0.6035222615066328\n",
            "\n",
            "Epoch : 709, train loss : 0.5485470457510513\n",
            "Epoch : 709, val loss : 0.5977450969972109\n",
            "\n",
            "Epoch : 710, train loss : 0.5475265968929636\n",
            "Epoch : 710, val loss : 0.5977504096533124\n",
            "\n",
            "Epoch : 711, train loss : 0.5463870494654683\n",
            "Epoch : 711, val loss : 0.6176618902306809\n",
            "\n",
            "Epoch : 712, train loss : 0.5508913119633994\n",
            "Epoch : 712, val loss : 0.6001250383101011\n",
            "\n",
            "Epoch : 713, train loss : 0.551036034750216\n",
            "Epoch : 713, val loss : 0.598514043971112\n",
            "\n",
            "Epoch : 714, train loss : 0.5500562129598676\n",
            "Epoch : 714, val loss : 0.6053551341357983\n",
            "\n",
            "Epoch : 715, train loss : 0.5459227717284001\n",
            "Epoch : 715, val loss : 0.5988116891760576\n",
            "\n",
            "Epoch : 716, train loss : 0.5468553764350488\n",
            "Epoch : 716, val loss : 0.5850641609806764\n",
            "\n",
            "Epoch : 717, train loss : 0.5472196210514416\n",
            "Epoch : 717, val loss : 0.5898368295870329\n",
            "\n",
            "Epoch : 718, train loss : 0.5491493214260449\n",
            "Epoch : 718, val loss : 0.6000843800996478\n",
            "\n",
            "Epoch : 719, train loss : 0.5467218585086591\n",
            "Epoch : 719, val loss : 0.6179983176683124\n",
            "\n",
            "Epoch : 720, train loss : 0.5482307856733151\n",
            "Epoch : 720, val loss : 0.6096433448164085\n",
            "\n",
            "Epoch : 721, train loss : 0.5468561978051155\n",
            "Epoch : 721, val loss : 0.5987082390408768\n",
            "\n",
            "Epoch : 722, train loss : 0.5453570315332124\n",
            "Epoch : 722, val loss : 0.5986018996489676\n",
            "\n",
            "Epoch : 723, train loss : 0.5473561552437871\n",
            "Epoch : 723, val loss : 0.595848245056052\n",
            "\n",
            "Epoch : 724, train loss : 0.5450604073929065\n",
            "Epoch : 724, val loss : 0.602176181579891\n",
            "\n",
            "Epoch : 725, train loss : 0.5459900297901848\n",
            "Epoch : 725, val loss : 0.6132083362654634\n",
            "\n",
            "Epoch : 726, train loss : 0.546560657024384\n",
            "Epoch : 726, val loss : 0.6084027792278089\n",
            "\n",
            "Epoch : 727, train loss : 0.5486649711926777\n",
            "Epoch : 727, val loss : 0.5898626525151103\n",
            "\n",
            "Epoch : 728, train loss : 0.5467616363005205\n",
            "Epoch : 728, val loss : 0.5964921822673397\n",
            "\n",
            "Epoch : 729, train loss : 0.5461771540569537\n",
            "Epoch : 729, val loss : 0.6002466976642609\n",
            "\n",
            "Epoch : 730, train loss : 0.5479788198615565\n",
            "Epoch : 730, val loss : 0.6379013657569885\n",
            "\n",
            "Epoch : 731, train loss : 0.549692317572507\n",
            "Epoch : 731, val loss : 0.5917179725672069\n",
            "\n",
            "Epoch : 732, train loss : 0.546602374857122\n",
            "Epoch : 732, val loss : 0.6220288527639289\n",
            "\n",
            "Epoch : 733, train loss : 0.5492751639900788\n",
            "Epoch : 733, val loss : 0.6047394636430239\n",
            "\n",
            "Epoch : 734, train loss : 0.5464098061576033\n",
            "Epoch : 734, val loss : 0.6054022516074934\n",
            "\n",
            "Epoch : 735, train loss : 0.5494022978074622\n",
            "Epoch : 735, val loss : 0.5980314706501206\n",
            "\n",
            "Epoch : 736, train loss : 0.546086939537164\n",
            "Epoch : 736, val loss : 0.6086406582280208\n",
            "\n",
            "Epoch : 737, train loss : 0.5470476544264588\n",
            "Epoch : 737, val loss : 0.616899771125693\n",
            "\n",
            "Epoch : 738, train loss : 0.5462709844112399\n",
            "Epoch : 738, val loss : 0.6222002443514373\n",
            "\n",
            "Epoch : 739, train loss : 0.5480447268847265\n",
            "Epoch : 739, val loss : 0.5915583560341283\n",
            "\n",
            "Epoch : 740, train loss : 0.5466126767071808\n",
            "Epoch : 740, val loss : 0.5984771173251301\n",
            "\n",
            "Epoch : 741, train loss : 0.5457276174516387\n",
            "Epoch : 741, val loss : 0.6289826992310974\n",
            "\n",
            "Epoch : 742, train loss : 0.5449578742186223\n",
            "Epoch : 742, val loss : 0.6047132250509761\n",
            "\n",
            "Epoch : 743, train loss : 0.5465865919084264\n",
            "Epoch : 743, val loss : 0.6141135567112973\n",
            "\n",
            "Epoch : 744, train loss : 0.5444374548666407\n",
            "Epoch : 744, val loss : 0.5939027682731024\n",
            "\n",
            "Epoch : 745, train loss : 0.54360084018924\n",
            "Epoch : 745, val loss : 0.6112033818897448\n",
            "\n",
            "Epoch : 746, train loss : 0.5457717911763627\n",
            "Epoch : 746, val loss : 0.5879970365449002\n",
            "\n",
            "Epoch : 747, train loss : 0.5431791523189254\n",
            "Epoch : 747, val loss : 0.6231742093437597\n",
            "\n",
            "Epoch : 748, train loss : 0.5468527159907597\n",
            "Epoch : 748, val loss : 0.6030004604866631\n",
            "\n",
            "Epoch : 749, train loss : 0.5447087517290404\n",
            "Epoch : 749, val loss : 0.6170257455424258\n",
            "\n",
            "Epoch : 750, train loss : 0.5463598581877622\n",
            "Epoch : 750, val loss : 0.5907989175696121\n",
            "\n",
            "Epoch : 751, train loss : 0.545536519722505\n",
            "Epoch : 751, val loss : 0.5886286434374358\n",
            "\n",
            "Epoch : 752, train loss : 0.5460129602388903\n",
            "Epoch : 752, val loss : 0.5984100228861758\n",
            "\n",
            "Epoch : 753, train loss : 0.545479384516225\n",
            "Epoch : 753, val loss : 0.6044339020001263\n",
            "\n",
            "Epoch : 754, train loss : 0.5425967825181558\n",
            "Epoch : 754, val loss : 0.6278303281257027\n",
            "\n",
            "Epoch : 755, train loss : 0.5465417074434685\n",
            "Epoch : 755, val loss : 0.6010067180583353\n",
            "\n",
            "Epoch : 756, train loss : 0.5450187982934894\n",
            "Epoch : 756, val loss : 0.5975762244902159\n",
            "\n",
            "Epoch : 757, train loss : 0.5459091804244303\n",
            "Epoch : 757, val loss : 0.5895132626357831\n",
            "\n",
            "Epoch : 758, train loss : 0.5436063417882627\n",
            "Epoch : 758, val loss : 0.5949988459285938\n",
            "\n",
            "Epoch : 759, train loss : 0.547677384181456\n",
            "Epoch : 759, val loss : 0.634404419284118\n",
            "\n",
            "Epoch : 760, train loss : 0.5453234885678145\n",
            "Epoch : 760, val loss : 0.5989609674403541\n",
            "\n",
            "Epoch : 761, train loss : 0.5459068796851415\n",
            "Epoch : 761, val loss : 0.6080602815276699\n",
            "\n",
            "Epoch : 762, train loss : 0.544290418697126\n",
            "Epoch : 762, val loss : 0.6217596264261948\n",
            "\n",
            "Epoch : 763, train loss : 0.542629180532513\n",
            "Epoch : 763, val loss : 0.6115616293329941\n",
            "\n",
            "Epoch : 764, train loss : 0.5446496903896332\n",
            "Epoch : 764, val loss : 0.5936334917419835\n",
            "\n",
            "Epoch : 765, train loss : 0.5443126770583064\n",
            "Epoch : 765, val loss : 0.5941886713630274\n",
            "\n",
            "Epoch : 766, train loss : 0.5456974439548721\n",
            "Epoch : 766, val loss : 0.5950898088906941\n",
            "\n",
            "Epoch : 767, train loss : 0.5457184379751031\n",
            "Epoch : 767, val loss : 0.6069181090907048\n",
            "\n",
            "Epoch : 768, train loss : 0.5436756547653313\n",
            "Epoch : 768, val loss : 0.5963959505683497\n",
            "\n",
            "Epoch : 769, train loss : 0.5441274465936602\n",
            "Epoch : 769, val loss : 0.5880961473050871\n",
            "\n",
            "Epoch : 770, train loss : 0.5433512904427268\n",
            "Epoch : 770, val loss : 0.5897828897363262\n",
            "\n",
            "Epoch : 771, train loss : 0.5423661961699979\n",
            "Epoch : 771, val loss : 0.5954332398740869\n",
            "\n",
            "Epoch : 772, train loss : 0.54194415789662\n",
            "Epoch : 772, val loss : 0.6005371542353378\n",
            "\n",
            "Epoch : 773, train loss : 0.543341751532121\n",
            "Epoch : 773, val loss : 0.6061926735074896\n",
            "\n",
            "Epoch : 774, train loss : 0.5431446080858056\n",
            "Epoch : 774, val loss : 0.5938274907438379\n",
            "\n",
            "Epoch : 775, train loss : 0.5420595066113901\n",
            "Epoch : 775, val loss : 0.607454566579116\n",
            "\n",
            "Epoch : 776, train loss : 0.5440803565762261\n",
            "Epoch : 776, val loss : 0.6193733921176509\n",
            "\n",
            "Epoch : 777, train loss : 0.5444308550971927\n",
            "Epoch : 777, val loss : 0.625906218039362\n",
            "\n",
            "Epoch : 778, train loss : 0.5430852340929437\n",
            "Epoch : 778, val loss : 0.5970474120817686\n",
            "\n",
            "Epoch : 779, train loss : 0.5433996677398681\n",
            "Epoch : 779, val loss : 0.6142428580083346\n",
            "\n",
            "Epoch : 780, train loss : 0.5447168294227485\n",
            "Epoch : 780, val loss : 0.6103021894630632\n",
            "\n",
            "Epoch : 781, train loss : 0.5429371877150101\n",
            "Epoch : 781, val loss : 0.6341758433141207\n",
            "\n",
            "Epoch : 782, train loss : 0.5450866359652898\n",
            "Epoch : 782, val loss : 0.6129401435977535\n",
            "\n",
            "Epoch : 783, train loss : 0.5440569126244745\n",
            "Epoch : 783, val loss : 0.6000719948818809\n",
            "\n",
            "Epoch : 784, train loss : 0.5422593412977276\n",
            "Epoch : 784, val loss : 0.5883624483095974\n",
            "\n",
            "Epoch : 785, train loss : 0.5421940247217817\n",
            "Epoch : 785, val loss : 0.6015813005597969\n",
            "\n",
            "Epoch : 786, train loss : 0.5441233241196833\n",
            "Epoch : 786, val loss : 0.5994757555033031\n",
            "\n",
            "Epoch : 787, train loss : 0.5431090369369048\n",
            "Epoch : 787, val loss : 0.6004309764033869\n",
            "\n",
            "Epoch : 788, train loss : 0.5432327313856646\n",
            "Epoch : 788, val loss : 0.5888487806445675\n",
            "\n",
            "Epoch : 789, train loss : 0.5428972256906102\n",
            "Epoch : 789, val loss : 0.5907526094662516\n",
            "\n",
            "Epoch : 790, train loss : 0.5435701565309002\n",
            "Epoch : 790, val loss : 0.5949540310784391\n",
            "\n",
            "Epoch : 791, train loss : 0.543465420332822\n",
            "Epoch : 791, val loss : 0.6028782364569212\n",
            "\n",
            "Epoch : 792, train loss : 0.5421925884304621\n",
            "Epoch : 792, val loss : 0.5977665248670077\n",
            "\n",
            "Epoch : 793, train loss : 0.5451153437296549\n",
            "Epoch : 793, val loss : 0.6091920639339246\n",
            "\n",
            "Epoch : 794, train loss : 0.5441824495792392\n",
            "Epoch : 794, val loss : 0.6104767542136341\n",
            "\n",
            "Epoch : 795, train loss : 0.5435750016660402\n",
            "Epoch : 795, val loss : 0.6000187820509859\n",
            "\n",
            "Epoch : 796, train loss : 0.5462645388010774\n",
            "Epoch : 796, val loss : 0.5944573941983675\n",
            "\n",
            "Epoch : 797, train loss : 0.5453168386762793\n",
            "Epoch : 797, val loss : 0.6177149195420114\n",
            "\n",
            "Epoch : 798, train loss : 0.5430744584762689\n",
            "Epoch : 798, val loss : 0.6104271474637483\n",
            "\n",
            "Epoch : 799, train loss : 0.543014866294283\n",
            "Epoch : 799, val loss : 0.5905539613021048\n",
            "\n",
            "Epoch : 800, train loss : 0.5442814668019611\n",
            "Epoch : 800, val loss : 0.579512780434207\n",
            "0: 2.0714547634124756 / 165: 0.012554271293408944\n",
            "1: 0.8681936264038086 / 165: 0.01781605084737142\n",
            "2: 1.6026928424835205 / 165: 0.027529340801816998\n",
            "3: 4.352251052856445 / 165: 0.05390661991003788\n",
            "4: 1.927280068397522 / 165: 0.06558710517305316\n",
            "5: 3.2877774238586426 / 165: 0.08551302895401464\n",
            "6: 3.5031375885009766 / 165: 0.10674416585402055\n",
            "7: 2.4028687477111816 / 165: 0.12130700674923983\n",
            "8: 1.3033407926559448 / 165: 0.12920604185624557\n",
            "9: 1.8092875480651855 / 165: 0.1401714209354285\n",
            "10: 3.5467562675476074 / 165: 0.16166691346602008\n",
            "11: 0.7749274373054504 / 165: 0.1663634433890834\n",
            "12: 2.5449166297912598 / 165: 0.18178718053933346\n",
            "13: 2.6736698150634766 / 165: 0.19799124002456664\n",
            "14: 1.9939422607421875 / 165: 0.21007573857451928\n",
            "15: 2.4283316135406494 / 165: 0.22479289986870504\n",
            "16: 2.2526144981384277 / 165: 0.23844510894833187\n",
            "17: 1.964930534362793 / 165: 0.2503537788535609\n",
            "18: 1.0233322381973267 / 165: 0.2565557924183932\n",
            "19: 0.5559697151184082 / 165: 0.25992530584335327\n",
            "20: 2.416182041168213 / 165: 0.27456883336558485\n",
            "21: 0.5119482278823853 / 165: 0.27767154989820536\n",
            "22: 1.7788746356964111 / 165: 0.28845260829636543\n",
            "23: 2.4976792335510254 / 165: 0.3035900581966747\n",
            "24: 1.8619722127914429 / 165: 0.31487473827419854\n",
            "25: 1.3695510625839233 / 165: 0.32317504774440414\n",
            "26: 1.7282509803771973 / 165: 0.33364929611032657\n",
            "27: 1.2963240146636963 / 165: 0.34150580529010655\n",
            "28: 1.0098683834075928 / 165: 0.3476262197350011\n",
            "29: 1.0216811895370483 / 165: 0.3538182269443165\n",
            "30: 1.0494590997695923 / 165: 0.3601785851247383\n",
            "31: 1.4027657508850098 / 165: 0.3686801957361626\n",
            "32: 1.825204849243164 / 165: 0.3797420433073333\n",
            "33: 1.6235666275024414 / 165: 0.38958184104977234\n",
            "34: 2.151979923248291 / 165: 0.4026241436149135\n",
            "35: 2.5451314449310303 / 165: 0.41804918267510155\n",
            "36: 1.1821197271347046 / 165: 0.42521354465773614\n",
            "37: 2.2976908683776855 / 165: 0.43913894386002517\n",
            "38: 1.903183102607727 / 165: 0.4506733869061326\n",
            "39: 2.4370079040527344 / 165: 0.4654431317791795\n",
            "40: 1.1496208906173706 / 165: 0.4724105311162545\n",
            "41: 2.499701499938965 / 165: 0.4875602371764906\n",
            "42: 1.999551773071289 / 165: 0.49967873277086206\n",
            "43: 1.90852952003479 / 165: 0.5112455783468305\n",
            "44: 0.6614574193954468 / 165: 0.5152544111916514\n",
            "45: 1.8677093982696533 / 165: 0.5265738620902554\n",
            "46: 1.2835134267807007 / 165: 0.5343527313434717\n",
            "47: 2.5745787620544434 / 165: 0.5499562389922865\n",
            "48: 0.6956292986869812 / 165: 0.554172174135844\n",
            "49: 2.8586483001708984 / 165: 0.571497315349001\n",
            "50: 1.7921457290649414 / 165: 0.5823588046160612\n",
            "51: 1.5110244750976562 / 165: 0.5915165287075621\n",
            "52: 2.6371994018554688 / 165: 0.6074995553854741\n",
            "53: 0.42976903915405273 / 165: 0.6101042162288319\n",
            "54: 0.5767446160316467 / 165: 0.6135996381441753\n",
            "55: 2.0203335285186768 / 165: 0.6258440837715612\n",
            "56: 1.292420744895935 / 165: 0.6336769367709305\n",
            "57: 0.7389388084411621 / 165: 0.638155353791786\n",
            "58: 1.4889476299285889 / 165: 0.6471792788216563\n",
            "59: 1.8237885236740112 / 165: 0.6582325426014988\n",
            "60: 1.594094157218933 / 165: 0.6678937193119165\n",
            "61: 2.5162291526794434 / 165: 0.6831435929645192\n",
            "62: 2.472681999206543 / 165: 0.6981295444748619\n",
            "63: 1.6924189329147339 / 165: 0.7083866289167693\n",
            "64: 0.6400294899940491 / 165: 0.7122655955227938\n",
            "65: 2.099493980407715 / 165: 0.7249898014646587\n",
            "66: 1.5177347660064697 / 165: 0.7341881939859101\n",
            "67: 2.3608171939849854 / 165: 0.7484961769797585\n",
            "68: 1.4507423639297485 / 165: 0.7572885549429691\n",
            "69: 1.6744794845581055 / 165: 0.7674369154554425\n",
            "70: 1.6555126905441284 / 165: 0.7774703257011645\n",
            "71: 1.570568561553955 / 165: 0.7869889230439158\n",
            "72: 1.7728404998779297 / 165: 0.7977334109219638\n",
            "73: 1.7008893489837646 / 165: 0.8080418312188351\n",
            "74: 0.9104537963867188 / 165: 0.8135597330151183\n",
            "75: 1.2721128463745117 / 165: 0.8212695078416304\n",
            "76: 0.8598299026489258 / 165: 0.8264805981607148\n",
            "77: 1.7369418144226074 / 165: 0.8370075182481246\n",
            "78: 3.0259323120117188 / 165: 0.8553465019572866\n",
            "79: 2.4151668548583984 / 165: 0.8699838768352163\n",
            "80: 0.9153215885162354 / 165: 0.8755312804019814\n",
            "81: 1.6988227367401123 / 165: 0.8858271757761639\n",
            "82: 1.168865442276001 / 165: 0.8929112087596548\n",
            "83: 1.4083313941955566 / 165: 0.9014465505426582\n",
            "84: 0.7858792543411255 / 165: 0.9062094551144226\n",
            "85: 1.404244303703308 / 165: 0.9147200266520185\n",
            "86: 3.0553369522094727 / 165: 0.9332372203017728\n",
            "87: 3.254639148712158 / 165: 0.9529623060515434\n",
            "88: 0.7907832860946655 / 165: 0.9577549320278748\n",
            "89: 1.2854894399642944 / 165: 0.9655457771185675\n",
            "90: 1.6286252737045288 / 165: 0.9754162333228373\n",
            "91: 1.7114081382751465 / 165: 0.9857884038578382\n",
            "92: 1.6314027309417725 / 165: 0.9956756931362731\n",
            "93: 0.7439867854118347 / 165: 1.000184703956951\n",
            "94: 1.2844411134719849 / 165: 1.0079691955537509\n",
            "95: 1.3092362880706787 / 165: 1.0159039609359974\n",
            "96: 0.8840144872665405 / 165: 1.0212616244951886\n",
            "97: 2.362757682800293 / 165: 1.0355813680273116\n",
            "98: 1.323159098625183 / 165: 1.0436005140795854\n",
            "99: 2.904958486557007 / 165: 1.0612063230890219\n",
            "100: 0.6714875102043152 / 165: 1.0652759443629873\n",
            "101: 1.1262553930282593 / 165: 1.0721017346237647\n",
            "102: 1.0536185503005981 / 165: 1.0784873015952834\n",
            "103: 1.5994601249694824 / 165: 1.0881809993223712\n",
            "104: 1.9361069202423096 / 165: 1.099914980657173\n",
            "105: 1.9681293964385986 / 165: 1.111843037605286\n",
            "106: 1.82761549949646 / 165: 1.1229194951779917\n",
            "107: 1.118375301361084 / 165: 1.1296975273074528\n",
            "108: 1.1297529935836792 / 165: 1.1365445151473539\n",
            "109: 1.0421192646026611 / 165: 1.1428603894782792\n",
            "110: 0.9985537528991699 / 165: 1.1489122304049408\n",
            "111: 1.3835978507995605 / 165: 1.1572976719249382\n",
            "112: 0.7230141758918762 / 165: 1.1616795760212526\n",
            "113: 1.2234587669372559 / 165: 1.1690944776390542\n",
            "114: 0.47434377670288086 / 165: 1.1719692884069504\n",
            "115: 0.9823125004768372 / 165: 1.1779226975007493\n",
            "116: 1.351198434829712 / 165: 1.1861117789239597\n",
            "117: 0.8329932689666748 / 165: 1.1911602229783032\n",
            "118: 0.6723207235336304 / 165: 1.1952348940300221\n",
            "119: 1.850535273551941 / 165: 1.2064502593242763\n",
            "120: 0.8422176837921143 / 165: 1.2115546089230165\n",
            "121: 2.0167112350463867 / 165: 1.223777101256631\n",
            "122: 0.9362856149673462 / 165: 1.2294515595291604\n",
            "123: 0.7134966850280762 / 165: 1.233775781862664\n",
            "124: 1.1517281532287598 / 165: 1.240755952488293\n",
            "125: 0.8196328282356262 / 165: 1.2457234241745694\n",
            "126: 0.7893247604370117 / 165: 1.2505072106014603\n",
            "127: 0.8671002984046936 / 165: 1.2557623639251252\n",
            "128: 1.7929950952529907 / 165: 1.2666290008660523\n",
            "129: 1.1984014511108398 / 165: 1.2738920399636937\n",
            "130: 1.2073850631713867 / 165: 1.2812095251950355\n",
            "131: 1.1802639961242676 / 165: 1.2883626403230615\n",
            "132: 0.49689918756484985 / 165: 1.2913741505507272\n",
            "133: 0.7907553911209106 / 165: 1.2961666074666116\n",
            "134: 0.7243011593818665 / 165: 1.3005563114628653\n",
            "135: 0.9138932228088379 / 165: 1.3060950582677673\n",
            "136: 0.8205230236053467 / 165: 1.3110679250774966\n",
            "137: 1.5663681030273438 / 165: 1.3205610650958441\n",
            "138: 0.8034276962280273 / 165: 1.3254303238608625\n",
            "139: 1.2034614086151123 / 165: 1.3327240293676208\n",
            "140: 0.9568906426429749 / 165: 1.33852336659576\n",
            "141: 0.8815239667892456 / 165: 1.3438659360914524\n",
            "142: 1.118991732597351 / 165: 1.3506477041678\n",
            "143: 1.2519164085388184 / 165: 1.3582350763407625\n",
            "144: 0.8956910371780396 / 165: 1.3636635068691143\n",
            "145: 1.5694832801818848 / 165: 1.3731755267490044\n",
            "146: 1.0394797325134277 / 165: 1.3794754039157524\n",
            "147: 0.8990478515625 / 165: 1.384924178773707\n",
            "148: 0.5156159400939941 / 165: 1.3880491238651858\n",
            "149: 1.1799840927124023 / 165: 1.3952005426088974\n",
            "150: 1.0652275085449219 / 165: 1.401656466903109\n",
            "151: 1.6765007972717285 / 165: 1.411817077795665\n",
            "152: 1.2049343585968018 / 165: 1.4191197102720092\n",
            "153: 0.7769691944122314 / 165: 1.4238286144805683\n",
            "154: 0.8079554438591003 / 165: 1.4287253141403204\n",
            "155: 0.974820077419281 / 165: 1.4346333146095283\n",
            "156: 0.945473313331604 / 165: 1.440363455902447\n",
            "157: 0.9224637746810913 / 165: 1.4459541454459688\n",
            "158: 0.8594784736633301 / 165: 1.4511631058924133\n",
            "159: 1.050804853439331 / 165: 1.4575316201556818\n",
            "160: 1.0009835958480835 / 165: 1.463598187403246\n",
            "161: 1.1168365478515625 / 165: 1.4703668937538614\n",
            "162: 0.8399567008018494 / 165: 1.4754575404253878\n",
            "163: 0.722198486328125 / 165: 1.4798345009485885\n",
            "164: 1.0638347864151 / 165: 1.4862819845026194\n",
            "\n",
            "Epoch : 1, train loss : 1.4862819845026194\n",
            "Epoch : 1, val loss : 0.9203283567177621\n",
            "\n",
            "Epoch : 2, train loss : 0.7570283167289964\n",
            "Epoch : 2, val loss : 0.6864785238316184\n",
            "\n",
            "Epoch : 3, train loss : 0.6999543977506228\n",
            "Epoch : 3, val loss : 0.6904753321095517\n",
            "\n",
            "Epoch : 4, train loss : 0.6964291182431309\n",
            "Epoch : 4, val loss : 0.6940518209808751\n",
            "\n",
            "Epoch : 5, train loss : 0.6948784997968961\n",
            "Epoch : 5, val loss : 0.6912157096360859\n",
            "\n",
            "Epoch : 6, train loss : 0.6926240198539965\n",
            "Epoch : 6, val loss : 0.6952858473125257\n",
            "\n",
            "Epoch : 7, train loss : 0.6915422038598495\n",
            "Epoch : 7, val loss : 0.6927070743159243\n",
            "\n",
            "Epoch : 8, train loss : 0.6911389896363923\n",
            "Epoch : 8, val loss : 0.691880288876985\n",
            "\n",
            "Epoch : 9, train loss : 0.6904649694760641\n",
            "Epoch : 9, val loss : 0.6949876923310128\n",
            "\n",
            "Epoch : 10, train loss : 0.6893504269195327\n",
            "Epoch : 10, val loss : 0.6985353105946593\n",
            "\n",
            "Epoch : 11, train loss : 0.6882471196579211\n",
            "Epoch : 11, val loss : 0.6962842690317255\n",
            "\n",
            "Epoch : 12, train loss : 0.6899484717484679\n",
            "Epoch : 12, val loss : 0.6995429835821452\n",
            "\n",
            "Epoch : 13, train loss : 0.6867075100089564\n",
            "Epoch : 13, val loss : 0.6960422333918118\n",
            "\n",
            "Epoch : 14, train loss : 0.6870076378186543\n",
            "Epoch : 14, val loss : 0.7019114149244207\n",
            "\n",
            "Epoch : 15, train loss : 0.6865407582485314\n",
            "Epoch : 15, val loss : 0.6929721110745479\n",
            "\n",
            "Epoch : 16, train loss : 0.6865577936172489\n",
            "Epoch : 16, val loss : 0.6981836494646574\n",
            "\n",
            "Epoch : 17, train loss : 0.6848708373127563\n",
            "Epoch : 17, val loss : 0.6985603821905036\n",
            "\n",
            "Epoch : 18, train loss : 0.6848760984160681\n",
            "Epoch : 18, val loss : 0.694263574324156\n",
            "\n",
            "Epoch : 19, train loss : 0.6834183562885633\n",
            "Epoch : 19, val loss : 0.6942477100773863\n",
            "\n",
            "Epoch : 20, train loss : 0.6821601228280505\n",
            "Epoch : 20, val loss : 0.7016441696568539\n",
            "\n",
            "Epoch : 21, train loss : 0.6847950075611924\n",
            "Epoch : 21, val loss : 0.6974566327898126\n",
            "\n",
            "Epoch : 22, train loss : 0.682260608673096\n",
            "Epoch : 22, val loss : 0.6949579182424044\n",
            "\n",
            "Epoch : 23, train loss : 0.6813344897645891\n",
            "Epoch : 23, val loss : 0.6959696443457353\n",
            "\n",
            "Epoch : 24, train loss : 0.6813724644256361\n",
            "Epoch : 24, val loss : 0.6968080871983579\n",
            "\n",
            "Epoch : 25, train loss : 0.6806477087916751\n",
            "Epoch : 25, val loss : 0.6932106865079779\n",
            "\n",
            "Epoch : 26, train loss : 0.6798598036621556\n",
            "Epoch : 26, val loss : 0.6929223757041127\n",
            "\n",
            "Epoch : 27, train loss : 0.6786215829126764\n",
            "Epoch : 27, val loss : 0.6936236619949341\n",
            "\n",
            "Epoch : 28, train loss : 0.6789505167440937\n",
            "Epoch : 28, val loss : 0.6873892671183535\n",
            "\n",
            "Epoch : 29, train loss : 0.6780849687980887\n",
            "Epoch : 29, val loss : 0.6972876034284894\n",
            "\n",
            "Epoch : 30, train loss : 0.6777359518137843\n",
            "Epoch : 30, val loss : 0.7009732974203009\n",
            "\n",
            "Epoch : 31, train loss : 0.6784820404919715\n",
            "Epoch : 31, val loss : 0.6915513684875085\n",
            "\n",
            "Epoch : 32, train loss : 0.6766538970398183\n",
            "Epoch : 32, val loss : 0.6935989417527851\n",
            "\n",
            "Epoch : 33, train loss : 0.6767254764383495\n",
            "Epoch : 33, val loss : 0.6907696974904914\n",
            "\n",
            "Epoch : 34, train loss : 0.6765354813951433\n",
            "Epoch : 34, val loss : 0.6954248641666613\n",
            "\n",
            "Epoch : 35, train loss : 0.6751240751960059\n",
            "Epoch : 35, val loss : 0.6902768486424495\n",
            "\n",
            "Epoch : 36, train loss : 0.6748518463337057\n",
            "Epoch : 36, val loss : 0.6948694774979038\n",
            "\n",
            "Epoch : 37, train loss : 0.6744060978744969\n",
            "Epoch : 37, val loss : 0.6908936437807586\n",
            "\n",
            "Epoch : 38, train loss : 0.6738055825233453\n",
            "Epoch : 38, val loss : 0.6908318965058579\n",
            "\n",
            "Epoch : 39, train loss : 0.6728726119706121\n",
            "Epoch : 39, val loss : 0.6871842208661533\n",
            "\n",
            "Epoch : 40, train loss : 0.6726282715797421\n",
            "Epoch : 40, val loss : 0.690881565997475\n",
            "\n",
            "Epoch : 41, train loss : 0.6719690348162799\n",
            "Epoch : 41, val loss : 0.6856370191825063\n",
            "\n",
            "Epoch : 42, train loss : 0.6734914107756182\n",
            "Epoch : 42, val loss : 0.6850615175146807\n",
            "\n",
            "Epoch : 43, train loss : 0.6728801170984905\n",
            "Epoch : 43, val loss : 0.6918959429389554\n",
            "\n",
            "Epoch : 44, train loss : 0.6716922377095077\n",
            "Epoch : 44, val loss : 0.6894157999440244\n",
            "\n",
            "Epoch : 45, train loss : 0.670977815353509\n",
            "Epoch : 45, val loss : 0.6847881988475197\n",
            "\n",
            "Epoch : 46, train loss : 0.6700953274062187\n",
            "Epoch : 46, val loss : 0.688609945146661\n",
            "\n",
            "Epoch : 47, train loss : 0.6700193701368391\n",
            "Epoch : 47, val loss : 0.6888009466623005\n",
            "\n",
            "Epoch : 48, train loss : 0.6700250571424309\n",
            "Epoch : 48, val loss : 0.692985779360721\n",
            "\n",
            "Epoch : 49, train loss : 0.6690355001073894\n",
            "Epoch : 49, val loss : 0.6891281291058188\n",
            "\n",
            "Epoch : 50, train loss : 0.6692989324078413\n",
            "Epoch : 50, val loss : 0.6926934907310887\n",
            "\n",
            "Epoch : 51, train loss : 0.6685492284370196\n",
            "Epoch : 51, val loss : 0.6856615135544224\n",
            "\n",
            "Epoch : 52, train loss : 0.6669929930658053\n",
            "Epoch : 52, val loss : 0.6903136278453627\n",
            "\n",
            "Epoch : 53, train loss : 0.667065872929313\n",
            "Epoch : 53, val loss : 0.6862811289335552\n",
            "\n",
            "Epoch : 54, train loss : 0.6677389412215262\n",
            "Epoch : 54, val loss : 0.6889004581852963\n",
            "\n",
            "Epoch : 55, train loss : 0.6664915160699324\n",
            "Epoch : 55, val loss : 0.6862594924475017\n",
            "\n",
            "Epoch : 56, train loss : 0.6663817011948788\n",
            "Epoch : 56, val loss : 0.6826977667055633\n",
            "\n",
            "Epoch : 57, train loss : 0.6656210346655417\n",
            "Epoch : 57, val loss : 0.689532785039199\n",
            "\n",
            "Epoch : 58, train loss : 0.6661727728265705\n",
            "Epoch : 58, val loss : 0.6821355160913971\n",
            "\n",
            "Epoch : 59, train loss : 0.6653585878285492\n",
            "Epoch : 59, val loss : 0.6845387094899228\n",
            "\n",
            "Epoch : 60, train loss : 0.6642615354422368\n",
            "Epoch : 60, val loss : 0.6881565984926725\n",
            "\n",
            "Epoch : 61, train loss : 0.6657477526953726\n",
            "Epoch : 61, val loss : 0.6829142382270412\n",
            "\n",
            "Epoch : 62, train loss : 0.6632158279418946\n",
            "Epoch : 62, val loss : 0.6912912475435357\n",
            "\n",
            "Epoch : 63, train loss : 0.6631826928167631\n",
            "Epoch : 63, val loss : 0.6816752145164892\n",
            "\n",
            "Epoch : 64, train loss : 0.6631859634861803\n",
            "Epoch : 64, val loss : 0.6937197415452254\n",
            "\n",
            "Epoch : 65, train loss : 0.6628821376598243\n",
            "Epoch : 65, val loss : 0.6859984523371645\n",
            "\n",
            "Epoch : 66, train loss : 0.6629313609816812\n",
            "Epoch : 66, val loss : 0.6801421234482213\n",
            "\n",
            "Epoch : 67, train loss : 0.6617881760452734\n",
            "Epoch : 67, val loss : 0.6808765561957109\n",
            "\n",
            "Epoch : 68, train loss : 0.6619028532143791\n",
            "Epoch : 68, val loss : 0.6857937009711015\n",
            "\n",
            "Epoch : 69, train loss : 0.6605939742290614\n",
            "Epoch : 69, val loss : 0.6852036463586907\n",
            "\n",
            "Epoch : 70, train loss : 0.6593714573166587\n",
            "Epoch : 70, val loss : 0.6825614132379231\n",
            "\n",
            "Epoch : 71, train loss : 0.6604965412255491\n",
            "Epoch : 71, val loss : 0.6833171593515498\n",
            "\n",
            "Epoch : 72, train loss : 0.6605687061945599\n",
            "Epoch : 72, val loss : 0.6901056954735203\n",
            "\n",
            "Epoch : 73, train loss : 0.6584401399800273\n",
            "Epoch : 73, val loss : 0.6832157342057479\n",
            "\n",
            "Epoch : 74, train loss : 0.6593961964954029\n",
            "Epoch : 74, val loss : 0.6790752850080791\n",
            "\n",
            "Epoch : 75, train loss : 0.6593218189297301\n",
            "Epoch : 75, val loss : 0.6821146576028121\n",
            "\n",
            "Epoch : 76, train loss : 0.6590158000136866\n",
            "Epoch : 76, val loss : 0.6846626777397958\n",
            "\n",
            "Epoch : 77, train loss : 0.6581564036282627\n",
            "Epoch : 77, val loss : 0.681904601423364\n",
            "\n",
            "Epoch : 78, train loss : 0.6589598272785998\n",
            "Epoch : 78, val loss : 0.6811002524275529\n",
            "\n",
            "Epoch : 79, train loss : 0.657361442753763\n",
            "Epoch : 79, val loss : 0.6850022359898217\n",
            "\n",
            "Epoch : 80, train loss : 0.6569754817269068\n",
            "Epoch : 80, val loss : 0.6790246681163186\n",
            "\n",
            "Epoch : 81, train loss : 0.6571731527646383\n",
            "Epoch : 81, val loss : 0.681302867437664\n",
            "\n",
            "Epoch : 82, train loss : 0.6569693493120595\n",
            "Epoch : 82, val loss : 0.6809154152870178\n",
            "\n",
            "Epoch : 83, train loss : 0.6581449960217327\n",
            "Epoch : 83, val loss : 0.6787007952991285\n",
            "\n",
            "Epoch : 84, train loss : 0.6567630782271876\n",
            "Epoch : 84, val loss : 0.683090583274239\n",
            "\n",
            "Epoch : 85, train loss : 0.6550569873867609\n",
            "Epoch : 85, val loss : 0.6720761688132033\n",
            "\n",
            "Epoch : 86, train loss : 0.6549956101359742\n",
            "Epoch : 86, val loss : 0.6806191362832722\n",
            "\n",
            "Epoch : 87, train loss : 0.6559072862971915\n",
            "Epoch : 87, val loss : 0.6852863838798122\n",
            "\n",
            "Epoch : 88, train loss : 0.6538235321189416\n",
            "Epoch : 88, val loss : 0.6738190839165135\n",
            "\n",
            "Epoch : 89, train loss : 0.6556815978252529\n",
            "Epoch : 89, val loss : 0.6863939260181627\n",
            "\n",
            "Epoch : 90, train loss : 0.6543588403499487\n",
            "Epoch : 90, val loss : 0.6836118917716177\n",
            "\n",
            "Epoch : 91, train loss : 0.6547071688102958\n",
            "Epoch : 91, val loss : 0.6897669936481274\n",
            "\n",
            "Epoch : 92, train loss : 0.6526654633608733\n",
            "Epoch : 92, val loss : 0.6689304207500659\n",
            "\n",
            "Epoch : 93, train loss : 0.6541361433086973\n",
            "Epoch : 93, val loss : 0.6729904444594133\n",
            "\n",
            "Epoch : 94, train loss : 0.6530442974784155\n",
            "Epoch : 94, val loss : 0.6758845354381359\n",
            "\n",
            "Epoch : 95, train loss : 0.6535271286964416\n",
            "Epoch : 95, val loss : 0.6683414892146463\n",
            "\n",
            "Epoch : 96, train loss : 0.6529014085278367\n",
            "Epoch : 96, val loss : 0.6680419884229962\n",
            "\n",
            "Epoch : 97, train loss : 0.6527101729855388\n",
            "Epoch : 97, val loss : 0.6762743623633134\n",
            "\n",
            "Epoch : 98, train loss : 0.6511712814822341\n",
            "Epoch : 98, val loss : 0.6745734340266178\n",
            "\n",
            "Epoch : 99, train loss : 0.650074673060215\n",
            "Epoch : 99, val loss : 0.6868249491641397\n",
            "\n",
            "Epoch : 100, train loss : 0.6516393769871102\n",
            "Epoch : 100, val loss : 0.6794173403790124\n",
            "\n",
            "Epoch : 101, train loss : 0.6500569199070789\n",
            "Epoch : 101, val loss : 0.6807943958985179\n",
            "\n",
            "Epoch : 102, train loss : 0.6515512455593452\n",
            "Epoch : 102, val loss : 0.6791246564764727\n",
            "\n",
            "Epoch : 103, train loss : 0.6504539007490332\n",
            "Epoch : 103, val loss : 0.6720248398027922\n",
            "\n",
            "Epoch : 104, train loss : 0.6496106295874623\n",
            "Epoch : 104, val loss : 0.6798280508894669\n",
            "\n",
            "Epoch : 105, train loss : 0.6492134148424321\n",
            "Epoch : 105, val loss : 0.6763711101130436\n",
            "\n",
            "Epoch : 106, train loss : 0.649503725586515\n",
            "Epoch : 106, val loss : 0.6776160786026403\n",
            "\n",
            "Epoch : 107, train loss : 0.64869191736886\n",
            "Epoch : 107, val loss : 0.6669242476162158\n",
            "\n",
            "Epoch : 108, train loss : 0.6495171770905004\n",
            "Epoch : 108, val loss : 0.6744938492774963\n",
            "\n",
            "Epoch : 109, train loss : 0.6490113937493526\n",
            "Epoch : 109, val loss : 0.6776765239866156\n",
            "\n",
            "Epoch : 110, train loss : 0.648686117475683\n",
            "Epoch : 110, val loss : 0.6757717948210867\n",
            "\n",
            "Epoch : 111, train loss : 0.6484202493320813\n",
            "Epoch : 111, val loss : 0.6793926734673349\n",
            "\n",
            "Epoch : 112, train loss : 0.6485251553130874\n",
            "Epoch : 112, val loss : 0.6705546504572817\n",
            "\n",
            "Epoch : 113, train loss : 0.6476077448238021\n",
            "Epoch : 113, val loss : 0.6726316150866056\n",
            "\n",
            "Epoch : 114, train loss : 0.6469533255605987\n",
            "Epoch : 114, val loss : 0.6837264393505298\n",
            "\n",
            "Epoch : 115, train loss : 0.6476494156953062\n",
            "Epoch : 115, val loss : 0.6678976949892547\n",
            "\n",
            "Epoch : 116, train loss : 0.6473930900747126\n",
            "Epoch : 116, val loss : 0.679316781069103\n",
            "\n",
            "Epoch : 117, train loss : 0.6462909752672366\n",
            "Epoch : 117, val loss : 0.6717551538818759\n",
            "\n",
            "Epoch : 118, train loss : 0.6462584121660753\n",
            "Epoch : 118, val loss : 0.6723490859332837\n",
            "\n",
            "Epoch : 119, train loss : 0.6449907230608392\n",
            "Epoch : 119, val loss : 0.6684406964402451\n",
            "\n",
            "Epoch : 120, train loss : 0.6441823769699445\n",
            "Epoch : 120, val loss : 0.6807931065559387\n",
            "\n",
            "Epoch : 121, train loss : 0.6459052880605065\n",
            "Epoch : 121, val loss : 0.6698144988009804\n",
            "\n",
            "Epoch : 122, train loss : 0.6460449428269359\n",
            "Epoch : 122, val loss : 0.6733839323646145\n",
            "\n",
            "Epoch : 123, train loss : 0.6450948794682824\n",
            "Epoch : 123, val loss : 0.6741788418669451\n",
            "\n",
            "Epoch : 124, train loss : 0.6446641241059161\n",
            "Epoch : 124, val loss : 0.6656187772750854\n",
            "\n",
            "Epoch : 125, train loss : 0.6438725514845415\n",
            "Epoch : 125, val loss : 0.6686428597098903\n",
            "\n",
            "Epoch : 126, train loss : 0.6448415846535652\n",
            "Epoch : 126, val loss : 0.6654843186077317\n",
            "\n",
            "Epoch : 127, train loss : 0.6445351257468716\n",
            "Epoch : 127, val loss : 0.6749282479286194\n",
            "\n",
            "Epoch : 128, train loss : 0.6432693569949178\n",
            "Epoch : 128, val loss : 0.6678455534734224\n",
            "\n",
            "Epoch : 129, train loss : 0.6444377602952903\n",
            "Epoch : 129, val loss : 0.667295986100247\n",
            "\n",
            "Epoch : 130, train loss : 0.6441656730391767\n",
            "Epoch : 130, val loss : 0.6689561448599163\n",
            "\n",
            "Epoch : 131, train loss : 0.6431984569087174\n",
            "Epoch : 131, val loss : 0.6727805451342934\n",
            "\n",
            "Epoch : 132, train loss : 0.6433605241053031\n",
            "Epoch : 132, val loss : 0.6732185671204015\n",
            "\n",
            "Epoch : 133, train loss : 0.6420961856842047\n",
            "Epoch : 133, val loss : 0.67014931063903\n",
            "\n",
            "Epoch : 134, train loss : 0.6421938220659894\n",
            "Epoch : 134, val loss : 0.6737568566673681\n",
            "\n",
            "Epoch : 135, train loss : 0.6421096563339235\n",
            "Epoch : 135, val loss : 0.6689249873161315\n",
            "\n",
            "Epoch : 136, train loss : 0.6413205143177148\n",
            "Epoch : 136, val loss : 0.67057897856361\n",
            "\n",
            "Epoch : 137, train loss : 0.6417066574096679\n",
            "Epoch : 137, val loss : 0.6676298411268937\n",
            "\n",
            "Epoch : 138, train loss : 0.6415070839000475\n",
            "Epoch : 138, val loss : 0.6705870596986068\n",
            "\n",
            "Epoch : 139, train loss : 0.6415371463154302\n",
            "Epoch : 139, val loss : 0.6633199704320807\n",
            "\n",
            "Epoch : 140, train loss : 0.6406399531797929\n",
            "Epoch : 140, val loss : 0.6713804163430868\n",
            "\n",
            "Epoch : 141, train loss : 0.6399833617788372\n",
            "Epoch : 141, val loss : 0.6724801722325776\n",
            "\n",
            "Epoch : 142, train loss : 0.6410889957890368\n",
            "Epoch : 142, val loss : 0.6656432998807807\n",
            "\n",
            "Epoch : 143, train loss : 0.6399033145471055\n",
            "Epoch : 143, val loss : 0.6580664719405928\n",
            "\n",
            "Epoch : 144, train loss : 0.6406116380836023\n",
            "Epoch : 144, val loss : 0.6667965117253755\n",
            "\n",
            "Epoch : 145, train loss : 0.6386074542999266\n",
            "Epoch : 145, val loss : 0.6760226488113403\n",
            "\n",
            "Epoch : 146, train loss : 0.6418257214806293\n",
            "Epoch : 146, val loss : 0.6707747547250047\n",
            "\n",
            "Epoch : 147, train loss : 0.6395858643632945\n",
            "Epoch : 147, val loss : 0.6607770637462014\n",
            "\n",
            "Epoch : 148, train loss : 0.6373999900890122\n",
            "Epoch : 148, val loss : 0.6732937787708483\n",
            "\n",
            "Epoch : 149, train loss : 0.6384784936904904\n",
            "Epoch : 149, val loss : 0.6653165880002473\n",
            "\n",
            "Epoch : 150, train loss : 0.6396905548644789\n",
            "Epoch : 150, val loss : 0.6648407515726593\n",
            "\n",
            "Epoch : 151, train loss : 0.6387582827698101\n",
            "Epoch : 151, val loss : 0.6684234330528662\n",
            "\n",
            "Epoch : 152, train loss : 0.6369242404446458\n",
            "Epoch : 152, val loss : 0.6693972443279467\n",
            "\n",
            "Epoch : 153, train loss : 0.6374929283604476\n",
            "Epoch : 153, val loss : 0.6551965945645383\n",
            "\n",
            "Epoch : 154, train loss : 0.6361586209499475\n",
            "Epoch : 154, val loss : 0.6815955513402034\n",
            "\n",
            "Epoch : 155, train loss : 0.6391595251632458\n",
            "Epoch : 155, val loss : 0.662077828457481\n",
            "\n",
            "Epoch : 156, train loss : 0.6376484522313783\n",
            "Epoch : 156, val loss : 0.6640783893434624\n",
            "\n",
            "Epoch : 157, train loss : 0.6378727143461055\n",
            "Epoch : 157, val loss : 0.6571094299617568\n",
            "\n",
            "Epoch : 158, train loss : 0.6367127920642043\n",
            "Epoch : 158, val loss : 0.6631082327742326\n",
            "\n",
            "Epoch : 159, train loss : 0.637979886206714\n",
            "Epoch : 159, val loss : 0.663051608361696\n",
            "\n",
            "Epoch : 160, train loss : 0.6367335724108147\n",
            "Epoch : 160, val loss : 0.6596101679300007\n",
            "\n",
            "Epoch : 161, train loss : 0.6361193703882622\n",
            "Epoch : 161, val loss : 0.6563783664452401\n",
            "\n",
            "Epoch : 162, train loss : 0.6355712924942823\n",
            "Epoch : 162, val loss : 0.6664229662794817\n",
            "\n",
            "Epoch : 163, train loss : 0.636822045752496\n",
            "Epoch : 163, val loss : 0.663380764032665\n",
            "\n",
            "Epoch : 164, train loss : 0.6356531224467538\n",
            "Epoch : 164, val loss : 0.6600792439360368\n",
            "\n",
            "Epoch : 165, train loss : 0.6357819470492274\n",
            "Epoch : 165, val loss : 0.6618052689652693\n",
            "\n",
            "Epoch : 166, train loss : 0.6352604903958057\n",
            "Epoch : 166, val loss : 0.6640811782134206\n",
            "\n",
            "Epoch : 167, train loss : 0.6345586928454315\n",
            "Epoch : 167, val loss : 0.6619340306834172\n",
            "\n",
            "Epoch : 168, train loss : 0.6346610370910529\n",
            "Epoch : 168, val loss : 0.6644342824032432\n",
            "\n",
            "Epoch : 169, train loss : 0.6347155558340479\n",
            "Epoch : 169, val loss : 0.660954591475035\n",
            "\n",
            "Epoch : 170, train loss : 0.6334356784820558\n",
            "Epoch : 170, val loss : 0.6665982478543332\n",
            "\n",
            "Epoch : 171, train loss : 0.6339915644038805\n",
            "Epoch : 171, val loss : 0.6572411342671042\n",
            "\n",
            "Epoch : 172, train loss : 0.6343345690857279\n",
            "Epoch : 172, val loss : 0.6655708833744651\n",
            "\n",
            "Epoch : 173, train loss : 0.6344072728446041\n",
            "Epoch : 173, val loss : 0.6648509502410889\n",
            "\n",
            "Epoch : 174, train loss : 0.6325418076731939\n",
            "Epoch : 174, val loss : 0.6572780107197009\n",
            "\n",
            "Epoch : 175, train loss : 0.6336397653276271\n",
            "Epoch : 175, val loss : 0.6581113150245265\n",
            "\n",
            "Epoch : 176, train loss : 0.6328207911867084\n",
            "Epoch : 176, val loss : 0.6681092036397833\n",
            "\n",
            "Epoch : 177, train loss : 0.6317849439201938\n",
            "Epoch : 177, val loss : 0.6741270297452024\n",
            "\n",
            "Epoch : 178, train loss : 0.6322824857451697\n",
            "Epoch : 178, val loss : 0.6691272133275082\n",
            "\n",
            "Epoch : 179, train loss : 0.631678415789749\n",
            "Epoch : 179, val loss : 0.6657277816220333\n",
            "\n",
            "Epoch : 180, train loss : 0.6320432825521985\n",
            "Epoch : 180, val loss : 0.6640607495056956\n",
            "\n",
            "Epoch : 181, train loss : 0.6337909752672368\n",
            "Epoch : 181, val loss : 0.6658870013136612\n",
            "\n",
            "Epoch : 182, train loss : 0.6319171369075777\n",
            "Epoch : 182, val loss : 0.6807683988621361\n",
            "\n",
            "Epoch : 183, train loss : 0.6310300158731863\n",
            "Epoch : 183, val loss : 0.6574961009778475\n",
            "\n",
            "Epoch : 184, train loss : 0.6325880001891745\n",
            "Epoch : 184, val loss : 0.6609931274464255\n",
            "\n",
            "Epoch : 185, train loss : 0.6301249749732741\n",
            "Epoch : 185, val loss : 0.6680729546045002\n",
            "\n",
            "Epoch : 186, train loss : 0.6314072746219058\n",
            "Epoch : 186, val loss : 0.6657221254549528\n",
            "\n",
            "Epoch : 187, train loss : 0.629212954911319\n",
            "Epoch : 187, val loss : 0.6684070386384663\n",
            "\n",
            "Epoch : 188, train loss : 0.631513888005054\n",
            "Epoch : 188, val loss : 0.664766151654093\n",
            "\n",
            "Epoch : 189, train loss : 0.6305716743974974\n",
            "Epoch : 189, val loss : 0.6619234837983783\n",
            "\n",
            "Epoch : 190, train loss : 0.6301045439460062\n",
            "Epoch : 190, val loss : 0.6656997266568636\n",
            "\n",
            "Epoch : 191, train loss : 0.6291050166794745\n",
            "Epoch : 191, val loss : 0.6597368497597544\n",
            "\n",
            "Epoch : 192, train loss : 0.6288072578834767\n",
            "Epoch : 192, val loss : 0.6598224796746907\n",
            "\n",
            "Epoch : 193, train loss : 0.6293962834459367\n",
            "Epoch : 193, val loss : 0.6576622755903948\n",
            "\n",
            "Epoch : 194, train loss : 0.6292910978649602\n",
            "Epoch : 194, val loss : 0.6536368570829693\n",
            "\n",
            "Epoch : 195, train loss : 0.6288859183138064\n",
            "Epoch : 195, val loss : 0.6620900756434389\n",
            "\n",
            "Epoch : 196, train loss : 0.6289150308478964\n",
            "Epoch : 196, val loss : 0.6657716192697224\n",
            "\n",
            "Epoch : 197, train loss : 0.6306198329636548\n",
            "Epoch : 197, val loss : 0.6585416386001988\n",
            "\n",
            "Epoch : 198, train loss : 0.6279783826885801\n",
            "Epoch : 198, val loss : 0.6510161757469176\n",
            "\n",
            "Epoch : 199, train loss : 0.6298391701597159\n",
            "Epoch : 199, val loss : 0.6559418157527321\n",
            "\n",
            "Epoch : 200, train loss : 0.6301578384457212\n",
            "Epoch : 200, val loss : 0.6583192411221956\n",
            "\n",
            "Epoch : 201, train loss : 0.628599632870067\n",
            "Epoch : 201, val loss : 0.6545927399083189\n",
            "\n",
            "Epoch : 202, train loss : 0.6284409304459886\n",
            "Epoch : 202, val loss : 0.654415249824524\n",
            "\n",
            "Epoch : 203, train loss : 0.6282384411855175\n",
            "Epoch : 203, val loss : 0.6573041112799395\n",
            "\n",
            "Epoch : 204, train loss : 0.6278656771688748\n",
            "Epoch : 204, val loss : 0.662515568105798\n",
            "\n",
            "Epoch : 205, train loss : 0.6272576198433383\n",
            "Epoch : 205, val loss : 0.6572898845923574\n",
            "\n",
            "Epoch : 206, train loss : 0.627665584556985\n",
            "Epoch : 206, val loss : 0.65233524222123\n",
            "\n",
            "Epoch : 207, train loss : 0.6283056208581632\n",
            "Epoch : 207, val loss : 0.6555923223495485\n",
            "\n",
            "Epoch : 208, train loss : 0.62712837602153\n",
            "Epoch : 208, val loss : 0.6598257987122786\n",
            "\n",
            "Epoch : 209, train loss : 0.6257024004603877\n",
            "Epoch : 209, val loss : 0.6641158587054203\n",
            "\n",
            "Epoch : 210, train loss : 0.6279470907919334\n",
            "Epoch : 210, val loss : 0.6580940030123058\n",
            "\n",
            "Epoch : 211, train loss : 0.6270704675804488\n",
            "Epoch : 211, val loss : 0.6458265859829752\n",
            "\n",
            "Epoch : 212, train loss : 0.6260167279026725\n",
            "Epoch : 212, val loss : 0.6519948840141295\n",
            "\n",
            "Epoch : 213, train loss : 0.6258191646951614\n",
            "Epoch : 213, val loss : 0.6486893045274835\n",
            "\n",
            "Epoch : 214, train loss : 0.6255713536883851\n",
            "Epoch : 214, val loss : 0.6572170649704181\n",
            "\n",
            "Epoch : 215, train loss : 0.6259543781930751\n",
            "Epoch : 215, val loss : 0.6607968399399206\n",
            "\n",
            "Epoch : 216, train loss : 0.625446256724271\n",
            "Epoch : 216, val loss : 0.6523325568751284\n",
            "\n",
            "Epoch : 217, train loss : 0.6255188586133896\n",
            "Epoch : 217, val loss : 0.6482167683149638\n",
            "\n",
            "Epoch : 218, train loss : 0.6255703042853963\n",
            "Epoch : 218, val loss : 0.6450717731526024\n",
            "\n",
            "Epoch : 219, train loss : 0.6253327239643446\n",
            "Epoch : 219, val loss : 0.6544733518048336\n",
            "\n",
            "Epoch : 220, train loss : 0.6251217679543928\n",
            "Epoch : 220, val loss : 0.6539554094013413\n",
            "\n",
            "Epoch : 221, train loss : 0.6251231162837058\n",
            "Epoch : 221, val loss : 0.6553294250839635\n",
            "\n",
            "Epoch : 222, train loss : 0.6254150197361452\n",
            "Epoch : 222, val loss : 0.6498541016327708\n",
            "\n",
            "Epoch : 223, train loss : 0.6246245644309305\n",
            "Epoch : 223, val loss : 0.6520677553979973\n",
            "\n",
            "Epoch : 224, train loss : 0.6242000950105265\n",
            "Epoch : 224, val loss : 0.6421191472756234\n",
            "\n",
            "Epoch : 225, train loss : 0.6266559138442528\n",
            "Epoch : 225, val loss : 0.6651497232286554\n",
            "\n",
            "Epoch : 226, train loss : 0.62529388738401\n",
            "Epoch : 226, val loss : 0.6623485653023972\n",
            "\n",
            "Epoch : 227, train loss : 0.6223236053278952\n",
            "Epoch : 227, val loss : 0.6520316475316099\n",
            "\n",
            "Epoch : 228, train loss : 0.624459844647032\n",
            "Epoch : 228, val loss : 0.6475434585621483\n",
            "\n",
            "Epoch : 229, train loss : 0.6248887266173507\n",
            "Epoch : 229, val loss : 0.6650050470703526\n",
            "\n",
            "Epoch : 230, train loss : 0.6232073491269892\n",
            "Epoch : 230, val loss : 0.645305700992283\n",
            "\n",
            "Epoch : 231, train loss : 0.6241066605755778\n",
            "Epoch : 231, val loss : 0.6541057919201099\n",
            "\n",
            "Epoch : 232, train loss : 0.6258819466287435\n",
            "Epoch : 232, val loss : 0.6579996347427368\n",
            "\n",
            "Epoch : 233, train loss : 0.6229595713543169\n",
            "Epoch : 233, val loss : 0.6513658385527762\n",
            "\n",
            "Epoch : 234, train loss : 0.6231931868827707\n",
            "Epoch : 234, val loss : 0.6561855956127769\n",
            "\n",
            "Epoch : 235, train loss : 0.622425663651842\n",
            "Epoch : 235, val loss : 0.6431627587268227\n",
            "\n",
            "Epoch : 236, train loss : 0.6216821504361701\n",
            "Epoch : 236, val loss : 0.6546785611855357\n",
            "\n",
            "Epoch : 237, train loss : 0.6239140698404022\n",
            "Epoch : 237, val loss : 0.6491584746461165\n",
            "\n",
            "Epoch : 238, train loss : 0.6225986397627628\n",
            "Epoch : 238, val loss : 0.6558211194841485\n",
            "\n",
            "Epoch : 239, train loss : 0.6221969754406905\n",
            "Epoch : 239, val loss : 0.6512944510108545\n",
            "\n",
            "Epoch : 240, train loss : 0.6202033172954212\n",
            "Epoch : 240, val loss : 0.6472369401078475\n",
            "\n",
            "Epoch : 241, train loss : 0.6202105625109243\n",
            "Epoch : 241, val loss : 0.6457128305184213\n",
            "\n",
            "Epoch : 242, train loss : 0.6211115855159184\n",
            "Epoch : 242, val loss : 0.644958977636538\n",
            "\n",
            "Epoch : 243, train loss : 0.6212496109081038\n",
            "Epoch : 243, val loss : 0.6441625795866313\n",
            "\n",
            "Epoch : 244, train loss : 0.6235079895366321\n",
            "Epoch : 244, val loss : 0.6517281500916732\n",
            "\n",
            "Epoch : 245, train loss : 0.6206885800217139\n",
            "Epoch : 245, val loss : 0.6558315973532827\n",
            "\n",
            "Epoch : 246, train loss : 0.6213397853302234\n",
            "Epoch : 246, val loss : 0.6463184952735902\n",
            "\n",
            "Epoch : 247, train loss : 0.621196602330063\n",
            "Epoch : 247, val loss : 0.6565801093452854\n",
            "\n",
            "Epoch : 248, train loss : 0.6202246810450698\n",
            "Epoch : 248, val loss : 0.6480427980422974\n",
            "\n",
            "Epoch : 249, train loss : 0.6202613973256312\n",
            "Epoch : 249, val loss : 0.645751278651388\n",
            "\n",
            "Epoch : 250, train loss : 0.6193999313946925\n",
            "Epoch : 250, val loss : 0.6458496482748735\n",
            "\n",
            "Epoch : 251, train loss : 0.6195583012971007\n",
            "Epoch : 251, val loss : 0.6472981634892916\n",
            "\n",
            "Epoch : 252, train loss : 0.6208385109901428\n",
            "Epoch : 252, val loss : 0.6431194810490861\n",
            "\n",
            "Epoch : 253, train loss : 0.6194515562418738\n",
            "Epoch : 253, val loss : 0.6545272595004031\n",
            "\n",
            "Epoch : 254, train loss : 0.6187418773318782\n",
            "Epoch : 254, val loss : 0.6490682143914073\n",
            "\n",
            "Epoch : 255, train loss : 0.6190905386751349\n",
            "Epoch : 255, val loss : 0.6475069209149009\n",
            "\n",
            "Epoch : 256, train loss : 0.6214495315696259\n",
            "Epoch : 256, val loss : 0.6449002560816313\n",
            "\n",
            "Epoch : 257, train loss : 0.6198766963048413\n",
            "Epoch : 257, val loss : 0.643921883482682\n",
            "\n",
            "Epoch : 258, train loss : 0.6196965177853899\n",
            "Epoch : 258, val loss : 0.6435383715127645\n",
            "\n",
            "Epoch : 259, train loss : 0.6180757260683813\n",
            "Epoch : 259, val loss : 0.6558941947786431\n",
            "\n",
            "Epoch : 260, train loss : 0.6184659183025362\n",
            "Epoch : 260, val loss : 0.6526310443878174\n",
            "\n",
            "Epoch : 261, train loss : 0.6192557920109141\n",
            "Epoch : 261, val loss : 0.6490466469212584\n",
            "\n",
            "Epoch : 262, train loss : 0.6191138175400821\n",
            "Epoch : 262, val loss : 0.6452462390849464\n",
            "\n",
            "Epoch : 263, train loss : 0.6200832065307731\n",
            "Epoch : 263, val loss : 0.6465592933328529\n",
            "\n",
            "Epoch : 264, train loss : 0.6180462774002191\n",
            "Epoch : 264, val loss : 0.6399702931705274\n",
            "\n",
            "Epoch : 265, train loss : 0.6192512161803966\n",
            "Epoch : 265, val loss : 0.6499348878860475\n",
            "\n",
            "Epoch : 266, train loss : 0.6183639855095836\n",
            "Epoch : 266, val loss : 0.6464557647705078\n",
            "\n",
            "Epoch : 267, train loss : 0.6167393245480279\n",
            "Epoch : 267, val loss : 0.6373045099409004\n",
            "\n",
            "Epoch : 268, train loss : 0.6177018073472107\n",
            "Epoch : 268, val loss : 0.6578664795348519\n",
            "\n",
            "Epoch : 269, train loss : 0.6175101061662039\n",
            "Epoch : 269, val loss : 0.6556361725455835\n",
            "\n",
            "Epoch : 270, train loss : 0.6166055146491889\n",
            "Epoch : 270, val loss : 0.6585750752373745\n",
            "\n",
            "Epoch : 271, train loss : 0.617013570034143\n",
            "Epoch : 271, val loss : 0.6454791050208242\n",
            "\n",
            "Epoch : 272, train loss : 0.6163234701662356\n",
            "Epoch : 272, val loss : 0.6533625031772413\n",
            "\n",
            "Epoch : 273, train loss : 0.6160296709248513\n",
            "Epoch : 273, val loss : 0.6545520424842834\n",
            "\n",
            "Epoch : 274, train loss : 0.6173422854958156\n",
            "Epoch : 274, val loss : 0.6401629965556296\n",
            "\n",
            "Epoch : 275, train loss : 0.6169131743185446\n",
            "Epoch : 275, val loss : 0.6340040423368153\n",
            "\n",
            "Epoch : 276, train loss : 0.617409587628914\n",
            "Epoch : 276, val loss : 0.6572109053009435\n",
            "\n",
            "Epoch : 277, train loss : 0.6168277529152957\n",
            "Epoch : 277, val loss : 0.6383872047850959\n",
            "\n",
            "Epoch : 278, train loss : 0.6164075378215673\n",
            "Epoch : 278, val loss : 0.6502557240034404\n",
            "\n",
            "Epoch : 279, train loss : 0.6162032488620643\n",
            "Epoch : 279, val loss : 0.6412167658931329\n",
            "\n",
            "Epoch : 280, train loss : 0.6162413739796841\n",
            "Epoch : 280, val loss : 0.6483098990038821\n",
            "\n",
            "Epoch : 281, train loss : 0.6159903522693752\n",
            "Epoch : 281, val loss : 0.6434884981105202\n",
            "\n",
            "Epoch : 282, train loss : 0.616653489705288\n",
            "Epoch : 282, val loss : 0.6412811012644516\n",
            "\n",
            "Epoch : 283, train loss : 0.6155038177967069\n",
            "Epoch : 283, val loss : 0.6504203677177429\n",
            "\n",
            "Epoch : 284, train loss : 0.6160724840380927\n",
            "Epoch : 284, val loss : 0.6495804065152218\n",
            "\n",
            "Epoch : 285, train loss : 0.6162466887271765\n",
            "Epoch : 285, val loss : 0.6495923776375621\n",
            "\n",
            "Epoch : 286, train loss : 0.6145500114469813\n",
            "Epoch : 286, val loss : 0.6452851766034177\n",
            "\n",
            "Epoch : 287, train loss : 0.6148667891820271\n",
            "Epoch : 287, val loss : 0.6445203894063046\n",
            "\n",
            "Epoch : 288, train loss : 0.6155549419648715\n",
            "Epoch : 288, val loss : 0.6424204738516555\n",
            "\n",
            "Epoch : 289, train loss : 0.6150717663042475\n",
            "Epoch : 289, val loss : 0.6470816637340344\n",
            "\n",
            "Epoch : 290, train loss : 0.6131767794941408\n",
            "Epoch : 290, val loss : 0.6435700560870924\n",
            "\n",
            "Epoch : 291, train loss : 0.6159503382263765\n",
            "Epoch : 291, val loss : 0.6496550848609521\n",
            "\n",
            "Epoch : 292, train loss : 0.6135015805562337\n",
            "Epoch : 292, val loss : 0.6424771170867116\n",
            "\n",
            "Epoch : 293, train loss : 0.6147850063714118\n",
            "Epoch : 293, val loss : 0.6465373698033784\n",
            "\n",
            "Epoch : 294, train loss : 0.6132186017253183\n",
            "Epoch : 294, val loss : 0.6419172506583365\n",
            "\n",
            "Epoch : 295, train loss : 0.6139785912903877\n",
            "Epoch : 295, val loss : 0.6490758845680638\n",
            "\n",
            "Epoch : 296, train loss : 0.6135738531748451\n",
            "Epoch : 296, val loss : 0.6431016074983698\n",
            "\n",
            "Epoch : 297, train loss : 0.6122590809157402\n",
            "Epoch : 297, val loss : 0.6400959836809259\n",
            "\n",
            "Epoch : 298, train loss : 0.6126112452059084\n",
            "Epoch : 298, val loss : 0.6563376815695512\n",
            "\n",
            "Epoch : 299, train loss : 0.6126957248557698\n",
            "Epoch : 299, val loss : 0.6487302654667905\n",
            "\n",
            "Epoch : 300, train loss : 0.6128743776769348\n",
            "Epoch : 300, val loss : 0.6521691538785634\n",
            "\n",
            "Epoch : 301, train loss : 0.6134087288018434\n",
            "Epoch : 301, val loss : 0.653732905262395\n",
            "\n",
            "Epoch : 302, train loss : 0.6136512348146147\n",
            "Epoch : 302, val loss : 0.6620174583635832\n",
            "\n",
            "Epoch : 303, train loss : 0.6114401649345053\n",
            "Epoch : 303, val loss : 0.6487417252440203\n",
            "\n",
            "Epoch : 304, train loss : 0.6141724149386087\n",
            "Epoch : 304, val loss : 0.6360636541717931\n",
            "\n",
            "Epoch : 305, train loss : 0.6134841375278705\n",
            "Epoch : 305, val loss : 0.6519564609778554\n",
            "\n",
            "Epoch : 306, train loss : 0.613353705586809\n",
            "Epoch : 306, val loss : 0.6454295766981025\n",
            "\n",
            "Epoch : 307, train loss : 0.6119169321927157\n",
            "Epoch : 307, val loss : 0.6415236247213263\n",
            "\n",
            "Epoch : 308, train loss : 0.6126463185657157\n",
            "Epoch : 308, val loss : 0.6459290698954934\n",
            "\n",
            "Epoch : 309, train loss : 0.6130336517637426\n",
            "Epoch : 309, val loss : 0.6429682060291895\n",
            "\n",
            "Epoch : 310, train loss : 0.6120388724587185\n",
            "Epoch : 310, val loss : 0.6419316279260736\n",
            "\n",
            "Epoch : 311, train loss : 0.6100737044305513\n",
            "Epoch : 311, val loss : 0.645801798293465\n",
            "\n",
            "Epoch : 312, train loss : 0.6133390488046587\n",
            "Epoch : 312, val loss : 0.6333076138245431\n",
            "\n",
            "Epoch : 313, train loss : 0.6141164842880132\n",
            "Epoch : 313, val loss : 0.6445093562728479\n",
            "\n",
            "Epoch : 314, train loss : 0.6108672640540381\n",
            "Epoch : 314, val loss : 0.6540681625667372\n",
            "\n",
            "Epoch : 315, train loss : 0.611470588048299\n",
            "Epoch : 315, val loss : 0.63308006681894\n",
            "\n",
            "Epoch : 316, train loss : 0.6118480718497077\n",
            "Epoch : 316, val loss : 0.6390096043285572\n",
            "\n",
            "Epoch : 317, train loss : 0.6114127867149584\n",
            "Epoch : 317, val loss : 0.6483406895085384\n",
            "\n",
            "Epoch : 318, train loss : 0.6112691676977907\n",
            "Epoch : 318, val loss : 0.6425284021779111\n",
            "\n",
            "Epoch : 319, train loss : 0.6114808729200651\n",
            "Epoch : 319, val loss : 0.640506436950282\n",
            "\n",
            "Epoch : 320, train loss : 0.6113821430639783\n",
            "Epoch : 320, val loss : 0.649542134059103\n",
            "\n",
            "Epoch : 321, train loss : 0.6100750321691688\n",
            "Epoch : 321, val loss : 0.6437272460837113\n",
            "\n",
            "Epoch : 322, train loss : 0.6095384447863608\n",
            "Epoch : 322, val loss : 0.6542736636964898\n",
            "\n",
            "Epoch : 323, train loss : 0.6107607482057631\n",
            "Epoch : 323, val loss : 0.6320540120727137\n",
            "\n",
            "Epoch : 324, train loss : 0.6115028872634425\n",
            "Epoch : 324, val loss : 0.6379128380825646\n",
            "\n",
            "Epoch : 325, train loss : 0.6095647692680357\n",
            "Epoch : 325, val loss : 0.6522063079633211\n",
            "\n",
            "Epoch : 326, train loss : 0.6116783940430842\n",
            "Epoch : 326, val loss : 0.6428840379965934\n",
            "\n",
            "Epoch : 327, train loss : 0.6119755658236419\n",
            "Epoch : 327, val loss : 0.636473276113209\n",
            "\n",
            "Epoch : 328, train loss : 0.610033468947266\n",
            "Epoch : 328, val loss : 0.6373470331493178\n",
            "\n",
            "Epoch : 329, train loss : 0.6120330443888001\n",
            "Epoch : 329, val loss : 0.6450181572060836\n",
            "\n",
            "Epoch : 330, train loss : 0.6080454067750409\n",
            "Epoch : 330, val loss : 0.6405113684503657\n",
            "\n",
            "Epoch : 331, train loss : 0.609099441766739\n",
            "Epoch : 331, val loss : 0.6452319120105945\n",
            "\n",
            "Epoch : 332, train loss : 0.6091707908745965\n",
            "Epoch : 332, val loss : 0.6405537630382336\n",
            "\n",
            "Epoch : 333, train loss : 0.6090525748151724\n",
            "Epoch : 333, val loss : 0.6343005676018564\n",
            "\n",
            "Epoch : 334, train loss : 0.6084778482263739\n",
            "Epoch : 334, val loss : 0.6359660531345165\n",
            "\n",
            "Epoch : 335, train loss : 0.6089357139486256\n",
            "Epoch : 335, val loss : 0.6494390023382085\n",
            "\n",
            "Epoch : 336, train loss : 0.6084792213006449\n",
            "Epoch : 336, val loss : 0.6390898651198337\n",
            "\n",
            "Epoch : 337, train loss : 0.6092045090415261\n",
            "Epoch : 337, val loss : 0.6468073349249989\n",
            "\n",
            "Epoch : 338, train loss : 0.6091040716026768\n",
            "Epoch : 338, val loss : 0.6385046620117991\n",
            "\n",
            "Epoch : 339, train loss : 0.6088410823634177\n",
            "Epoch : 339, val loss : 0.6419073907952558\n",
            "\n",
            "Epoch : 340, train loss : 0.6096949371424587\n",
            "Epoch : 340, val loss : 0.64689743832538\n",
            "\n",
            "Epoch : 341, train loss : 0.609343027346062\n",
            "Epoch : 341, val loss : 0.6460267415172175\n",
            "\n",
            "Epoch : 342, train loss : 0.6079320345864149\n",
            "Epoch : 342, val loss : 0.6380505248119955\n",
            "\n",
            "Epoch : 343, train loss : 0.6086899573152716\n",
            "Epoch : 343, val loss : 0.6480262122656171\n",
            "\n",
            "Epoch : 344, train loss : 0.608361652041926\n",
            "Epoch : 344, val loss : 0.6489653399116114\n",
            "\n",
            "Epoch : 345, train loss : 0.6081885877883798\n",
            "Epoch : 345, val loss : 0.6358811745518134\n",
            "\n",
            "Epoch : 346, train loss : 0.6079742747725863\n",
            "Epoch : 346, val loss : 0.6337786065904717\n",
            "\n",
            "Epoch : 347, train loss : 0.6068791608015695\n",
            "Epoch : 347, val loss : 0.6348755861583508\n",
            "\n",
            "Epoch : 348, train loss : 0.6079699070164647\n",
            "Epoch : 348, val loss : 0.6412478105018014\n",
            "\n",
            "Epoch : 349, train loss : 0.6079995393753049\n",
            "Epoch : 349, val loss : 0.6361525999872307\n",
            "\n",
            "Epoch : 350, train loss : 0.6079559862613676\n",
            "Epoch : 350, val loss : 0.6394904255867004\n",
            "\n",
            "Epoch : 351, train loss : 0.6070744413318051\n",
            "Epoch : 351, val loss : 0.6417278851333419\n",
            "\n",
            "Epoch : 352, train loss : 0.606507947589412\n",
            "Epoch : 352, val loss : 0.6424973732546757\n",
            "\n",
            "Epoch : 353, train loss : 0.6070767158811743\n",
            "Epoch : 353, val loss : 0.6408480844999613\n",
            "\n",
            "Epoch : 354, train loss : 0.6071201725439592\n",
            "Epoch : 354, val loss : 0.6387714869097662\n",
            "\n",
            "Epoch : 355, train loss : 0.6075238354278333\n",
            "Epoch : 355, val loss : 0.6504891514778137\n",
            "\n",
            "Epoch : 356, train loss : 0.6072389488870447\n",
            "Epoch : 356, val loss : 0.6375653837856494\n",
            "\n",
            "Epoch : 357, train loss : 0.6065941234429683\n",
            "Epoch : 357, val loss : 0.6338976245177419\n",
            "\n",
            "Epoch : 358, train loss : 0.6068502082969198\n",
            "Epoch : 358, val loss : 0.6483770608901978\n",
            "\n",
            "Epoch : 359, train loss : 0.6048368977777885\n",
            "Epoch : 359, val loss : 0.6456950746084514\n",
            "\n",
            "Epoch : 360, train loss : 0.6060686073519965\n",
            "Epoch : 360, val loss : 0.6525360408582185\n",
            "\n",
            "Epoch : 361, train loss : 0.6052519032449434\n",
            "Epoch : 361, val loss : 0.6335257542760748\n",
            "\n",
            "Epoch : 362, train loss : 0.6076044131409037\n",
            "Epoch : 362, val loss : 0.6378053144404763\n",
            "\n",
            "Epoch : 363, train loss : 0.6052864096381448\n",
            "Epoch : 363, val loss : 0.643560379743576\n",
            "\n",
            "Epoch : 364, train loss : 0.6062775138652685\n",
            "Epoch : 364, val loss : 0.6412880703022605\n",
            "\n",
            "Epoch : 365, train loss : 0.6055766959985102\n",
            "Epoch : 365, val loss : 0.6440158172657616\n",
            "\n",
            "Epoch : 366, train loss : 0.6049002997802966\n",
            "Epoch : 366, val loss : 0.6462705950987967\n",
            "\n",
            "Epoch : 367, train loss : 0.605951517639738\n",
            "Epoch : 367, val loss : 0.6317922049447109\n",
            "\n",
            "Epoch : 368, train loss : 0.6065513379646071\n",
            "Epoch : 368, val loss : 0.6296815825136084\n",
            "\n",
            "Epoch : 369, train loss : 0.60629896106142\n",
            "Epoch : 369, val loss : 0.6371786594390868\n",
            "\n",
            "Epoch : 370, train loss : 0.6048446906335427\n",
            "Epoch : 370, val loss : 0.6349618058455619\n",
            "\n",
            "Epoch : 371, train loss : 0.6052674851634289\n",
            "Epoch : 371, val loss : 0.6449095638174761\n",
            "\n",
            "Epoch : 372, train loss : 0.6081735491752625\n",
            "Epoch : 372, val loss : 0.6334188423658671\n",
            "\n",
            "Epoch : 373, train loss : 0.6046076225511962\n",
            "Epoch : 373, val loss : 0.6414002838887666\n",
            "\n",
            "Epoch : 374, train loss : 0.6055222401113223\n",
            "Epoch : 374, val loss : 0.6451566125217236\n",
            "\n",
            "Epoch : 375, train loss : 0.6043485012921421\n",
            "Epoch : 375, val loss : 0.6281772491179014\n",
            "\n",
            "Epoch : 376, train loss : 0.6048438848871175\n",
            "Epoch : 376, val loss : 0.6443945734124434\n",
            "\n",
            "Epoch : 377, train loss : 0.6051463506438511\n",
            "Epoch : 377, val loss : 0.6434790805766457\n",
            "\n",
            "Epoch : 378, train loss : 0.6036269121097797\n",
            "Epoch : 378, val loss : 0.6317371537810879\n",
            "\n",
            "Epoch : 379, train loss : 0.6022408425807952\n",
            "Epoch : 379, val loss : 0.6499817308626678\n",
            "\n",
            "Epoch : 380, train loss : 0.6055575865687745\n",
            "Epoch : 380, val loss : 0.6493813207274989\n",
            "\n",
            "Epoch : 381, train loss : 0.6050713013518942\n",
            "Epoch : 381, val loss : 0.6286708885117581\n",
            "\n",
            "Epoch : 382, train loss : 0.6039062521674416\n",
            "Epoch : 382, val loss : 0.6405935648240542\n",
            "\n",
            "Epoch : 383, train loss : 0.6042874529506223\n",
            "Epoch : 383, val loss : 0.6376510685996005\n",
            "\n",
            "Epoch : 384, train loss : 0.604030421105298\n",
            "Epoch : 384, val loss : 0.6309038432020889\n",
            "\n",
            "Epoch : 385, train loss : 0.6047101221301343\n",
            "Epoch : 385, val loss : 0.6346677478991056\n",
            "\n",
            "Epoch : 386, train loss : 0.6034904566678138\n",
            "Epoch : 386, val loss : 0.6449627593943947\n",
            "\n",
            "Epoch : 387, train loss : 0.6043205216075436\n",
            "Epoch : 387, val loss : 0.6471728949170364\n",
            "\n",
            "Epoch : 388, train loss : 0.6034298810091887\n",
            "Epoch : 388, val loss : 0.6262757762482293\n",
            "\n",
            "Epoch : 389, train loss : 0.6020385324954991\n",
            "Epoch : 389, val loss : 0.6252359418492568\n",
            "\n",
            "Epoch : 390, train loss : 0.6026835645690113\n",
            "Epoch : 390, val loss : 0.6226221699463692\n",
            "\n",
            "Epoch : 391, train loss : 0.6025228415474749\n",
            "Epoch : 391, val loss : 0.62338913898719\n",
            "\n",
            "Epoch : 392, train loss : 0.6035095521897982\n",
            "Epoch : 392, val loss : 0.6360848561713569\n",
            "\n",
            "Epoch : 393, train loss : 0.6017437315348422\n",
            "Epoch : 393, val loss : 0.6371188838230936\n",
            "\n",
            "Epoch : 394, train loss : 0.6033981693513465\n",
            "Epoch : 394, val loss : 0.6293530715139289\n",
            "\n",
            "Epoch : 395, train loss : 0.6039562239791404\n",
            "Epoch : 395, val loss : 0.6328036785125731\n",
            "\n",
            "Epoch : 396, train loss : 0.6026276053804339\n",
            "Epoch : 396, val loss : 0.6313282376841496\n",
            "\n",
            "Epoch : 397, train loss : 0.602779760685834\n",
            "Epoch : 397, val loss : 0.6344073885365535\n",
            "\n",
            "Epoch : 398, train loss : 0.6031271824330996\n",
            "Epoch : 398, val loss : 0.6392274436197782\n",
            "\n",
            "Epoch : 399, train loss : 0.6021839871551052\n",
            "Epoch : 399, val loss : 0.6276865209403791\n",
            "\n",
            "Epoch : 400, train loss : 0.6039102258104267\n",
            "Epoch : 400, val loss : 0.6416398820124174\n",
            "\n",
            "Epoch : 401, train loss : 0.6018670365665899\n",
            "Epoch : 401, val loss : 0.6476810637273287\n",
            "\n",
            "Epoch : 402, train loss : 0.6034098421082349\n",
            "Epoch : 402, val loss : 0.6262828158704857\n",
            "\n",
            "Epoch : 403, train loss : 0.6034491602218515\n",
            "Epoch : 403, val loss : 0.6277007288054416\n",
            "\n",
            "Epoch : 404, train loss : 0.6017025060725935\n",
            "Epoch : 404, val loss : 0.6511806782923246\n",
            "\n",
            "Epoch : 405, train loss : 0.601342744177038\n",
            "Epoch : 405, val loss : 0.637745361579092\n",
            "\n",
            "Epoch : 406, train loss : 0.6035805122418842\n",
            "Epoch : 406, val loss : 0.6279124366609674\n",
            "\n",
            "Epoch : 407, train loss : 0.6004974274924304\n",
            "Epoch : 407, val loss : 0.634590782617268\n",
            "\n",
            "Epoch : 408, train loss : 0.6008462569930343\n",
            "Epoch : 408, val loss : 0.6268597213845505\n",
            "\n",
            "Epoch : 409, train loss : 0.6006183799469114\n",
            "Epoch : 409, val loss : 0.6258607255785089\n",
            "\n",
            "Epoch : 410, train loss : 0.6012018857580244\n",
            "Epoch : 410, val loss : 0.6258570520501388\n",
            "\n",
            "Epoch : 411, train loss : 0.5996689408114462\n",
            "Epoch : 411, val loss : 0.6365786822218643\n",
            "\n",
            "Epoch : 412, train loss : 0.6003741638226943\n",
            "Epoch : 412, val loss : 0.6284709541421186\n",
            "\n",
            "Epoch : 413, train loss : 0.6003337793277972\n",
            "Epoch : 413, val loss : 0.6295396779712878\n",
            "\n",
            "Epoch : 414, train loss : 0.6010740847298596\n",
            "Epoch : 414, val loss : 0.6347099354392605\n",
            "\n",
            "Epoch : 415, train loss : 0.6023331266460995\n",
            "Epoch : 415, val loss : 0.6403291554827439\n",
            "\n",
            "Epoch : 416, train loss : 0.6012063880761463\n",
            "Epoch : 416, val loss : 0.6334591304001056\n",
            "\n",
            "Epoch : 417, train loss : 0.5994590125300668\n",
            "Epoch : 417, val loss : 0.6296705791824744\n",
            "\n",
            "Epoch : 418, train loss : 0.6022763380498597\n",
            "Epoch : 418, val loss : 0.6344689946425588\n",
            "\n",
            "Epoch : 419, train loss : 0.601996457757372\n",
            "Epoch : 419, val loss : 0.6322091974710164\n",
            "\n",
            "Epoch : 420, train loss : 0.6001126195445207\n",
            "Epoch : 420, val loss : 0.633338407466286\n",
            "\n",
            "Epoch : 421, train loss : 0.6006661035797809\n",
            "Epoch : 421, val loss : 0.6346190697268437\n",
            "\n",
            "Epoch : 422, train loss : 0.5994902908802033\n",
            "Epoch : 422, val loss : 0.6424831120591414\n",
            "\n",
            "Epoch : 423, train loss : 0.6021128314914125\n",
            "Epoch : 423, val loss : 0.6327279523799294\n",
            "\n",
            "Epoch : 424, train loss : 0.6005721420952768\n",
            "Epoch : 424, val loss : 0.6432539193253768\n",
            "\n",
            "Epoch : 425, train loss : 0.5982448530919623\n",
            "Epoch : 425, val loss : 0.6214197199595601\n",
            "\n",
            "Epoch : 426, train loss : 0.6007333381609486\n",
            "Epoch : 426, val loss : 0.6444014985310403\n",
            "\n",
            "Epoch : 427, train loss : 0.5978416758956335\n",
            "Epoch : 427, val loss : 0.6396318909369016\n",
            "\n",
            "Epoch : 428, train loss : 0.5996578173203901\n",
            "Epoch : 428, val loss : 0.632579982280731\n",
            "\n",
            "Epoch : 429, train loss : 0.6008871795553151\n",
            "Epoch : 429, val loss : 0.6383021897391271\n",
            "\n",
            "Epoch : 430, train loss : 0.5994089076013276\n",
            "Epoch : 430, val loss : 0.6227926549158599\n",
            "\n",
            "Epoch : 431, train loss : 0.5991629251928042\n",
            "Epoch : 431, val loss : 0.6308484642129194\n",
            "\n",
            "Epoch : 432, train loss : 0.5980135870702341\n",
            "Epoch : 432, val loss : 0.6356999968227588\n",
            "\n",
            "Epoch : 433, train loss : 0.5999093359166924\n",
            "Epoch : 433, val loss : 0.6266497436322664\n",
            "\n",
            "Epoch : 434, train loss : 0.5996050085082196\n",
            "Epoch : 434, val loss : 0.6408635801390598\n",
            "\n",
            "Epoch : 435, train loss : 0.598523073124163\n",
            "Epoch : 435, val loss : 0.6375747526946821\n",
            "\n",
            "Epoch : 436, train loss : 0.5993123139395857\n",
            "Epoch : 436, val loss : 0.6286269991021408\n",
            "\n",
            "Epoch : 437, train loss : 0.5982919571977673\n",
            "Epoch : 437, val loss : 0.6315230538970545\n",
            "\n",
            "Epoch : 438, train loss : 0.598033913518443\n",
            "Epoch : 438, val loss : 0.628640027422654\n",
            "\n",
            "Epoch : 439, train loss : 0.6025302921280716\n",
            "Epoch : 439, val loss : 0.6366337224056847\n",
            "\n",
            "Epoch : 440, train loss : 0.5970395546970946\n",
            "Epoch : 440, val loss : 0.6266951372748927\n",
            "\n",
            "Epoch : 441, train loss : 0.5997472423495668\n",
            "Epoch : 441, val loss : 0.6271708717471675\n",
            "\n",
            "Epoch : 442, train loss : 0.5969219186089256\n",
            "Epoch : 442, val loss : 0.6142205595970154\n",
            "\n",
            "Epoch : 443, train loss : 0.5992556046355854\n",
            "Epoch : 443, val loss : 0.623243826000314\n",
            "\n",
            "Epoch : 444, train loss : 0.5964245946118327\n",
            "Epoch : 444, val loss : 0.6441119658319575\n",
            "\n",
            "Epoch : 445, train loss : 0.5972933476621453\n",
            "Epoch : 445, val loss : 0.6375614294880315\n",
            "\n",
            "Epoch : 446, train loss : 0.5986720451802922\n",
            "Epoch : 446, val loss : 0.6310282098619561\n",
            "\n",
            "Epoch : 447, train loss : 0.598499888904167\n",
            "Epoch : 447, val loss : 0.6198180757070841\n",
            "\n",
            "Epoch : 448, train loss : 0.597095395760103\n",
            "Epoch : 448, val loss : 0.6224740508355592\n",
            "\n",
            "Epoch : 449, train loss : 0.5985044078393412\n",
            "Epoch : 449, val loss : 0.6197829403375325\n",
            "\n",
            "Epoch : 450, train loss : 0.5974461683721256\n",
            "Epoch : 450, val loss : 0.6348088258191159\n",
            "\n",
            "Epoch : 451, train loss : 0.597726396900235\n",
            "Epoch : 451, val loss : 0.6177488957580767\n",
            "\n",
            "Epoch : 452, train loss : 0.5983625834638421\n",
            "Epoch : 452, val loss : 0.6414466766934646\n",
            "\n",
            "Epoch : 453, train loss : 0.5977359058278975\n",
            "Epoch : 453, val loss : 0.623296248285394\n",
            "\n",
            "Epoch : 454, train loss : 0.5984053577437543\n",
            "Epoch : 454, val loss : 0.6212793259244216\n",
            "\n",
            "Epoch : 455, train loss : 0.5988411789590661\n",
            "Epoch : 455, val loss : 0.6358152627944946\n",
            "\n",
            "Epoch : 456, train loss : 0.5958431679191007\n",
            "Epoch : 456, val loss : 0.6398016653562848\n",
            "\n",
            "Epoch : 457, train loss : 0.5962826494014623\n",
            "Epoch : 457, val loss : 0.6251792860658545\n",
            "\n",
            "Epoch : 458, train loss : 0.5959496702208664\n",
            "Epoch : 458, val loss : 0.6330620897443671\n",
            "\n",
            "Epoch : 459, train loss : 0.5966576476891841\n",
            "Epoch : 459, val loss : 0.6369730673338239\n",
            "\n",
            "Epoch : 460, train loss : 0.5961465795834863\n",
            "Epoch : 460, val loss : 0.6300428007778369\n",
            "\n",
            "Epoch : 461, train loss : 0.5971663993416416\n",
            "Epoch : 461, val loss : 0.6362726672699577\n",
            "\n",
            "Epoch : 462, train loss : 0.594692385377306\n",
            "Epoch : 462, val loss : 0.6344926953315735\n",
            "\n",
            "Epoch : 463, train loss : 0.5976406397241534\n",
            "Epoch : 463, val loss : 0.6299950135381598\n",
            "\n",
            "Epoch : 464, train loss : 0.5970781360611768\n",
            "Epoch : 464, val loss : 0.6288513792188546\n",
            "\n",
            "Epoch : 465, train loss : 0.5974575692957097\n",
            "Epoch : 465, val loss : 0.6196686939189309\n",
            "\n",
            "Epoch : 466, train loss : 0.5963960593396965\n",
            "Epoch : 466, val loss : 0.625425094052365\n",
            "\n",
            "Epoch : 467, train loss : 0.5975627855821091\n",
            "Epoch : 467, val loss : 0.6255058614831223\n",
            "\n",
            "Epoch : 468, train loss : 0.5972861385706699\n",
            "Epoch : 468, val loss : 0.6172028403533133\n",
            "\n",
            "Epoch : 469, train loss : 0.5957730264374708\n",
            "Epoch : 469, val loss : 0.6399725832437214\n",
            "\n",
            "Epoch : 470, train loss : 0.595542231834296\n",
            "Epoch : 470, val loss : 0.6327337936351175\n",
            "\n",
            "Epoch : 471, train loss : 0.5947616714419741\n",
            "Epoch : 471, val loss : 0.6194543995355305\n",
            "\n",
            "Epoch : 472, train loss : 0.5948774937427406\n",
            "Epoch : 472, val loss : 0.6381058771359293\n",
            "\n",
            "Epoch : 473, train loss : 0.5939040922757349\n",
            "Epoch : 473, val loss : 0.6252110976921884\n",
            "\n",
            "Epoch : 474, train loss : 0.5950938186862251\n",
            "Epoch : 474, val loss : 0.6272482777896681\n",
            "\n",
            "Epoch : 475, train loss : 0.595182092442657\n",
            "Epoch : 475, val loss : 0.6194796828847184\n",
            "\n",
            "Epoch : 476, train loss : 0.596214144518881\n",
            "Epoch : 476, val loss : 0.6300066223270016\n",
            "\n",
            "Epoch : 477, train loss : 0.5977117840087772\n",
            "Epoch : 477, val loss : 0.6238319716955487\n",
            "\n",
            "Epoch : 478, train loss : 0.5975111349062487\n",
            "Epoch : 478, val loss : 0.622863233089447\n",
            "\n",
            "Epoch : 479, train loss : 0.5942089564872511\n",
            "Epoch : 479, val loss : 0.6248711222096492\n",
            "\n",
            "Epoch : 480, train loss : 0.5931190019304102\n",
            "Epoch : 480, val loss : 0.6361441251478697\n",
            "\n",
            "Epoch : 481, train loss : 0.5973941181645248\n",
            "Epoch : 481, val loss : 0.6246860466505353\n",
            "\n",
            "Epoch : 482, train loss : 0.5937220611355526\n",
            "Epoch : 482, val loss : 0.6220352069327705\n",
            "\n",
            "Epoch : 483, train loss : 0.5965127524101371\n",
            "Epoch : 483, val loss : 0.6186742186546326\n",
            "\n",
            "Epoch : 484, train loss : 0.5936942951245742\n",
            "Epoch : 484, val loss : 0.6215589328816062\n",
            "\n",
            "Epoch : 485, train loss : 0.5941197061177457\n",
            "Epoch : 485, val loss : 0.6260541065743095\n",
            "\n",
            "Epoch : 486, train loss : 0.5945724758234887\n",
            "Epoch : 486, val loss : 0.6250960215141897\n",
            "\n",
            "Epoch : 487, train loss : 0.5940269695990015\n",
            "Epoch : 487, val loss : 0.6242241122220692\n",
            "\n",
            "Epoch : 488, train loss : 0.5930568828727258\n",
            "Epoch : 488, val loss : 0.6188638006386004\n",
            "\n",
            "Epoch : 489, train loss : 0.5966988745963936\n",
            "Epoch : 489, val loss : 0.6239378138592367\n",
            "\n",
            "Epoch : 490, train loss : 0.5950352957754426\n",
            "Epoch : 490, val loss : 0.6359728119875255\n",
            "\n",
            "Epoch : 491, train loss : 0.5937828705166328\n",
            "Epoch : 491, val loss : 0.619906478806546\n",
            "\n",
            "Epoch : 492, train loss : 0.5944539077354204\n",
            "Epoch : 492, val loss : 0.6400197383604552\n",
            "\n",
            "Epoch : 493, train loss : 0.59324888659246\n",
            "Epoch : 493, val loss : 0.6303342580795288\n",
            "\n",
            "Epoch : 494, train loss : 0.5943710621559255\n",
            "Epoch : 494, val loss : 0.6304364863194918\n",
            "\n",
            "Epoch : 495, train loss : 0.5933738746426325\n",
            "Epoch : 495, val loss : 0.6370802929526882\n",
            "\n",
            "Epoch : 496, train loss : 0.593876786304243\n",
            "Epoch : 496, val loss : 0.6275159151930558\n",
            "\n",
            "Epoch : 497, train loss : 0.5950438503063085\n",
            "Epoch : 497, val loss : 0.6288460932279888\n",
            "\n",
            "Epoch : 498, train loss : 0.5938196501948618\n",
            "Epoch : 498, val loss : 0.6188097078549235\n",
            "\n",
            "Epoch : 499, train loss : 0.5946230337475282\n",
            "Epoch : 499, val loss : 0.6255732006148287\n",
            "\n",
            "Epoch : 500, train loss : 0.5935913075100292\n",
            "Epoch : 500, val loss : 0.6360860419900795\n",
            "\n",
            "Epoch : 501, train loss : 0.5932614626306477\n",
            "Epoch : 501, val loss : 0.6327563775213142\n",
            "\n",
            "Epoch : 502, train loss : 0.5928806689652529\n",
            "Epoch : 502, val loss : 0.6362514919356296\n",
            "\n",
            "Epoch : 503, train loss : 0.5923868547786367\n",
            "Epoch : 503, val loss : 0.6356853215317977\n",
            "\n",
            "Epoch : 504, train loss : 0.5930024515498769\n",
            "Epoch : 504, val loss : 0.6266489295583022\n",
            "\n",
            "Epoch : 505, train loss : 0.5941250040675655\n",
            "Epoch : 505, val loss : 0.6245293695675699\n",
            "\n",
            "Epoch : 506, train loss : 0.593094213442369\n",
            "Epoch : 506, val loss : 0.6283045285626461\n",
            "\n",
            "Epoch : 507, train loss : 0.5922850794864424\n",
            "Epoch : 507, val loss : 0.6317650911055114\n",
            "\n",
            "Epoch : 508, train loss : 0.5936589846105287\n",
            "Epoch : 508, val loss : 0.6299129081399817\n",
            "\n",
            "Epoch : 509, train loss : 0.5950285140312078\n",
            "Epoch : 509, val loss : 0.6258365135443837\n",
            "\n",
            "Epoch : 510, train loss : 0.5922655752210907\n",
            "Epoch : 510, val loss : 0.6300715976639798\n",
            "\n",
            "Epoch : 511, train loss : 0.5927681108315785\n",
            "Epoch : 511, val loss : 0.6306746727541873\n",
            "\n",
            "Epoch : 512, train loss : 0.591601982622436\n",
            "Epoch : 512, val loss : 0.6199783416170823\n",
            "\n",
            "Epoch : 513, train loss : 0.5923847422455297\n",
            "Epoch : 513, val loss : 0.6329694531465831\n",
            "\n",
            "Epoch : 514, train loss : 0.5923937947461095\n",
            "Epoch : 514, val loss : 0.6275748030135506\n",
            "\n",
            "Epoch : 515, train loss : 0.5924660334081359\n",
            "Epoch : 515, val loss : 0.6299782254193957\n",
            "\n",
            "Epoch : 516, train loss : 0.5929837837363734\n",
            "Epoch : 516, val loss : 0.6200398865498997\n",
            "\n",
            "Epoch : 517, train loss : 0.5927734477953479\n",
            "Epoch : 517, val loss : 0.6262709278809396\n",
            "\n",
            "Epoch : 518, train loss : 0.5910718043645223\n",
            "Epoch : 518, val loss : 0.6199277262938652\n",
            "\n",
            "Epoch : 519, train loss : 0.5918171340768986\n",
            "Epoch : 519, val loss : 0.6236011150636172\n",
            "\n",
            "Epoch : 520, train loss : 0.5916514768744963\n",
            "Epoch : 520, val loss : 0.6161835099521437\n",
            "\n",
            "Epoch : 521, train loss : 0.5914887713663503\n",
            "Epoch : 521, val loss : 0.6332996299392298\n",
            "\n",
            "Epoch : 522, train loss : 0.5915983770832871\n",
            "Epoch : 522, val loss : 0.6218175354756805\n",
            "\n",
            "Epoch : 523, train loss : 0.5916298378597605\n",
            "Epoch : 523, val loss : 0.6255244408783159\n",
            "\n",
            "Epoch : 524, train loss : 0.5918287596919319\n",
            "Epoch : 524, val loss : 0.6344089476685775\n",
            "\n",
            "Epoch : 525, train loss : 0.5934233938202713\n",
            "Epoch : 525, val loss : 0.6320112535828039\n",
            "\n",
            "Epoch : 526, train loss : 0.5938762491399593\n",
            "Epoch : 526, val loss : 0.6226751271047091\n",
            "\n",
            "Epoch : 527, train loss : 0.5907840215798579\n",
            "Epoch : 527, val loss : 0.6148553694549361\n",
            "\n",
            "Epoch : 528, train loss : 0.5915395548849394\n",
            "Epoch : 528, val loss : 0.6243944168090819\n",
            "\n",
            "Epoch : 529, train loss : 0.5901505278818536\n",
            "Epoch : 529, val loss : 0.6162849664688109\n",
            "\n",
            "Epoch : 530, train loss : 0.5926107112205388\n",
            "Epoch : 530, val loss : 0.6204264273768978\n",
            "\n",
            "Epoch : 531, train loss : 0.5907696447589181\n",
            "Epoch : 531, val loss : 0.6308700928562565\n",
            "\n",
            "Epoch : 532, train loss : 0.5899137536684671\n",
            "Epoch : 532, val loss : 0.6210941954662924\n",
            "\n",
            "Epoch : 533, train loss : 0.593213898304737\n",
            "Epoch : 533, val loss : 0.6153370110612166\n",
            "\n",
            "Epoch : 534, train loss : 0.5901729578321631\n",
            "Epoch : 534, val loss : 0.6345079839229584\n",
            "\n",
            "Epoch : 535, train loss : 0.5903135247302779\n",
            "Epoch : 535, val loss : 0.6301158948948509\n",
            "\n",
            "Epoch : 536, train loss : 0.5926683751019564\n",
            "Epoch : 536, val loss : 0.615092164591739\n",
            "\n",
            "Epoch : 537, train loss : 0.5890368792143732\n",
            "Epoch : 537, val loss : 0.6293084119495593\n",
            "\n",
            "Epoch : 538, train loss : 0.5904293923666981\n",
            "Epoch : 538, val loss : 0.6194943722925687\n",
            "\n",
            "Epoch : 539, train loss : 0.5900356655771086\n",
            "Epoch : 539, val loss : 0.6334318785290968\n",
            "\n",
            "Epoch : 540, train loss : 0.5911141957297468\n",
            "Epoch : 540, val loss : 0.642388376750444\n",
            "\n",
            "Epoch : 541, train loss : 0.5922036485238509\n",
            "Epoch : 541, val loss : 0.6298732381117971\n",
            "\n",
            "Epoch : 542, train loss : 0.5917552570501965\n",
            "Epoch : 542, val loss : 0.6274253882859883\n",
            "\n",
            "Epoch : 543, train loss : 0.5911741869016128\n",
            "Epoch : 543, val loss : 0.616141151440771\n",
            "\n",
            "Epoch : 544, train loss : 0.5905113249114068\n",
            "Epoch : 544, val loss : 0.624758812942003\n",
            "\n",
            "Epoch : 545, train loss : 0.5912010915351634\n",
            "Epoch : 545, val loss : 0.6226696826909718\n",
            "\n",
            "Epoch : 546, train loss : 0.5909732780673285\n",
            "Epoch : 546, val loss : 0.6127473178662752\n",
            "\n",
            "Epoch : 547, train loss : 0.5891452175198175\n",
            "Epoch : 547, val loss : 0.6224979946487827\n",
            "\n",
            "Epoch : 548, train loss : 0.5905038781238325\n",
            "Epoch : 548, val loss : 0.6256169300330312\n",
            "\n",
            "Epoch : 549, train loss : 0.5891102586731767\n",
            "Epoch : 549, val loss : 0.6135069523987017\n",
            "\n",
            "Epoch : 550, train loss : 0.5889189460060811\n",
            "Epoch : 550, val loss : 0.6265428599558377\n",
            "\n",
            "Epoch : 551, train loss : 0.5896199667092527\n",
            "Epoch : 551, val loss : 0.6189512390839426\n",
            "\n",
            "Epoch : 552, train loss : 0.5907636440161502\n",
            "Epoch : 552, val loss : 0.6188027450912876\n",
            "\n",
            "Epoch : 553, train loss : 0.5889772749308385\n",
            "Epoch : 553, val loss : 0.6222649969552693\n",
            "\n",
            "Epoch : 554, train loss : 0.5904340124491486\n",
            "Epoch : 554, val loss : 0.6233811268680974\n",
            "\n",
            "Epoch : 555, train loss : 0.5887318298672184\n",
            "Epoch : 555, val loss : 0.6174066254967138\n",
            "\n",
            "Epoch : 556, train loss : 0.5883251921697099\n",
            "Epoch : 556, val loss : 0.6276248411128394\n",
            "\n",
            "Epoch : 557, train loss : 0.5887335909135414\n",
            "Epoch : 557, val loss : 0.6118496577990683\n",
            "\n",
            "Epoch : 558, train loss : 0.5882700663624385\n",
            "Epoch : 558, val loss : 0.628973474628047\n",
            "\n",
            "Epoch : 559, train loss : 0.5889200967369654\n",
            "Epoch : 559, val loss : 0.6233189294212744\n",
            "\n",
            "Epoch : 560, train loss : 0.5884426843036309\n",
            "Epoch : 560, val loss : 0.6183689754260215\n",
            "\n",
            "Epoch : 561, train loss : 0.5881703523072329\n",
            "Epoch : 561, val loss : 0.6210878318861911\n",
            "\n",
            "Epoch : 562, train loss : 0.5887043653112469\n",
            "Epoch : 562, val loss : 0.6309176652055037\n",
            "\n",
            "Epoch : 563, train loss : 0.5890510636748691\n",
            "Epoch : 563, val loss : 0.6172141730785369\n",
            "\n",
            "Epoch : 564, train loss : 0.5890559743751177\n",
            "Epoch : 564, val loss : 0.6267111395534717\n",
            "\n",
            "Epoch : 565, train loss : 0.5894520932977849\n",
            "Epoch : 565, val loss : 0.6112956326258809\n",
            "\n",
            "Epoch : 566, train loss : 0.5902556692108961\n",
            "Epoch : 566, val loss : 0.6148692338090194\n",
            "\n",
            "Epoch : 567, train loss : 0.588566064292734\n",
            "Epoch : 567, val loss : 0.6136558479384372\n",
            "\n",
            "Epoch : 568, train loss : 0.587763036742355\n",
            "Epoch : 568, val loss : 0.6197630543457835\n",
            "\n",
            "Epoch : 569, train loss : 0.5888742930961377\n",
            "Epoch : 569, val loss : 0.6343234902934025\n",
            "\n",
            "Epoch : 570, train loss : 0.5873019989692805\n",
            "Epoch : 570, val loss : 0.6282041464981281\n",
            "\n",
            "Epoch : 571, train loss : 0.5878950030514688\n",
            "Epoch : 571, val loss : 0.622177381264536\n",
            "\n",
            "Epoch : 572, train loss : 0.5882444334752636\n",
            "Epoch : 572, val loss : 0.6108355302559703\n",
            "\n",
            "Epoch : 573, train loss : 0.5872653395840622\n",
            "Epoch : 573, val loss : 0.614935909446917\n",
            "\n",
            "Epoch : 574, train loss : 0.5871176681735298\n",
            "Epoch : 574, val loss : 0.6203638942618119\n",
            "\n",
            "Epoch : 575, train loss : 0.588975803418593\n",
            "Epoch : 575, val loss : 0.6281698408879733\n",
            "\n",
            "Epoch : 576, train loss : 0.5862959036321355\n",
            "Epoch : 576, val loss : 0.6373466974810549\n",
            "\n",
            "Epoch : 577, train loss : 0.5864131631273215\n",
            "Epoch : 577, val loss : 0.6176066806441859\n",
            "\n",
            "Epoch : 578, train loss : 0.5874962797670652\n",
            "Epoch : 578, val loss : 0.6159714115293403\n",
            "\n",
            "Epoch : 579, train loss : 0.5880521877245469\n",
            "Epoch : 579, val loss : 0.6282750979850167\n",
            "\n",
            "Epoch : 580, train loss : 0.58764970573512\n",
            "Epoch : 580, val loss : 0.6134725134623679\n",
            "\n",
            "Epoch : 581, train loss : 0.5890692024519951\n",
            "Epoch : 581, val loss : 0.6109265371372825\n",
            "\n",
            "Epoch : 582, train loss : 0.5880205577070062\n",
            "Epoch : 582, val loss : 0.6178690367623378\n",
            "\n",
            "Epoch : 583, train loss : 0.5876919713887306\n",
            "Epoch : 583, val loss : 0.6217813460450425\n",
            "\n",
            "Epoch : 584, train loss : 0.5864117990840564\n",
            "Epoch : 584, val loss : 0.6383458485728817\n",
            "\n",
            "Epoch : 585, train loss : 0.5855511647282224\n",
            "Epoch : 585, val loss : 0.6170323491096498\n",
            "\n",
            "Epoch : 586, train loss : 0.5873774550177834\n",
            "Epoch : 586, val loss : 0.6303276905888006\n",
            "\n",
            "Epoch : 587, train loss : 0.5901045661984067\n",
            "Epoch : 587, val loss : 0.6157734111735697\n",
            "\n",
            "Epoch : 588, train loss : 0.5889360198468873\n",
            "Epoch : 588, val loss : 0.6237565592715614\n",
            "\n",
            "Epoch : 589, train loss : 0.5869401736692945\n",
            "Epoch : 589, val loss : 0.632512905095753\n",
            "\n",
            "Epoch : 590, train loss : 0.5868249947374516\n",
            "Epoch : 590, val loss : 0.6093985344234265\n",
            "\n",
            "Epoch : 591, train loss : 0.5856933384230644\n",
            "Epoch : 591, val loss : 0.6285879408058366\n",
            "\n",
            "Epoch : 592, train loss : 0.5876569540211648\n",
            "Epoch : 592, val loss : 0.6288062428173267\n",
            "\n",
            "Epoch : 593, train loss : 0.5868558031139949\n",
            "Epoch : 593, val loss : 0.6228575659425637\n",
            "\n",
            "Epoch : 594, train loss : 0.5855942608732165\n",
            "Epoch : 594, val loss : 0.6172926551417302\n",
            "\n",
            "Epoch : 595, train loss : 0.5858292095588917\n",
            "Epoch : 595, val loss : 0.6082867431013207\n",
            "\n",
            "Epoch : 596, train loss : 0.5857529239221051\n",
            "Epoch : 596, val loss : 0.6271390711006366\n",
            "\n",
            "Epoch : 597, train loss : 0.5866916114633736\n",
            "Epoch : 597, val loss : 0.6096571715254533\n",
            "\n",
            "Epoch : 598, train loss : 0.5870370142387621\n",
            "Epoch : 598, val loss : 0.6195783505314276\n",
            "\n",
            "Epoch : 599, train loss : 0.5864961875207498\n",
            "Epoch : 599, val loss : 0.6259309523984006\n",
            "\n",
            "Epoch : 600, train loss : 0.5873432190129252\n",
            "Epoch : 600, val loss : 0.6168138776954852\n",
            "\n",
            "Epoch : 601, train loss : 0.5849202130780072\n",
            "Epoch : 601, val loss : 0.6182456126338556\n",
            "\n",
            "Epoch : 602, train loss : 0.5865532412673484\n",
            "Epoch : 602, val loss : 0.6134142907042252\n",
            "\n",
            "Epoch : 603, train loss : 0.5882845952655329\n",
            "Epoch : 603, val loss : 0.6122582492075469\n",
            "\n",
            "Epoch : 604, train loss : 0.5843463841712835\n",
            "Epoch : 604, val loss : 0.6191664322426444\n",
            "\n",
            "Epoch : 605, train loss : 0.5864625379894718\n",
            "Epoch : 605, val loss : 0.6190755304537322\n",
            "\n",
            "Epoch : 606, train loss : 0.5866332232952117\n",
            "Epoch : 606, val loss : 0.6120529159119253\n",
            "\n",
            "Epoch : 607, train loss : 0.5842716748064215\n",
            "Epoch : 607, val loss : 0.610893465970692\n",
            "\n",
            "Epoch : 608, train loss : 0.5857037959676801\n",
            "Epoch : 608, val loss : 0.6229141746696671\n",
            "\n",
            "Epoch : 609, train loss : 0.5851793628750426\n",
            "Epoch : 609, val loss : 0.6118984567491633\n",
            "\n",
            "Epoch : 610, train loss : 0.5854443228606022\n",
            "Epoch : 610, val loss : 0.6197980485464397\n",
            "\n",
            "Epoch : 611, train loss : 0.5844157784274131\n",
            "Epoch : 611, val loss : 0.6138524664075752\n",
            "\n",
            "Epoch : 612, train loss : 0.5855036526015308\n",
            "Epoch : 612, val loss : 0.6242251176583141\n",
            "\n",
            "Epoch : 613, train loss : 0.5853880938255426\n",
            "Epoch : 613, val loss : 0.6230393616776717\n",
            "\n",
            "Epoch : 614, train loss : 0.5852471902514946\n",
            "Epoch : 614, val loss : 0.6026169422425722\n",
            "\n",
            "Epoch : 615, train loss : 0.585634695399891\n",
            "Epoch : 615, val loss : 0.6289473975959575\n",
            "\n",
            "Epoch : 616, train loss : 0.5847399406360857\n",
            "Epoch : 616, val loss : 0.616955512448361\n",
            "\n",
            "Epoch : 617, train loss : 0.5851624660419691\n",
            "Epoch : 617, val loss : 0.61665383765572\n",
            "\n",
            "Epoch : 618, train loss : 0.584933733398264\n",
            "Epoch : 618, val loss : 0.6394483321591428\n",
            "\n",
            "Epoch : 619, train loss : 0.5873067434990047\n",
            "Epoch : 619, val loss : 0.6206082576199582\n",
            "\n",
            "Epoch : 620, train loss : 0.584267957824649\n",
            "Epoch : 620, val loss : 0.6345434251584504\n",
            "\n",
            "Epoch : 621, train loss : 0.5852251784367999\n",
            "Epoch : 621, val loss : 0.6142436548283228\n",
            "\n",
            "Epoch : 622, train loss : 0.5848535604549178\n",
            "Epoch : 622, val loss : 0.632253773902592\n",
            "\n",
            "Epoch : 623, train loss : 0.5843730355754043\n",
            "Epoch : 623, val loss : 0.6051110233131208\n",
            "\n",
            "Epoch : 624, train loss : 0.5838008488669539\n",
            "Epoch : 624, val loss : 0.6134077514472761\n",
            "\n",
            "Epoch : 625, train loss : 0.5844664739839956\n",
            "Epoch : 625, val loss : 0.6211737425703752\n",
            "\n",
            "Epoch : 626, train loss : 0.5844371035243525\n",
            "Epoch : 626, val loss : 0.6184586223803069\n",
            "\n",
            "Epoch : 627, train loss : 0.5840000649293262\n",
            "Epoch : 627, val loss : 0.6216599062869423\n",
            "\n",
            "Epoch : 628, train loss : 0.5844794538888063\n",
            "Epoch : 628, val loss : 0.6237156861706785\n",
            "\n",
            "Epoch : 629, train loss : 0.5834810168454143\n",
            "Epoch : 629, val loss : 0.6138387636134499\n",
            "\n",
            "Epoch : 630, train loss : 0.5828206858851689\n",
            "Epoch : 630, val loss : 0.6115258524292393\n",
            "\n",
            "Epoch : 631, train loss : 0.5839314113963733\n",
            "Epoch : 631, val loss : 0.6339533799572995\n",
            "\n",
            "Epoch : 632, train loss : 0.584762513637543\n",
            "Epoch : 632, val loss : 0.6052560320025996\n",
            "\n",
            "Epoch : 633, train loss : 0.5840174046429719\n",
            "Epoch : 633, val loss : 0.6139352509849949\n",
            "\n",
            "Epoch : 634, train loss : 0.584969370654135\n",
            "Epoch : 634, val loss : 0.6049956114668593\n",
            "\n",
            "Epoch : 635, train loss : 0.5828415220434017\n",
            "Epoch : 635, val loss : 0.605951975834997\n",
            "\n",
            "Epoch : 636, train loss : 0.5830685846733326\n",
            "Epoch : 636, val loss : 0.6248071554460023\n",
            "\n",
            "Epoch : 637, train loss : 0.5861327104496235\n",
            "Epoch : 637, val loss : 0.6233884300056257\n",
            "\n",
            "Epoch : 638, train loss : 0.5856690892667483\n",
            "Epoch : 638, val loss : 0.6145851565034768\n",
            "\n",
            "Epoch : 639, train loss : 0.5845543749404676\n",
            "Epoch : 639, val loss : 0.6295728526617352\n",
            "\n",
            "Epoch : 640, train loss : 0.5829356265790536\n",
            "Epoch : 640, val loss : 0.6247998287803248\n",
            "\n",
            "Epoch : 641, train loss : 0.5839865722439506\n",
            "Epoch : 641, val loss : 0.6156905560117019\n",
            "\n",
            "Epoch : 642, train loss : 0.582708317944498\n",
            "Epoch : 642, val loss : 0.6175619552009984\n",
            "\n",
            "Epoch : 643, train loss : 0.5828589276833969\n",
            "Epoch : 643, val loss : 0.6225170242158988\n",
            "\n",
            "Epoch : 644, train loss : 0.5845478816465896\n",
            "Epoch : 644, val loss : 0.6164303343547018\n",
            "\n",
            "Epoch : 645, train loss : 0.5853218813737233\n",
            "Epoch : 645, val loss : 0.604106267816142\n",
            "\n",
            "Epoch : 646, train loss : 0.5835455144896653\n",
            "Epoch : 646, val loss : 0.613135731533954\n",
            "\n",
            "Epoch : 647, train loss : 0.5830706551219478\n",
            "Epoch : 647, val loss : 0.6187946890529833\n",
            "\n",
            "Epoch : 648, train loss : 0.583192790999557\n",
            "Epoch : 648, val loss : 0.6162718440357008\n",
            "\n",
            "Epoch : 649, train loss : 0.5822744629599832\n",
            "Epoch : 649, val loss : 0.6165087583817935\n",
            "\n",
            "Epoch : 650, train loss : 0.5846392732678037\n",
            "Epoch : 650, val loss : 0.6243066238729578\n",
            "\n",
            "Epoch : 651, train loss : 0.5834872352354451\n",
            "Epoch : 651, val loss : 0.6362912576449545\n",
            "\n",
            "Epoch : 652, train loss : 0.5830600799936237\n",
            "Epoch : 652, val loss : 0.6208719391571849\n",
            "\n",
            "Epoch : 653, train loss : 0.5830549157027042\n",
            "Epoch : 653, val loss : 0.6092855020573263\n",
            "\n",
            "Epoch : 654, train loss : 0.5848836519501426\n",
            "Epoch : 654, val loss : 0.6198649845625225\n",
            "\n",
            "Epoch : 655, train loss : 0.5839165422049435\n",
            "Epoch : 655, val loss : 0.6198768647093521\n",
            "\n",
            "Epoch : 656, train loss : 0.5821167019280521\n",
            "Epoch : 656, val loss : 0.6077410735582051\n",
            "\n",
            "Epoch : 657, train loss : 0.5811827877254198\n",
            "Epoch : 657, val loss : 0.6163954264239262\n",
            "\n",
            "Epoch : 658, train loss : 0.5838027578411679\n",
            "Epoch : 658, val loss : 0.6082891931659297\n",
            "\n",
            "Epoch : 659, train loss : 0.5805202397433195\n",
            "Epoch : 659, val loss : 0.6226939386443087\n",
            "\n",
            "Epoch : 660, train loss : 0.5813830440694633\n",
            "Epoch : 660, val loss : 0.6183163225650787\n",
            "\n",
            "Epoch : 661, train loss : 0.5816614840969893\n",
            "Epoch : 661, val loss : 0.6144978200134479\n",
            "\n",
            "Epoch : 662, train loss : 0.5803532732255527\n",
            "Epoch : 662, val loss : 0.6203687191009521\n",
            "\n",
            "Epoch : 663, train loss : 0.583338402076201\n",
            "Epoch : 663, val loss : 0.6062315075021042\n",
            "\n",
            "Epoch : 664, train loss : 0.5805228457306368\n",
            "Epoch : 664, val loss : 0.621346735640576\n",
            "\n",
            "Epoch : 665, train loss : 0.5807140178752669\n",
            "Epoch : 665, val loss : 0.606537894198769\n",
            "\n",
            "Epoch : 666, train loss : 0.5822967047041112\n",
            "Epoch : 666, val loss : 0.6202749233496816\n",
            "\n",
            "Epoch : 667, train loss : 0.5816000055183065\n",
            "Epoch : 667, val loss : 0.6114758143299504\n",
            "\n",
            "Epoch : 668, train loss : 0.583134347742254\n",
            "Epoch : 668, val loss : 0.6148062003286261\n",
            "\n",
            "Epoch : 669, train loss : 0.5816056432146012\n",
            "Epoch : 669, val loss : 0.6049472940595526\n",
            "\n",
            "Epoch : 670, train loss : 0.5790145162380103\n",
            "Epoch : 670, val loss : 0.6236829694948698\n",
            "\n",
            "Epoch : 671, train loss : 0.5834310593027057\n",
            "Epoch : 671, val loss : 0.6332081524949327\n",
            "\n",
            "Epoch : 672, train loss : 0.5809781905376551\n",
            "Epoch : 672, val loss : 0.6178681583780993\n",
            "\n",
            "Epoch : 673, train loss : 0.5792742682225775\n",
            "Epoch : 673, val loss : 0.6134891572751497\n",
            "\n",
            "Epoch : 674, train loss : 0.5804039602929896\n",
            "Epoch : 674, val loss : 0.6051235355828939\n",
            "\n",
            "Epoch : 675, train loss : 0.5808227703426827\n",
            "Epoch : 675, val loss : 0.6163643269162429\n",
            "\n",
            "Epoch : 676, train loss : 0.580083007704128\n",
            "Epoch : 676, val loss : 0.6212346898882013\n",
            "\n",
            "Epoch : 677, train loss : 0.5811554679364865\n",
            "Epoch : 677, val loss : 0.6149702417223077\n",
            "\n",
            "Epoch : 678, train loss : 0.5809417329051279\n",
            "Epoch : 678, val loss : 0.6067990723409151\n",
            "\n",
            "Epoch : 679, train loss : 0.5802784786079871\n",
            "Epoch : 679, val loss : 0.6090629634104276\n",
            "\n",
            "Epoch : 680, train loss : 0.5828955616011767\n",
            "Epoch : 680, val loss : 0.6253343757830168\n",
            "\n",
            "Epoch : 681, train loss : 0.5826038073409688\n",
            "Epoch : 681, val loss : 0.6108075992057197\n",
            "\n",
            "Epoch : 682, train loss : 0.5807602707183723\n",
            "Epoch : 682, val loss : 0.6335341287286658\n",
            "\n",
            "Epoch : 683, train loss : 0.5803757400223702\n",
            "Epoch : 683, val loss : 0.608839579318699\n",
            "\n",
            "Epoch : 684, train loss : 0.5810674163428221\n",
            "Epoch : 684, val loss : 0.6201711940137963\n",
            "\n",
            "Epoch : 685, train loss : 0.5799222080996541\n",
            "Epoch : 685, val loss : 0.6280013448313663\n",
            "\n",
            "Epoch : 686, train loss : 0.5808194301345134\n",
            "Epoch : 686, val loss : 0.6153250587613959\n",
            "\n",
            "Epoch : 687, train loss : 0.5788704628294163\n",
            "Epoch : 687, val loss : 0.605581594140906\n",
            "\n",
            "Epoch : 688, train loss : 0.5773285705031774\n",
            "Epoch : 688, val loss : 0.6037523432781822\n",
            "\n",
            "Epoch : 689, train loss : 0.5812975587266863\n",
            "Epoch : 689, val loss : 0.6016395154752229\n",
            "\n",
            "Epoch : 690, train loss : 0.5813902860338034\n",
            "Epoch : 690, val loss : 0.6130585654785758\n",
            "\n",
            "Epoch : 691, train loss : 0.5790144291791052\n",
            "Epoch : 691, val loss : 0.6062120842306239\n",
            "\n",
            "Epoch : 692, train loss : 0.5803482987663964\n",
            "Epoch : 692, val loss : 0.6200257257411353\n",
            "\n",
            "Epoch : 693, train loss : 0.5809997629035603\n",
            "Epoch : 693, val loss : 0.6148422925095809\n",
            "\n",
            "Epoch : 694, train loss : 0.5807944008798305\n",
            "Epoch : 694, val loss : 0.6166941680406269\n",
            "\n",
            "Epoch : 695, train loss : 0.579803928461942\n",
            "Epoch : 695, val loss : 0.6187039096104471\n",
            "\n",
            "Epoch : 696, train loss : 0.5798879589095263\n",
            "Epoch : 696, val loss : 0.6212040515322437\n",
            "\n",
            "Epoch : 697, train loss : 0.5820399322293021\n",
            "Epoch : 697, val loss : 0.6253031649087605\n",
            "\n",
            "Epoch : 698, train loss : 0.5798422045779951\n",
            "Epoch : 698, val loss : 0.6236766890475625\n",
            "\n",
            "Epoch : 699, train loss : 0.5806028331771044\n",
            "Epoch : 699, val loss : 0.6143116621594682\n",
            "\n",
            "Epoch : 700, train loss : 0.5797342591213455\n",
            "Epoch : 700, val loss : 0.6073994856131706\n",
            "\n",
            "Epoch : 701, train loss : 0.5797802199016915\n",
            "Epoch : 701, val loss : 0.6149967689263193\n",
            "\n",
            "Epoch : 702, train loss : 0.5800006008509436\n",
            "Epoch : 702, val loss : 0.606272595493417\n",
            "\n",
            "Epoch : 703, train loss : 0.5786109967665243\n",
            "Epoch : 703, val loss : 0.6132615663503345\n",
            "\n",
            "Epoch : 704, train loss : 0.5795711956240914\n",
            "Epoch : 704, val loss : 0.6107184165402462\n",
            "\n",
            "Epoch : 705, train loss : 0.5791129244096351\n",
            "Epoch : 705, val loss : 0.6175897968442818\n",
            "\n",
            "Epoch : 706, train loss : 0.5792559296795813\n",
            "Epoch : 706, val loss : 0.6158959724401173\n",
            "\n",
            "Epoch : 707, train loss : 0.5819246423966957\n",
            "Epoch : 707, val loss : 0.6069439853492534\n",
            "\n",
            "Epoch : 708, train loss : 0.58063806403767\n",
            "Epoch : 708, val loss : 0.6037458275493823\n",
            "\n",
            "Epoch : 709, train loss : 0.5793377556584097\n",
            "Epoch : 709, val loss : 0.6129552878831562\n",
            "\n",
            "Epoch : 710, train loss : 0.5797943899125763\n",
            "Epoch : 710, val loss : 0.6132826460035223\n",
            "\n",
            "Epoch : 711, train loss : 0.5786089980241028\n",
            "Epoch : 711, val loss : 0.615642028419595\n",
            "\n",
            "Epoch : 712, train loss : 0.5798192914688225\n",
            "Epoch : 712, val loss : 0.6099341778378737\n",
            "\n",
            "Epoch : 713, train loss : 0.5780690088416595\n",
            "Epoch : 713, val loss : 0.6005402897533617\n",
            "\n",
            "Epoch : 714, train loss : 0.5786153932412464\n",
            "Epoch : 714, val loss : 0.6088020472150099\n",
            "\n",
            "Epoch : 715, train loss : 0.5782494033827926\n",
            "Epoch : 715, val loss : 0.6207154126543748\n",
            "\n",
            "Epoch : 716, train loss : 0.5788704563270913\n",
            "Epoch : 716, val loss : 0.6021221295783393\n",
            "\n",
            "Epoch : 717, train loss : 0.5788424990393896\n",
            "Epoch : 717, val loss : 0.6079499062738921\n",
            "\n",
            "Epoch : 718, train loss : 0.5804493696400616\n",
            "Epoch : 718, val loss : 0.6259290983802392\n",
            "\n",
            "Epoch : 719, train loss : 0.577777734669772\n",
            "Epoch : 719, val loss : 0.62093270452399\n",
            "\n",
            "Epoch : 720, train loss : 0.5790298828572938\n",
            "Epoch : 720, val loss : 0.5987625482835269\n",
            "\n",
            "Epoch : 721, train loss : 0.57706080512567\n",
            "Epoch : 721, val loss : 0.604100813991145\n",
            "\n",
            "Epoch : 722, train loss : 0.5774008346326428\n",
            "Epoch : 722, val loss : 0.6140401582968862\n",
            "\n",
            "Epoch : 723, train loss : 0.5799990202441363\n",
            "Epoch : 723, val loss : 0.6243963367060611\n",
            "\n",
            "Epoch : 724, train loss : 0.5782810827096304\n",
            "Epoch : 724, val loss : 0.6071405097057945\n",
            "\n",
            "Epoch : 725, train loss : 0.5798412976842942\n",
            "Epoch : 725, val loss : 0.6237663956064927\n",
            "\n",
            "Epoch : 726, train loss : 0.5787115501635003\n",
            "Epoch : 726, val loss : 0.6030717052911456\n",
            "\n",
            "Epoch : 727, train loss : 0.5770875479235796\n",
            "Epoch : 727, val loss : 0.6082037844155964\n",
            "\n",
            "Epoch : 728, train loss : 0.5793298201127487\n",
            "Epoch : 728, val loss : 0.6078791492863705\n",
            "\n",
            "Epoch : 729, train loss : 0.578629729422656\n",
            "Epoch : 729, val loss : 0.6058003243647123\n",
            "\n",
            "Epoch : 730, train loss : 0.5767743889129524\n",
            "Epoch : 730, val loss : 0.6210129574725503\n",
            "\n",
            "Epoch : 731, train loss : 0.5792480042486482\n",
            "Epoch : 731, val loss : 0.6148969531059266\n",
            "\n",
            "Epoch : 732, train loss : 0.5778801370750772\n",
            "Epoch : 732, val loss : 0.6121253261440678\n",
            "\n",
            "Epoch : 733, train loss : 0.5773873939658655\n",
            "Epoch : 733, val loss : 0.6063804955858932\n",
            "\n",
            "Epoch : 734, train loss : 0.5781300833730989\n",
            "Epoch : 734, val loss : 0.6179477603811967\n",
            "\n",
            "Epoch : 735, train loss : 0.5790756505547147\n",
            "Epoch : 735, val loss : 0.6025465507256357\n",
            "\n",
            "Epoch : 736, train loss : 0.5776517698259065\n",
            "Epoch : 736, val loss : 0.6155131647461338\n",
            "\n",
            "Epoch : 737, train loss : 0.5775015758745599\n",
            "Epoch : 737, val loss : 0.6185925681340068\n",
            "\n",
            "Epoch : 738, train loss : 0.5807386141834835\n",
            "Epoch : 738, val loss : 0.6213804985347546\n",
            "\n",
            "Epoch : 739, train loss : 0.5788161917166275\n",
            "Epoch : 739, val loss : 0.6158625577625475\n",
            "\n",
            "Epoch : 740, train loss : 0.5774844837911202\n",
            "Epoch : 740, val loss : 0.6146748316915412\n",
            "\n",
            "Epoch : 741, train loss : 0.5781684797821627\n",
            "Epoch : 741, val loss : 0.6206978402639691\n",
            "\n",
            "Epoch : 742, train loss : 0.5772185625451987\n",
            "Epoch : 742, val loss : 0.6298544438261735\n",
            "\n",
            "Epoch : 743, train loss : 0.5754634905945171\n",
            "Epoch : 743, val loss : 0.6153245097712466\n",
            "\n",
            "Epoch : 744, train loss : 0.5762579878171281\n",
            "Epoch : 744, val loss : 0.6075428473322015\n",
            "\n",
            "Epoch : 745, train loss : 0.5768581330776212\n",
            "Epoch : 745, val loss : 0.6194913685321809\n",
            "\n",
            "Epoch : 746, train loss : 0.5766648019805101\n",
            "Epoch : 746, val loss : 0.632334822102597\n",
            "\n",
            "Epoch : 747, train loss : 0.5784986904173187\n",
            "Epoch : 747, val loss : 0.5992992551703202\n",
            "\n",
            "Epoch : 748, train loss : 0.5760407658237403\n",
            "Epoch : 748, val loss : 0.5988698005676272\n",
            "\n",
            "Epoch : 749, train loss : 0.5765177791768857\n",
            "Epoch : 749, val loss : 0.6007280192877117\n",
            "\n",
            "Epoch : 750, train loss : 0.5782916948650823\n",
            "Epoch : 750, val loss : 0.6070617845183925\n",
            "\n",
            "Epoch : 751, train loss : 0.5771428035967278\n",
            "Epoch : 751, val loss : 0.6047366684988925\n",
            "\n",
            "Epoch : 752, train loss : 0.5790171664772613\n",
            "Epoch : 752, val loss : 0.6052551489127309\n",
            "\n",
            "Epoch : 753, train loss : 0.5792305272636988\n",
            "Epoch : 753, val loss : 0.6037208767313706\n",
            "\n",
            "Epoch : 754, train loss : 0.5806304507183307\n",
            "Epoch : 754, val loss : 0.6103823922182384\n",
            "\n",
            "Epoch : 755, train loss : 0.5791895803176994\n",
            "Epoch : 755, val loss : 0.6022672904165167\n",
            "\n",
            "Epoch : 756, train loss : 0.5755703559427552\n",
            "Epoch : 756, val loss : 0.6035664222742382\n",
            "\n",
            "Epoch : 757, train loss : 0.5772288591572734\n",
            "Epoch : 757, val loss : 0.6091418768230237\n",
            "\n",
            "Epoch : 758, train loss : 0.5748799672632506\n",
            "Epoch : 758, val loss : 0.6074152008483285\n",
            "\n",
            "Epoch : 759, train loss : 0.578558080485373\n",
            "Epoch : 759, val loss : 0.6164945991415727\n",
            "\n",
            "Epoch : 760, train loss : 0.5767594339269583\n",
            "Epoch : 760, val loss : 0.6099850331482134\n",
            "\n",
            "Epoch : 761, train loss : 0.5761961902632856\n",
            "Epoch : 761, val loss : 0.6040377789422084\n",
            "\n",
            "Epoch : 762, train loss : 0.576506434426163\n",
            "Epoch : 762, val loss : 0.6161432987765262\n",
            "\n",
            "Epoch : 763, train loss : 0.5766632192062611\n",
            "Epoch : 763, val loss : 0.6267324842904745\n",
            "\n",
            "Epoch : 764, train loss : 0.576929717352896\n",
            "Epoch : 764, val loss : 0.6014736765309383\n",
            "\n",
            "Epoch : 765, train loss : 0.5764182354464674\n",
            "Epoch : 765, val loss : 0.6027714845381287\n",
            "\n",
            "Epoch : 766, train loss : 0.5773514249108053\n",
            "Epoch : 766, val loss : 0.6109307784783213\n",
            "\n",
            "Epoch : 767, train loss : 0.5747264432184626\n",
            "Epoch : 767, val loss : 0.6129503877539384\n",
            "\n",
            "Epoch : 768, train loss : 0.5757706970879526\n",
            "Epoch : 768, val loss : 0.6078910294332004\n",
            "\n",
            "Epoch : 769, train loss : 0.5754643037463679\n",
            "Epoch : 769, val loss : 0.6020521882333252\n",
            "\n",
            "Epoch : 770, train loss : 0.5753757465969432\n",
            "Epoch : 770, val loss : 0.6093135381999769\n",
            "\n",
            "Epoch : 771, train loss : 0.5749563524217315\n",
            "Epoch : 771, val loss : 0.6078124312978043\n",
            "\n",
            "Epoch : 772, train loss : 0.5761489319078847\n",
            "Epoch : 772, val loss : 0.5990670856676603\n",
            "\n",
            "Epoch : 773, train loss : 0.5764545256441289\n",
            "Epoch : 773, val loss : 0.604376832121297\n",
            "\n",
            "Epoch : 774, train loss : 0.5762681896036322\n",
            "Epoch : 774, val loss : 0.621769379628332\n",
            "\n",
            "Epoch : 775, train loss : 0.5758561724966222\n",
            "Epoch : 775, val loss : 0.6022705655348929\n",
            "\n",
            "Epoch : 776, train loss : 0.5761079864068466\n",
            "Epoch : 776, val loss : 0.6112702896720487\n",
            "\n",
            "Epoch : 777, train loss : 0.5760721553455699\n",
            "Epoch : 777, val loss : 0.5971627282468894\n",
            "\n",
            "Epoch : 778, train loss : 0.5748002924702386\n",
            "Epoch : 778, val loss : 0.6168699750774785\n",
            "\n",
            "Epoch : 779, train loss : 0.5779973754377076\n",
            "Epoch : 779, val loss : 0.6156385007657503\n",
            "\n",
            "Epoch : 780, train loss : 0.5770239089474534\n",
            "Epoch : 780, val loss : 0.6160277724266053\n",
            "\n",
            "Epoch : 781, train loss : 0.5772271344155974\n",
            "Epoch : 781, val loss : 0.6056487230878126\n",
            "\n",
            "Epoch : 782, train loss : 0.5754476713411731\n",
            "Epoch : 782, val loss : 0.6100936782987494\n",
            "\n",
            "Epoch : 783, train loss : 0.5734363776264766\n",
            "Epoch : 783, val loss : 0.6176313350075171\n",
            "\n",
            "Epoch : 784, train loss : 0.5750920321002149\n",
            "Epoch : 784, val loss : 0.6172167203928295\n",
            "\n",
            "Epoch : 785, train loss : 0.5732604509050195\n",
            "Epoch : 785, val loss : 0.6042194695849168\n",
            "\n",
            "Epoch : 786, train loss : 0.5763191226756934\n",
            "Epoch : 786, val loss : 0.6172365812878862\n",
            "\n",
            "Epoch : 787, train loss : 0.5740627731337692\n",
            "Epoch : 787, val loss : 0.605514356964513\n",
            "\n",
            "Epoch : 788, train loss : 0.5755240494554699\n",
            "Epoch : 788, val loss : 0.6203323963441346\n",
            "\n",
            "Epoch : 789, train loss : 0.5750284073930803\n",
            "Epoch : 789, val loss : 0.6009008649148438\n",
            "\n",
            "Epoch : 790, train loss : 0.5756707236622319\n",
            "Epoch : 790, val loss : 0.6027203076764157\n",
            "\n",
            "Epoch : 791, train loss : 0.5760712665138825\n",
            "Epoch : 791, val loss : 0.6058825285811174\n",
            "\n",
            "Epoch : 792, train loss : 0.5765562386223766\n",
            "Epoch : 792, val loss : 0.6230982714577726\n",
            "\n",
            "Epoch : 793, train loss : 0.5760061098770665\n",
            "Epoch : 793, val loss : 0.6126467679676256\n",
            "\n",
            "Epoch : 794, train loss : 0.5737444957097371\n",
            "Epoch : 794, val loss : 0.6264265430601019\n",
            "\n",
            "Epoch : 795, train loss : 0.573782724864555\n",
            "Epoch : 795, val loss : 0.6038072548414531\n",
            "\n",
            "Epoch : 796, train loss : 0.5725746021126256\n",
            "Epoch : 796, val loss : 0.6011537658540825\n",
            "\n",
            "Epoch : 797, train loss : 0.5742716968059538\n",
            "Epoch : 797, val loss : 0.6123735073365663\n",
            "\n",
            "Epoch : 798, train loss : 0.5752364089994718\n",
            "Epoch : 798, val loss : 0.6147251474229912\n",
            "\n",
            "Epoch : 799, train loss : 0.5736860535361551\n",
            "Epoch : 799, val loss : 0.6113775027425665\n",
            "\n",
            "Epoch : 800, train loss : 0.5752638955910997\n",
            "Epoch : 800, val loss : 0.6116957099814163\n"
          ]
        }
      ]
    }
  ]
}